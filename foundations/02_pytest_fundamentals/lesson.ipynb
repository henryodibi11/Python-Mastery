{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Pytest Fundamentals\n",
    "\n",
    "Master professional testing with pytest - from basics to advanced patterns used in production frameworks like Odibi.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Key Terms Glossary\n",
    "\n",
    "**Testing Types**:\n",
    "- **Unit Test**: Test a single function/class in isolation (fast, focused)\n",
    "- **Integration Test**: Test components working together (slower, broader)\n",
    "- **E2E Test**: Test complete user workflow end-to-end (slowest, most realistic)\n",
    "\n",
    "**Testing Concepts**:\n",
    "- **Fixture**: Reusable test setup/teardown (like test data, temp files)\n",
    "- **Mock**: Fake replacement for external dependencies (API, database, files)\n",
    "- **Assertion**: Check that actual result matches expected (the actual test)\n",
    "- **Parametrize**: Run same test with different inputs (data-driven testing)\n",
    "- **Coverage**: Percentage of code executed by tests (line or branch level)\n",
    "- **Marker**: Tag tests for selective execution (like `@pytest.mark.slow`)\n",
    "\n",
    "**Common Issues**:\n",
    "- **Regression**: When a fix or change breaks previously working code\n",
    "- **Flaky Test**: Test that sometimes passes, sometimes fails (usually from timing, randomness, or external state)\n",
    "- **Brittle Test**: Test that breaks easily when code changes, even if behavior is correct\n",
    "- **Technical Debt**: Accumulated shortcuts that make code harder to maintain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Problem: Why Testing Matters for Frameworks\n",
    "\n",
    "**Scenario**: You built a data pipeline framework. A user reports:\n",
    "- \"Pipeline crashes with Azure connections\"\n",
    "- \"Transform fails on edge cases\"\n",
    "- \"Config validation breaks with certain inputs\"\n",
    "\n",
    "**Without tests**:\n",
    "- You manually test each scenario (hours)\n",
    "- Fix breaks another feature (regression)\n",
    "- Fear changing code (technical debt)\n",
    "\n",
    "**With tests**:\n",
    "- Run 416 tests in seconds\n",
    "- Catch regressions immediately\n",
    "- Refactor with confidence\n",
    "\n",
    "**Odibi's testing stats** (416 tests total):\n",
    "- Coverage: ~85%\n",
    "- Test execution: ~5-10 seconds\n",
    "- Prevents 90%+ of production bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¦‰ First Principles: Testing Philosophy\n",
    "\n",
    "### Test Pyramid\n",
    "\n",
    "```\n",
    "       /\\      E2E Tests (few, slow, brittle)\n",
    "      /  \\     \n",
    "     /____\\    Integration Tests (moderate)\n",
    "    /      \\   \n",
    "   /________\\  Unit Tests (many, fast, focused)\n",
    "```\n",
    "\n",
    "**Unit Tests (80%)**:\n",
    "- Test single function/class\n",
    "- Fast (milliseconds)\n",
    "- Example: `test_transform_adds_column()`\n",
    "\n",
    "**Integration Tests (15%)**:\n",
    "- Test components working together\n",
    "- Slower (seconds)\n",
    "- Example: `test_pipeline_reads_transforms_writes()`\n",
    "\n",
    "**E2E Tests (5%)**:\n",
    "- Test full user workflow\n",
    "- Slowest (minutes)\n",
    "- Example: `test_cli_generates_project()`\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **Fast**: Tests should run in seconds\n",
    "2. **Isolated**: Each test independent\n",
    "3. **Repeatable**: Same result every time\n",
    "4. **Self-validating**: Pass/fail, no manual check\n",
    "5. **Timely**: Written with code, not after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 1: Pytest Basics\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\hodibi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\semver-2.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\hodibi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\semver-2.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\hodibi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\semver-2.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\hodibi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\semver-2.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "# Install pytest and plugins\n",
    "!pip install pytest pytest-cov pytest-mock -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Assertions\n",
    "\n",
    "**Why this matters**: Pytest's `assert` statement gives detailed failure messages showing exactly what went wrong. Unlike `unittest` which requires `self.assertEqual()`, pytest uses Python's native `assert` with \"assert rewriting\" magic that provides rich diffs automatically.\n",
    "\n",
    "**Key concepts**:\n",
    "- `assert` - Basic check (pytest shows both sides of comparison)\n",
    "- `pytest.approx()` - Floating-point comparisons (avoids precision issues)\n",
    "- `pytest.raises()` - Test that code raises expected exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple module to test\n",
    "def add(a, b):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a, b):\n",
    "    \"\"\"Divide a by b.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    return a / b\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self, initial=0):\n",
    "        self.value = initial\n",
    "    \n",
    "    def add(self, x):\n",
    "        self.value += x\n",
    "        return self.value\n",
    "    \n",
    "    def reset(self):\n",
    "        self.value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_basic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_basic.py\n",
    "\"\"\"Basic pytest examples.\"\"\"\n",
    "import pytest\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def divide(a, b):\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    return a / b\n",
    "\n",
    "# Test functions must start with 'test_'\n",
    "def test_add():\n",
    "    assert add(2, 3) == 5\n",
    "    assert add(-1, 1) == 0\n",
    "    assert add(0, 0) == 0\n",
    "\n",
    "def test_add_floats():\n",
    "    \"\"\"Floating-point precision requires approximate comparison.\"\"\"\n",
    "    result = add(0.1, 0.2)\n",
    "    # Without pytest.approx, this would fail: 0.1 + 0.2 = 0.30000000000000004\n",
    "    assert result == pytest.approx(0.3)  # Handle float precision\n",
    "\n",
    "def test_divide():\n",
    "    assert divide(10, 2) == 5\n",
    "    assert divide(9, 3) == 3\n",
    "\n",
    "def test_divide_by_zero():\n",
    "    \"\"\"Test exception handling.\n",
    "    \n",
    "    The 'match' parameter uses regex to verify the error message.\n",
    "    This ensures we're catching the RIGHT error, not just any ValueError.\n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n",
    "        divide(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.6.0 -- C:\\Users\\hodibi\\AppData\\Local\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\hodibi\\OneDrive - Ingredion\\Desktop\\Repos\\Python-Mastery\\foundations\\02_pytest_fundamentals\n",
      "plugins: anyio-3.5.0, dash-3.0.2, Faker-37.3.0, cov-7.0.0, mock-3.15.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "test_basic.py::test_add \u001b[32mPASSED\u001b[0m\u001b[32m                                           [ 25%]\u001b[0m\n",
      "test_basic.py::test_add_floats \u001b[32mPASSED\u001b[0m\u001b[32m                                    [ 50%]\u001b[0m\n",
      "test_basic.py::test_divide \u001b[32mPASSED\u001b[0m\u001b[32m                                        [ 75%]\u001b[0m\n",
      "test_basic.py::test_divide_by_zero \u001b[32mPASSED\u001b[0m\u001b[32m                                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.35s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run tests\n",
    "!pytest test_basic.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Discovery\n",
    "\n",
    "Pytest automatically finds tests matching:\n",
    "- Files: `test_*.py` or `*_test.py`\n",
    "- Functions: `test_*()`\n",
    "- Classes: `Test*` (no `__init__`)\n",
    "- Methods: `test_*()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 2: Fixtures (Setup/Teardown)\n",
    "\n",
    "**Why this matters**: Fixtures solve three critical testing problems:\n",
    "1. **Code reuse** - Write setup once, use in many tests\n",
    "2. **Test isolation** - Each test gets fresh data (no shared state bugs)\n",
    "3. **Readability** - Test function shows what it tests, not setup noise\n",
    "\n",
    "**Without fixtures (painful)**:\n",
    "```python\n",
    "def test_data_shape():\n",
    "    df = pd.DataFrame({\"id\": [1,2,3], \"value\": [10,20,30]})  # Duplicate setup\n",
    "    assert df.shape == (3, 2)\n",
    "\n",
    "def test_data_columns():\n",
    "    df = pd.DataFrame({\"id\": [1,2,3], \"value\": [10,20,30]})  # Same setup again!\n",
    "    assert list(df.columns) == [\"id\", \"value\"]\n",
    "```\n",
    "\n",
    "**With fixtures (clean)**:\n",
    "```python\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return pd.DataFrame({\"id\": [1,2,3], \"value\": [10,20,30]})\n",
    "\n",
    "def test_data_shape(sample_data):  # Pytest auto-injects fixture\n",
    "    assert sample_data.shape == (3, 2)\n",
    "\n",
    "def test_data_columns(sample_data):  # Fresh copy for each test\n",
    "    assert list(sample_data.columns) == [\"id\", \"value\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_fixtures.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_fixtures.py\n",
    "\"\"\"Fixture examples.\"\"\"\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Simple fixture\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    \"\"\"Provide sample DataFrame.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"value\": [10, 20, 30]\n",
    "    })\n",
    "\n",
    "def test_data_shape(sample_data):\n",
    "    assert sample_data.shape == (3, 2)\n",
    "\n",
    "def test_data_columns(sample_data):\n",
    "    assert list(sample_data.columns) == [\"id\", \"value\"]\n",
    "\n",
    "# Fixture with setup/teardown using yield\n",
    "@pytest.fixture\n",
    "def temp_csv(tmp_path):\n",
    "    \"\"\"Create temp CSV file with automatic cleanup.\n",
    "    \n",
    "    yield pattern:\n",
    "    - Code BEFORE yield = setup (runs before test)\n",
    "    - yield value = what test receives\n",
    "    - Code AFTER yield = teardown (runs after test, even if test fails)\n",
    "    \"\"\"\n",
    "    # Setup: create temp file\n",
    "    csv_file = tmp_path / \"data.csv\"\n",
    "    df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    yield csv_file  # Provide to test\n",
    "    \n",
    "    # Teardown: cleanup (tmp_path auto-deletes, but this shows the pattern)\n",
    "    print(f\"Cleaned up {csv_file}\")\n",
    "\n",
    "def test_read_csv(temp_csv):\n",
    "    df = pd.read_csv(temp_csv)\n",
    "    assert len(df) == 2\n",
    "    assert \"a\" in df.columns\n",
    "\n",
    "# Fixture scopes control how often setup runs\n",
    "# Scopes (fastest to slowest):\n",
    "# - function (default): New instance per test function\n",
    "# - class: Shared by all tests in a class\n",
    "# - module: Shared by all tests in a file\n",
    "# - session: Shared by entire test run (use for expensive DB connections, etc.)\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def expensive_resource():\n",
    "    \"\"\"Created ONCE per module (file), reused by all tests.\n",
    "    \n",
    "    Use case: Database connection, spark session, large file load.\n",
    "    Trade-off: Faster, but tests share state (could leak bugs).\n",
    "    \"\"\"\n",
    "    print(\"\\nSetup expensive resource\")\n",
    "    resource = {\"data\": \"expensive to create\"}\n",
    "    yield resource\n",
    "    print(\"\\nTeardown expensive resource\")\n",
    "\n",
    "def test_resource_1(expensive_resource):\n",
    "    assert expensive_resource[\"data\"] == \"expensive to create\"\n",
    "\n",
    "def test_resource_2(expensive_resource):\n",
    "    # Uses same instance as test_resource_1\n",
    "    assert \"data\" in expensive_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.6.0 -- C:\\Users\\hodibi\\AppData\\Local\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\hodibi\\OneDrive - Ingredion\\Desktop\\Repos\\Python-Mastery\\foundations\\02_pytest_fundamentals\n",
      "plugins: anyio-3.5.0, dash-3.0.2, Faker-37.3.0, cov-7.0.0, mock-3.15.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "test_fixtures.py::test_data_shape \u001b[32mPASSED\u001b[0m\n",
      "test_fixtures.py::test_data_columns \u001b[32mPASSED\u001b[0m\n",
      "test_fixtures.py::test_read_csv \u001b[32mPASSED\u001b[0mCleaned up C:\\Users\\hodibi\\AppData\\Local\\Temp\\pytest-of-hodibi\\pytest-0\\test_read_csv0\\data.csv\n",
      "\n",
      "test_fixtures.py::test_resource_1 \n",
      "Setup expensive resource\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_fixtures.py::test_resource_2 \u001b[32mPASSED\u001b[0m\n",
      "Teardown expensive resource\n",
      "\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 12.89s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_fixtures.py -v -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Fixtures\n",
    "\n",
    "Pytest provides powerful fixtures out of the box:\n",
    "\n",
    "- **`tmp_path`**: Temporary directory (Path object) - Auto-deleted after test. Safe for parallel tests.\n",
    "  - *Use case*: Testing file I/O without polluting file system\n",
    "- **`tmp_path_factory`**: Create multiple temp dirs (session-scoped)\n",
    "  - *Use case*: Shared temp directories across tests\n",
    "- **`monkeypatch`**: Safely modify objects, dicts, environment variables\n",
    "  - *Use case*: Mock environment vars, patch functions, fake system state\n",
    "- **`capsys`**: Capture stdout/stderr\n",
    "  - *Use case*: Test print statements, CLI output\n",
    "- **`caplog`**: Capture log messages\n",
    "  - *Use case*: Test logging behavior\n",
    "- **`request`**: Access test context and metadata\n",
    "  - *Use case*: Get test name, parametrize values in fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 3: Parametrize (Data-Driven Tests)\n",
    "\n",
    "**Why this matters**: Instead of writing loops or duplicate tests, parametrize runs the same test with different inputs. Each combination becomes a separate test case with its own pass/fail status.\n",
    "\n",
    "**Why not loops?**\n",
    "```python\n",
    "# âŒ BAD: Loop stops at first failure, unclear which case failed\n",
    "def test_palindrome_loop():\n",
    "    for word in [\"racecar\", \"madam\", \"hello\"]:\n",
    "        assert is_palindrome(word)  # \"hello\" fails - other cases not tested!\n",
    "```\n",
    "\n",
    "**Why parametrize?**\n",
    "```python\n",
    "# âœ… GOOD: Each case is a separate test, all run independently\n",
    "@pytest.mark.parametrize(\"word\", [\"racecar\", \"madam\", \"hello\"])\n",
    "def test_palindrome(word):\n",
    "    assert is_palindrome(word)  # Shows exactly which word failed\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- All cases run (failure doesn't stop others)\n",
    "- Clear which input failed\n",
    "- Easier to add more test cases\n",
    "- Better test reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_parametrize.py\n",
    "\"\"\"Parametrize examples.\"\"\"\n",
    "import pytest\n",
    "\n",
    "def is_palindrome(s):\n",
    "    return s == s[::-1]\n",
    "\n",
    "# Single parameter\n",
    "@pytest.mark.parametrize(\"word\", [\n",
    "    \"racecar\",\n",
    "    \"madam\",\n",
    "    \"noon\",\n",
    "])\n",
    "def test_palindromes(word):\n",
    "    assert is_palindrome(word)\n",
    "\n",
    "# Multiple parameters\n",
    "@pytest.mark.parametrize(\"input,expected\", [\n",
    "    (\"racecar\", True),\n",
    "    (\"hello\", False),\n",
    "    (\"\", True),\n",
    "    (\"a\", True),\n",
    "    (\"ab\", False),\n",
    "])\n",
    "def test_is_palindrome(input, expected):\n",
    "    assert is_palindrome(input) == expected\n",
    "\n",
    "# Named test IDs for clarity\n",
    "@pytest.mark.parametrize(\"input,expected\", [\n",
    "    (\"racecar\", True),\n",
    "    (\"hello\", False),\n",
    "], ids=[\"palindrome\", \"not_palindrome\"])\n",
    "def test_with_ids(input, expected):\n",
    "    assert is_palindrome(input) == expected\n",
    "\n",
    "# Complex example from Odibi\n",
    "@pytest.mark.parametrize(\"connection_type,format,expected_class\", [\n",
    "    (\"local\", \"csv\", \"PandasEngine\"),\n",
    "    (\"local\", \"parquet\", \"PandasEngine\"),\n",
    "    (\"azure\", \"csv\", \"PandasEngine\"),\n",
    "])\n",
    "def test_engine_selection(connection_type, format, expected_class):\n",
    "    # Simulate engine selection logic\n",
    "    engine = expected_class\n",
    "    assert engine == expected_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_parametrize.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 4: Mocking (Isolate External Dependencies)\n",
    "\n",
    "**Why mock?** Unit tests should be:\n",
    "1. **Fast** - No waiting for networks, databases, or file I/O\n",
    "2. **Deterministic** - Same result every time (no random failures)\n",
    "3. **Isolated** - Test YOUR code, not external systems\n",
    "4. **Safe** - Don't send real emails, charge credit cards, or delete files!\n",
    "\n",
    "**What to mock**:\n",
    "- âœ… External APIs (requests, httpx)\n",
    "- âœ… Databases (SQLAlchemy, psycopg2)\n",
    "- âœ… File system (os.path.exists, open)\n",
    "- âœ… Time (time.time, datetime.now)\n",
    "- âœ… Random (random.random)\n",
    "- âœ… Environment variables (os.getenv)\n",
    "\n",
    "**What NOT to mock**:\n",
    "- âŒ Your own code (defeats the purpose of testing)\n",
    "- âŒ Simple data structures (just use real ones)\n",
    "- âŒ Pure functions (they're already deterministic)\n",
    "\n",
    "**Mocking tools**:\n",
    "- `monkeypatch` (pytest fixture) - Easiest for simple cases\n",
    "- `unittest.mock` (stdlib) - More power, more complex\n",
    "- `pytest-mock` (plugin) - Combines both (requires `pip install pytest-mock`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_mocking.py\n",
    "\"\"\"Mocking examples.\"\"\"\n",
    "import pytest\n",
    "from unittest.mock import Mock, MagicMock\n",
    "\n",
    "# Function that calls external API\n",
    "def fetch_user(user_id):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://api.example.com/users/{user_id}\")\n",
    "    return response.json()\n",
    "\n",
    "# Test with monkeypatch\n",
    "def test_fetch_user_monkeypatch(monkeypatch):\n",
    "    \"\"\"Mock requests without hitting real API.\"\"\"\n",
    "    def mock_get(url):\n",
    "        mock_response = Mock()\n",
    "        mock_response.json.return_value = {\"id\": 1, \"name\": \"Alice\"}\n",
    "        return mock_response\n",
    "    \n",
    "    # Replace requests.get with mock\n",
    "    import requests\n",
    "    monkeypatch.setattr(requests, \"get\", mock_get)\n",
    "    \n",
    "    result = fetch_user(1)\n",
    "    assert result[\"name\"] == \"Alice\"\n",
    "\n",
    "# Mock environment variables\n",
    "def get_api_key():\n",
    "    import os\n",
    "    return os.getenv(\"API_KEY\")\n",
    "\n",
    "def test_api_key_env(monkeypatch):\n",
    "    monkeypatch.setenv(\"API_KEY\", \"test-key-123\")\n",
    "    assert get_api_key() == \"test-key-123\"\n",
    "\n",
    "# Mock file system\n",
    "def test_file_exists(monkeypatch, tmp_path):\n",
    "    \"\"\"Mock Path.exists().\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    def mock_exists(self):\n",
    "        return True\n",
    "    \n",
    "    monkeypatch.setattr(Path, \"exists\", mock_exists)\n",
    "    \n",
    "    # Any path now \"exists\"\n",
    "    assert Path(\"/fake/path\").exists() == True\n",
    "\n",
    "# Mock class method\n",
    "class DataLoader:\n",
    "    def load(self, path):\n",
    "        # Expensive operation\n",
    "        import pandas as pd\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "def test_data_loader(monkeypatch):\n",
    "    import pandas as pd\n",
    "    \n",
    "    def mock_load(self, path):\n",
    "        return pd.DataFrame({\"a\": [1, 2]})\n",
    "    \n",
    "    monkeypatch.setattr(DataLoader, \"load\", mock_load)\n",
    "    \n",
    "    loader = DataLoader()\n",
    "    df = loader.load(\"/fake/path.csv\")\n",
    "    assert len(df) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_mocking.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 5: Coverage (Measure Test Completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile math_utils.py\n",
    "\"\"\"Math utilities to test coverage.\"\"\"\n",
    "\n",
    "def divide(a, b):\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    return a / b\n",
    "\n",
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "def process_list(items):\n",
    "    if not items:\n",
    "        return []\n",
    "    \n",
    "    result = []\n",
    "    for item in items:\n",
    "        if item > 0:\n",
    "            result.append(item * 2)\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_math_utils.py\n",
    "\"\"\"Tests for math_utils.\"\"\"\n",
    "import pytest\n",
    "from math_utils import divide, is_even, process_list\n",
    "\n",
    "def test_divide():\n",
    "    assert divide(10, 2) == 5\n",
    "\n",
    "def test_divide_zero():\n",
    "    with pytest.raises(ValueError):\n",
    "        divide(10, 0)\n",
    "\n",
    "def test_is_even():\n",
    "    assert is_even(4) == True\n",
    "    assert is_even(3) == False\n",
    "\n",
    "def test_process_list_empty():\n",
    "    assert process_list([]) == []\n",
    "\n",
    "def test_process_list_positive():\n",
    "    assert process_list([1, 2, 3]) == [2, 4, 6]\n",
    "\n",
    "# Missing: test with negative numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with coverage report\n",
    "!pytest test_math_utils.py --cov=math_utils --cov-report=term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage shows:\n",
    "- Which lines executed\n",
    "- Missing branches (if/else not tested)\n",
    "- Percentage covered\n",
    "\n",
    "**Target**: 80-90% coverage (100% not always practical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 6: Markers and conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_markers.py\n",
    "\"\"\"Marker examples.\"\"\"\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "# Skip test\n",
    "@pytest.mark.skip(reason=\"Not implemented yet\")\n",
    "def test_future_feature():\n",
    "    assert False\n",
    "\n",
    "# Conditional skip\n",
    "@pytest.mark.skipif(sys.platform == \"win32\", reason=\"Fails on Windows\")\n",
    "def test_unix_only():\n",
    "    assert True\n",
    "\n",
    "# Expected to fail\n",
    "@pytest.mark.xfail(reason=\"Known bug #123\")\n",
    "def test_known_bug():\n",
    "    assert 1 == 2\n",
    "\n",
    "# Custom markers (group tests)\n",
    "@pytest.mark.slow\n",
    "def test_slow_operation():\n",
    "    import time\n",
    "    time.sleep(0.1)\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_database_integration():\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.unit\n",
    "def test_fast_unit():\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only specific markers\n",
    "!pytest test_markers.py -v -m \"unit\"  # Only unit tests\n",
    "!pytest test_markers.py -v -m \"not slow\"  # Skip slow tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conftest.py (Shared Fixtures)\n",
    "\n",
    "Create `conftest.py` in test directory for shared fixtures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conftest.py\n",
    "\"\"\"Shared test fixtures and configuration.\"\"\"\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_df():\n",
    "    \"\"\"Fixture available to all tests.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"value\": [10, 20, 30]\n",
    "    })\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def test_config():\n",
    "    \"\"\"Config created once per test session.\"\"\"\n",
    "    return {\n",
    "        \"env\": \"test\",\n",
    "        \"debug\": True\n",
    "    }\n",
    "\n",
    "# Register custom markers\n",
    "def pytest_configure(config):\n",
    "    config.addinivalue_line(\"markers\", \"slow: marks tests as slow\")\n",
    "    config.addinivalue_line(\"markers\", \"integration: integration tests\")\n",
    "    config.addinivalue_line(\"markers\", \"unit: unit tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Part 7: Testing Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_exceptions.py\n",
    "\"\"\"Exception testing examples.\"\"\"\n",
    "import pytest\n",
    "\n",
    "def validate_age(age):\n",
    "    if age < 0:\n",
    "        raise ValueError(\"Age cannot be negative\")\n",
    "    if age > 150:\n",
    "        raise ValueError(\"Age too high\")\n",
    "    return age\n",
    "\n",
    "# Basic exception test\n",
    "def test_negative_age():\n",
    "    with pytest.raises(ValueError):\n",
    "        validate_age(-1)\n",
    "\n",
    "# Check exception message\n",
    "def test_negative_age_message():\n",
    "    with pytest.raises(ValueError, match=\"cannot be negative\"):\n",
    "        validate_age(-5)\n",
    "\n",
    "# Capture exception for inspection\n",
    "def test_exception_details():\n",
    "    with pytest.raises(ValueError) as exc_info:\n",
    "        validate_age(200)\n",
    "    \n",
    "    assert \"too high\" in str(exc_info.value)\n",
    "    assert exc_info.type == ValueError\n",
    "\n",
    "# Test multiple exceptions\n",
    "@pytest.mark.parametrize(\"age,expected_msg\", [\n",
    "    (-1, \"cannot be negative\"),\n",
    "    (200, \"too high\"),\n",
    "])\n",
    "def test_validation_errors(age, expected_msg):\n",
    "    with pytest.raises(ValueError, match=expected_msg):\n",
    "        validate_age(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_exceptions.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Part 8: Odibi Testing Patterns Analysis\n",
    "\n",
    "Let's analyze how Odibi's 416 tests are organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odibi test structure\n",
    "odibi_structure = \"\"\"\n",
    "tests/\n",
    "â”œâ”€â”€ unit/                    # 15 test files (unit tests)\n",
    "â”‚   â”œâ”€â”€ test_operations.py\n",
    "â”‚   â”œâ”€â”€ test_cli.py\n",
    "â”‚   â”œâ”€â”€ test_registry.py\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ integration/             # 1 test file (integration tests)\n",
    "â”‚   â””â”€â”€ test_cli_integration.py\n",
    "â”œâ”€â”€ fixtures/                # Test data\n",
    "â”‚   â””â”€â”€ sample_data.csv\n",
    "â”œâ”€â”€ test_pipeline.py         # Core pipeline tests\n",
    "â”œâ”€â”€ test_config.py           # Configuration tests\n",
    "â”œâ”€â”€ test_connections_paths.py # Connection tests\n",
    "â””â”€â”€ conftest.py (likely)     # Shared fixtures\n",
    "\"\"\"\n",
    "print(odibi_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Patterns from Odibi Tests\n",
    "\n",
    "**1. Class-based test organization** (from `test_pipeline.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_odibi_pattern1.py\n",
    "\"\"\"Pattern 1: Class-based organization.\"\"\"\n",
    "import pytest\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "class TestPipelineExecution:\n",
    "    \"\"\"Group related pipeline tests.\"\"\"\n",
    "    \n",
    "    def setup_method(self):\n",
    "        \"\"\"Run before each test method.\"\"\"\n",
    "        self.test_dir = tempfile.mkdtemp()\n",
    "        self.test_path = Path(self.test_dir)\n",
    "        print(f\"\\nSetup: {self.test_dir}\")\n",
    "    \n",
    "    def teardown_method(self):\n",
    "        \"\"\"Run after each test method.\"\"\"\n",
    "        import shutil\n",
    "        shutil.rmtree(self.test_dir)\n",
    "        print(f\"\\nTeardown: {self.test_dir}\")\n",
    "    \n",
    "    def test_read_pipeline(self):\n",
    "        assert self.test_path.exists()\n",
    "    \n",
    "    def test_write_pipeline(self):\n",
    "        assert self.test_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Parametrize for connection types** (from `test_pandas_engine_full_coverage.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_odibi_pattern2.py\n",
    "\"\"\"Pattern 2: Parametrize connections and formats.\"\"\"\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"format,options\", [\n",
    "    (\"csv\", {\"sep\": \",\"}),\n",
    "    (\"parquet\", {\"engine\": \"pyarrow\"}),\n",
    "    (\"json\", {\"orient\": \"records\"}),\n",
    "])\n",
    "def test_read_formats(format, options):\n",
    "    \"\"\"Test reading different formats.\"\"\"\n",
    "    assert format in [\"csv\", \"parquet\", \"json\"]\n",
    "    assert isinstance(options, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Monkeypatch for optional dependencies** (from `test_extras_imports.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_odibi_pattern3.py\n",
    "\"\"\"Pattern 3: Test missing optional dependencies.\"\"\"\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "@pytest.mark.extras\n",
    "def test_import_without_pyspark(monkeypatch):\n",
    "    \"\"\"Test graceful handling when PySpark not installed.\"\"\"\n",
    "    # Simulate missing pyspark\n",
    "    monkeypatch.setitem(sys.modules, \"pyspark\", None)\n",
    "    \n",
    "    # Code should handle missing dependency\n",
    "    try:\n",
    "        import pyspark\n",
    "        assert pyspark is None\n",
    "    except ImportError:\n",
    "        pass  # Expected\n",
    "\n",
    "@pytest.mark.skipif(\"pyspark\" not in sys.modules, reason=\"PySpark not installed\")\n",
    "def test_with_pyspark():\n",
    "    \"\"\"Only run if PySpark available.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Custom markers for test categories**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Odibi's tests:\n",
    "# @pytest.mark.extras - Tests for optional features\n",
    "# @pytest.mark.skip - Known issues or platform-specific\n",
    "\n",
    "# Run only extra features tests:\n",
    "# pytest -m extras\n",
    "\n",
    "# Skip slow integration tests:\n",
    "# pytest -m \"not integration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Build: Create Test Suite for Mini Pipeline\n",
    "\n",
    "Let's build a complete test suite for a mini data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mini_pipeline.py\n",
    "\"\"\"Mini data pipeline to test.\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.transforms = []\n",
    "    \n",
    "    def add_transform(self, func: Callable):\n",
    "        \"\"\"Add transformation function.\"\"\"\n",
    "        self.transforms.append(func)\n",
    "    \n",
    "    def run(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Execute pipeline.\"\"\"\n",
    "        result = df.copy()\n",
    "        for transform in self.transforms:\n",
    "            result = transform(result)\n",
    "        return result\n",
    "\n",
    "def read_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV file.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def write_csv(df: pd.DataFrame, path: Path):\n",
    "    \"\"\"Write CSV file.\"\"\"\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "# Sample transforms\n",
    "def add_total_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add total column.\"\"\"\n",
    "    if \"quantity\" in df.columns and \"price\" in df.columns:\n",
    "        df[\"total\"] = df[\"quantity\"] * df[\"price\"]\n",
    "    return df\n",
    "\n",
    "def filter_positive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter positive values.\"\"\"\n",
    "    if \"total\" in df.columns:\n",
    "        return df[df[\"total\"] > 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_mini_pipeline.py\n",
    "\"\"\"Complete test suite for mini pipeline.\"\"\"\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mini_pipeline import (\n",
    "    Pipeline, read_csv, write_csv,\n",
    "    add_total_column, filter_positive\n",
    ")\n",
    "\n",
    "# Fixtures\n",
    "@pytest.fixture\n",
    "def sample_df():\n",
    "    \"\"\"Sample DataFrame.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"quantity\": [2, 3, 0],\n",
    "        \"price\": [10, 20, 15]\n",
    "    })\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_csv(tmp_path, sample_df):\n",
    "    \"\"\"Sample CSV file.\"\"\"\n",
    "    csv_path = tmp_path / \"data.csv\"\n",
    "    sample_df.to_csv(csv_path, index=False)\n",
    "    return csv_path\n",
    "\n",
    "# Unit tests\n",
    "class TestPipeline:\n",
    "    def test_create_pipeline(self):\n",
    "        pipeline = Pipeline(\"test\")\n",
    "        assert pipeline.name == \"test\"\n",
    "        assert len(pipeline.transforms) == 0\n",
    "    \n",
    "    def test_add_transform(self):\n",
    "        pipeline = Pipeline(\"test\")\n",
    "        pipeline.add_transform(add_total_column)\n",
    "        assert len(pipeline.transforms) == 1\n",
    "    \n",
    "    def test_run_empty_pipeline(self, sample_df):\n",
    "        pipeline = Pipeline(\"test\")\n",
    "        result = pipeline.run(sample_df)\n",
    "        pd.testing.assert_frame_equal(result, sample_df)\n",
    "    \n",
    "    def test_run_with_transform(self, sample_df):\n",
    "        pipeline = Pipeline(\"test\")\n",
    "        pipeline.add_transform(add_total_column)\n",
    "        result = pipeline.run(sample_df)\n",
    "        assert \"total\" in result.columns\n",
    "\n",
    "# I/O tests\n",
    "class TestIO:\n",
    "    def test_read_csv(self, sample_csv):\n",
    "        df = read_csv(sample_csv)\n",
    "        assert len(df) == 3\n",
    "        assert \"quantity\" in df.columns\n",
    "    \n",
    "    def test_read_nonexistent_file(self, tmp_path):\n",
    "        with pytest.raises(FileNotFoundError, match=\"File not found\"):\n",
    "            read_csv(tmp_path / \"missing.csv\")\n",
    "    \n",
    "    def test_write_csv(self, tmp_path, sample_df):\n",
    "        output_path = tmp_path / \"output.csv\"\n",
    "        write_csv(sample_df, output_path)\n",
    "        assert output_path.exists()\n",
    "        \n",
    "        # Verify contents\n",
    "        df = pd.read_csv(output_path)\n",
    "        pd.testing.assert_frame_equal(df, sample_df)\n",
    "\n",
    "# Transform tests\n",
    "class TestTransforms:\n",
    "    def test_add_total_column(self, sample_df):\n",
    "        result = add_total_column(sample_df)\n",
    "        assert \"total\" in result.columns\n",
    "        assert list(result[\"total\"]) == [20, 60, 0]\n",
    "    \n",
    "    def test_add_total_missing_columns(self):\n",
    "        df = pd.DataFrame({\"other\": [1, 2]})\n",
    "        result = add_total_column(df)\n",
    "        assert \"total\" not in result.columns\n",
    "    \n",
    "    def test_filter_positive(self):\n",
    "        df = pd.DataFrame({\"total\": [10, -5, 0, 20]})\n",
    "        result = filter_positive(df)\n",
    "        assert len(result) == 2\n",
    "        assert list(result[\"total\"]) == [10, 20]\n",
    "\n",
    "# Integration test\n",
    "@pytest.mark.integration\n",
    "def test_full_pipeline_integration(tmp_path):\n",
    "    \"\"\"Test complete pipeline workflow.\"\"\"\n",
    "    # Setup input data\n",
    "    input_path = tmp_path / \"input.csv\"\n",
    "    df = pd.DataFrame({\n",
    "        \"quantity\": [2, 3, -1],\n",
    "        \"price\": [10, 20, 15]\n",
    "    })\n",
    "    write_csv(df, input_path)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(\"full_test\")\n",
    "    pipeline.add_transform(add_total_column)\n",
    "    pipeline.add_transform(filter_positive)\n",
    "    \n",
    "    # Execute\n",
    "    input_df = read_csv(input_path)\n",
    "    result = pipeline.run(input_df)\n",
    "    \n",
    "    # Verify\n",
    "    assert len(result) == 2  # Filtered negative\n",
    "    assert \"total\" in result.columns\n",
    "    \n",
    "    # Write output\n",
    "    output_path = tmp_path / \"output.csv\"\n",
    "    write_csv(result, output_path)\n",
    "    assert output_path.exists()\n",
    "\n",
    "# Parametrized tests\n",
    "@pytest.mark.parametrize(\"quantity,price,expected\", [\n",
    "    (2, 10, 20),\n",
    "    (3, 20, 60),\n",
    "    (0, 15, 0),\n",
    "    (-1, 10, -10),\n",
    "])\n",
    "def test_total_calculation(quantity, price, expected):\n",
    "    df = pd.DataFrame({\"quantity\": [quantity], \"price\": [price]})\n",
    "    result = add_total_column(df)\n",
    "    assert result[\"total\"].iloc[0] == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Validate: Run Tests and Check Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests with verbose output\n",
    "!pytest test_mini_pipeline.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with coverage\n",
    "!pytest test_mini_pipeline.py --cov=mini_pipeline --cov-report=term-missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only unit tests\n",
    "!pytest test_mini_pipeline.py -v -m \"not integration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HTML coverage report\n",
    "!pytest test_mini_pipeline.py --cov=mini_pipeline --cov-report=html\n",
    "print(\"\\nOpen htmlcov/index.html to see detailed coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary: Pytest Mastery Checklist\n",
    "\n",
    "### You've learned:\n",
    "\n",
    "âœ… **Basics**: Assertions, test discovery, running tests\n",
    "\n",
    "âœ… **Fixtures**: Setup/teardown, scope, `tmp_path`, shared fixtures\n",
    "\n",
    "âœ… **Parametrize**: Data-driven tests, multiple inputs\n",
    "\n",
    "âœ… **Mocking**: `monkeypatch`, isolate dependencies\n",
    "\n",
    "âœ… **Coverage**: Measure completeness, find gaps\n",
    "\n",
    "âœ… **Markers**: Skip, xfail, custom markers, organize tests\n",
    "\n",
    "âœ… **Exceptions**: Test error handling\n",
    "\n",
    "âœ… **Odibi patterns**: Class-based tests, fixtures, real-world examples\n",
    "\n",
    "### Key Commands:\n",
    "\n",
    "```bash\n",
    "# Run all tests\n",
    "pytest\n",
    "\n",
    "# Verbose output\n",
    "pytest -v\n",
    "\n",
    "# Run specific file\n",
    "pytest test_file.py\n",
    "\n",
    "# Run specific test\n",
    "pytest test_file.py::test_name\n",
    "\n",
    "# Run with markers\n",
    "pytest -m \"unit\"\n",
    "pytest -m \"not slow\"\n",
    "\n",
    "# Coverage\n",
    "pytest --cov=module --cov-report=term-missing\n",
    "pytest --cov=module --cov-report=html\n",
    "\n",
    "# Show print statements\n",
    "pytest -s\n",
    "\n",
    "# Stop on first failure\n",
    "pytest -x\n",
    "\n",
    "# Run last failed tests\n",
    "pytest --lf\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Complete exercises in `exercises.ipynb`\n",
    "2. Read Odibi test analysis in `odibi_test_analysis.md`\n",
    "3. Practice: Write tests for your own code\n",
    "4. Explore advanced topics: `pytest-asyncio`, `pytest-benchmark`, `hypothesis`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
