{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Pytest Fundamentals - Exercises\n",
    "\n",
    "Practice your pytest skills with these hands-on exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Assertions (Easy)\n",
    "\n",
    "Write tests for the following string utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_string(s: str) -> str:\n",
    "    \"\"\"Reverse a string.\"\"\"\n",
    "    return s[::-1]\n",
    "\n",
    "def is_palindrome(s: str) -> bool:\n",
    "    \"\"\"Check if string is palindrome (case-insensitive).\"\"\"\n",
    "    s_clean = s.lower().replace(\" \", \"\")\n",
    "    return s_clean == s_clean[::-1]\n",
    "\n",
    "def count_vowels(s: str) -> int:\n",
    "    \"\"\"Count vowels in string.\"\"\"\n",
    "    return sum(1 for c in s.lower() if c in 'aeiou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Write at least 3 tests for each function\n",
    "2. Include edge cases (empty strings, special characters)\n",
    "3. Run tests with `pytest test_strings.py -v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_strings.py\n",
    "# TODO: Write your tests here\n",
    "\n",
    "def test_reverse_string():\n",
    "    pass  # Your code here\n",
    "\n",
    "# Add more tests..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Fixtures (Medium)\n",
    "\n",
    "Create fixtures for testing a shopping cart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "    \n",
    "    def add_item(self, name: str, price: float, quantity: int = 1):\n",
    "        self.items.append({\"name\": name, \"price\": price, \"quantity\": quantity})\n",
    "    \n",
    "    def total(self) -> float:\n",
    "        return sum(item[\"price\"] * item[\"quantity\"] for item in self.items)\n",
    "    \n",
    "    def item_count(self) -> int:\n",
    "        return sum(item[\"quantity\"] for item in self.items)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.items.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Create `empty_cart` fixture\n",
    "2. Create `cart_with_items` fixture with pre-populated items\n",
    "3. Write tests using both fixtures\n",
    "4. Use different fixture scopes appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_cart.py\n",
    "import pytest\n",
    "\n",
    "# TODO: Create your fixtures here\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_cart():\n",
    "    pass  # Your code here\n",
    "\n",
    "# TODO: Write tests using fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Parametrize (Medium)\n",
    "\n",
    "Use `@pytest.mark.parametrize` to test this calculator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(operation: str, a: float, b: float) -> float:\n",
    "    \"\"\"Perform calculation.\"\"\"\n",
    "    if operation == \"add\":\n",
    "        return a + b\n",
    "    elif operation == \"subtract\":\n",
    "        return a - b\n",
    "    elif operation == \"multiply\":\n",
    "        return a * b\n",
    "    elif operation == \"divide\":\n",
    "        if b == 0:\n",
    "            raise ValueError(\"Cannot divide by zero\")\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown operation: {operation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Parametrize test for all valid operations (add, subtract, multiply, divide)\n",
    "2. Test at least 5 different input combinations per operation\n",
    "3. Use named test IDs for clarity\n",
    "4. Separate parametrized test for error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_calculator.py\n",
    "import pytest\n",
    "\n",
    "# TODO: Write parametrized tests\n",
    "\n",
    "@pytest.mark.parametrize(\"operation,a,b,expected\", [\n",
    "    # Your test cases here\n",
    "])\n",
    "def test_calculate(operation, a, b, expected):\n",
    "    pass  # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Mocking (Medium-Hard)\n",
    "\n",
    "Mock external dependencies for this weather service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "class WeatherService:\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "        self.base_url = \"https://api.weather.com\"\n",
    "    \n",
    "    def get_temperature(self, city: str) -> float:\n",
    "        \"\"\"Get current temperature for city.\"\"\"\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"API key not configured\")\n",
    "        \n",
    "        url = f\"{self.base_url}/current?city={city}&key={self.api_key}\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        return data[\"temperature\"]\n",
    "    \n",
    "    def is_cold(self, city: str) -> bool:\n",
    "        \"\"\"Check if temperature is below 10Â°C.\"\"\"\n",
    "        temp = self.get_temperature(city)\n",
    "        return temp < 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Mock `os.getenv()` to provide fake API key\n",
    "2. Mock `requests.get()` to return fake weather data\n",
    "3. Test successful API call\n",
    "4. Test missing API key error\n",
    "5. Test `is_cold()` method with different temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_weather.py\n",
    "import pytest\n",
    "from unittest.mock import Mock\n",
    "\n",
    "# TODO: Write tests with mocking\n",
    "\n",
    "def test_get_temperature_success(monkeypatch):\n",
    "    pass  # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Coverage Challenge (Hard)\n",
    "\n",
    "Achieve **90%+ coverage** on this data validator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_validator.py\n",
    "\"\"\"Data validation module.\"\"\"\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class DataValidator:\n",
    "    def __init__(self, rules: Dict[str, Any]):\n",
    "        self.rules = rules\n",
    "    \n",
    "    def validate_dataframe(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Validate DataFrame against rules. Returns list of errors.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check required columns\n",
    "        if \"required_columns\" in self.rules:\n",
    "            for col in self.rules[\"required_columns\"]:\n",
    "                if col not in df.columns:\n",
    "                    errors.append(f\"Missing required column: {col}\")\n",
    "        \n",
    "        # Check data types\n",
    "        if \"column_types\" in self.rules:\n",
    "            for col, expected_type in self.rules[\"column_types\"].items():\n",
    "                if col in df.columns:\n",
    "                    if df[col].dtype != expected_type:\n",
    "                        errors.append(f\"Column {col} has wrong type: {df[col].dtype}\")\n",
    "        \n",
    "        # Check value ranges\n",
    "        if \"value_ranges\" in self.rules:\n",
    "            for col, (min_val, max_val) in self.rules[\"value_ranges\"].items():\n",
    "                if col in df.columns:\n",
    "                    if df[col].min() < min_val:\n",
    "                        errors.append(f\"Column {col} has values below {min_val}\")\n",
    "                    if df[col].max() > max_val:\n",
    "                        errors.append(f\"Column {col} has values above {max_val}\")\n",
    "        \n",
    "        # Check no nulls\n",
    "        if \"no_nulls\" in self.rules:\n",
    "            for col in self.rules[\"no_nulls\"]:\n",
    "                if col in df.columns:\n",
    "                    if df[col].isnull().any():\n",
    "                        errors.append(f\"Column {col} contains null values\")\n",
    "        \n",
    "        # Check unique values\n",
    "        if \"unique_columns\" in self.rules:\n",
    "            for col in self.rules[\"unique_columns\"]:\n",
    "                if col in df.columns:\n",
    "                    if df[col].duplicated().any():\n",
    "                        errors.append(f\"Column {col} contains duplicates\")\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def is_valid(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Check if DataFrame is valid.\"\"\"\n",
    "        return len(self.validate_dataframe(df)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Write comprehensive test suite\n",
    "2. Test each validation rule (required columns, types, ranges, nulls, unique)\n",
    "3. Test combinations of rules\n",
    "4. Run coverage: `pytest test_validator.py --cov=data_validator --cov-report=term-missing`\n",
    "5. Add tests until coverage >= 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_validator.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from data_validator import DataValidator\n",
    "\n",
    "# TODO: Write comprehensive tests for 90%+ coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Integration Test (Hard)\n",
    "\n",
    "Build a complete test suite for this ETL pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile etl_pipeline.py\n",
    "\"\"\"Simple ETL pipeline.\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Callable, List\n",
    "\n",
    "class ETLPipeline:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.transforms: List[Callable] = []\n",
    "    \n",
    "    def extract(self, source: Path) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from source.\"\"\"\n",
    "        if source.suffix == \".csv\":\n",
    "            return pd.read_csv(source)\n",
    "        elif source.suffix == \".json\":\n",
    "            return pd.read_json(source)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {source.suffix}\")\n",
    "    \n",
    "    def add_transform(self, func: Callable) -> None:\n",
    "        \"\"\"Add transformation function.\"\"\"\n",
    "        self.transforms.append(func)\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply all transformations.\"\"\"\n",
    "        result = df.copy()\n",
    "        for func in self.transforms:\n",
    "            result = func(result)\n",
    "        return result\n",
    "    \n",
    "    def load(self, df: pd.DataFrame, destination: Path) -> None:\n",
    "        \"\"\"Load data to destination.\"\"\"\n",
    "        if destination.suffix == \".csv\":\n",
    "            df.to_csv(destination, index=False)\n",
    "        elif destination.suffix == \".json\":\n",
    "            df.to_json(destination, orient=\"records\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {destination.suffix}\")\n",
    "    \n",
    "    def run(self, source: Path, destination: Path) -> pd.DataFrame:\n",
    "        \"\"\"Run complete ETL pipeline.\"\"\"\n",
    "        df = self.extract(source)\n",
    "        df = self.transform(df)\n",
    "        self.load(df, destination)\n",
    "        return df\n",
    "\n",
    "# Example transforms\n",
    "def uppercase_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"name\" in df.columns:\n",
    "        df[\"name\"] = df[\"name\"].str.upper()\n",
    "    return df\n",
    "\n",
    "def add_full_name(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"first_name\" in df.columns and \"last_name\" in df.columns:\n",
    "        df[\"full_name\"] = df[\"first_name\"] + \" \" + df[\"last_name\"]\n",
    "    return df\n",
    "\n",
    "def filter_active(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"status\" in df.columns:\n",
    "        return df[df[\"status\"] == \"active\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Write unit tests for each method\n",
    "2. Write tests for transform functions\n",
    "3. Write integration test for full `run()` pipeline\n",
    "4. Use fixtures for test data files\n",
    "5. Parametrize tests for different file formats (CSV, JSON)\n",
    "6. Test error cases (missing files, invalid formats)\n",
    "7. Achieve 85%+ coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_etl_pipeline.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from etl_pipeline import ETLPipeline, uppercase_names, add_full_name, filter_active\n",
    "\n",
    "# TODO: Write comprehensive test suite\n",
    "\n",
    "# Fixtures\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    pass  # Your code here\n",
    "\n",
    "# Unit tests\n",
    "class TestETLPipeline:\n",
    "    pass  # Your tests here\n",
    "\n",
    "# Integration test\n",
    "@pytest.mark.integration\n",
    "def test_full_pipeline(tmp_path):\n",
    "    pass  # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Odibi-Style Refactoring (Advanced)\n",
    "\n",
    "Refactor your Exercise 6 tests using patterns from Odibi:\n",
    "\n",
    "**Tasks**:\n",
    "1. Organize tests into classes by feature\n",
    "2. Use `setup_method()` and `teardown_method()`\n",
    "3. Create `conftest.py` with shared fixtures\n",
    "4. Add custom markers (`@pytest.mark.slow`, `@pytest.mark.integration`)\n",
    "5. Use parametrize for format variations\n",
    "6. Add docstrings to all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conftest.py\n",
    "\"\"\"Shared test fixtures.\"\"\"\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Add shared fixtures\n",
    "\n",
    "# Register markers\n",
    "def pytest_configure(config):\n",
    "    config.addinivalue_line(\"markers\", \"slow: slow tests\")\n",
    "    config.addinivalue_line(\"markers\", \"integration: integration tests\")\n",
    "    config.addinivalue_line(\"markers\", \"unit: unit tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Bonus Challenge: Test-Driven Development (TDD)\n",
    "\n",
    "Practice TDD by implementing a new feature **tests-first**:\n",
    "\n",
    "**Feature**: Add a `validate()` method to `ETLPipeline` that checks:\n",
    "- Required columns exist\n",
    "- No duplicate rows\n",
    "- No null values in specified columns\n",
    "\n",
    "**Steps**:\n",
    "1. Write tests for `validate()` method (it doesn't exist yet!)\n",
    "2. Run tests - they should fail\n",
    "3. Implement `validate()` method\n",
    "4. Run tests - they should pass\n",
    "5. Refactor code while keeping tests green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_etl_validation.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from etl_pipeline import ETLPipeline\n",
    "\n",
    "# TODO: Write tests FIRST, then implement validate() method\n",
    "\n",
    "def test_validate_required_columns():\n",
    "    \"\"\"Test that validate() checks for required columns.\"\"\"\n",
    "    pipeline = ETLPipeline(\"test\")\n",
    "    df = pd.DataFrame({\"a\": [1, 2]})\n",
    "    \n",
    "    # Should fail - missing column 'b'\n",
    "    is_valid, errors = pipeline.validate(df, required_columns=[\"a\", \"b\"])\n",
    "    assert not is_valid\n",
    "    assert \"Missing required column: b\" in errors\n",
    "\n",
    "# TODO: Add more tests..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Completion Checklist\n",
    "\n",
    "- [ ] Exercise 1: Basic assertions (3+ tests per function)\n",
    "- [ ] Exercise 2: Fixtures (2+ fixtures, multiple tests)\n",
    "- [ ] Exercise 3: Parametrize (5+ cases per operation)\n",
    "- [ ] Exercise 4: Mocking (5+ tests with mocks)\n",
    "- [ ] Exercise 5: Coverage (90%+ on data_validator.py)\n",
    "- [ ] Exercise 6: Integration (complete test suite, 85%+ coverage)\n",
    "- [ ] Exercise 7: Odibi-style refactoring\n",
    "- [ ] Bonus: TDD challenge\n",
    "\n",
    "## ðŸ“Š Self-Assessment\n",
    "\n",
    "Run this to check your progress:\n",
    "\n",
    "```bash\n",
    "# Run all tests\n",
    "pytest -v\n",
    "\n",
    "# Check coverage\n",
    "pytest --cov=. --cov-report=term-missing\n",
    "\n",
    "# Run only unit tests\n",
    "pytest -m \"unit\" -v\n",
    "\n",
    "# Run only integration tests\n",
    "pytest -m \"integration\" -v\n",
    "```\n",
    "\n",
    "**Target metrics**:\n",
    "- All tests passing âœ…\n",
    "- Overall coverage: 85%+\n",
    "- Test execution: < 5 seconds\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check `solutions.ipynb` for complete solutions with explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
