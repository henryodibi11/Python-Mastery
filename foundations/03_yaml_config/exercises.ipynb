{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAML Configuration - Exercises\n",
    "\n",
    "Practice what you've learned! Solutions in [solutions.ipynb](solutions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Fix the Broken YAML\n",
    "\n",
    "This YAML has **5 errors**. Find and fix them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "broken_yaml = \"\"\"\n",
    "project: Data Pipeline\n",
    "version: 1.0  # Should be string, not float\n",
    "enabled: YES  # Should be lowercase true/false\n",
    "\n",
    "connections\n",
    "  database:  # Missing colon above\n",
    "    host: localhost\n",
    "    port: \"5432\"  # Should be int, not string\n",
    "  storage:\n",
    "  type: local  # Wrong indentation\n",
    "    path: /data\n",
    "\n",
    "countries:\n",
    "  - NO  # Norway problem - becomes False!\n",
    "  - SE\n",
    "\"\"\"\n",
    "\n",
    "# Fix it here:\n",
    "fixed_yaml = \"\"\"\n",
    "# Your fixed YAML here\n",
    "\"\"\"\n",
    "\n",
    "# Test\n",
    "data = yaml.safe_load(fixed_yaml)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Use Anchors to DRY Up Config\n",
    "\n",
    "Refactor this repetitive config using anchors & aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitive_yaml = \"\"\"\n",
    "dev_database:\n",
    "  host: dev.db.com\n",
    "  port: 5432\n",
    "  timeout: 30\n",
    "  pool_size: 5\n",
    "  retry_attempts: 3\n",
    "\n",
    "staging_database:\n",
    "  host: staging.db.com\n",
    "  port: 5432\n",
    "  timeout: 30\n",
    "  pool_size: 5\n",
    "  retry_attempts: 3\n",
    "\n",
    "prod_database:\n",
    "  host: prod.db.com\n",
    "  port: 5432\n",
    "  timeout: 60  # Only difference: longer timeout\n",
    "  pool_size: 10  # And larger pool\n",
    "  retry_attempts: 3\n",
    "\"\"\"\n",
    "\n",
    "# Refactor using anchors:\n",
    "refactored_yaml = \"\"\"\n",
    "# Your refactored YAML here using &anchor and *alias\n",
    "\"\"\"\n",
    "\n",
    "# Test\n",
    "data = yaml.safe_load(refactored_yaml)\n",
    "assert data['dev_database']['port'] == 5432\n",
    "assert data['prod_database']['timeout'] == 60\n",
    "print(\"✅ Refactored successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Create a Pydantic Schema\n",
    "\n",
    "Write a Pydantic model to validate this Odibi-style pipeline config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional, Literal\n",
    "\n",
    "pipeline_yaml = \"\"\"\n",
    "project: Sales ETL\n",
    "engine: pandas\n",
    "\n",
    "connections:\n",
    "  data:\n",
    "    type: local\n",
    "    base_path: ./data\n",
    "\n",
    "retry:\n",
    "  max_attempts: 3\n",
    "  backoff_seconds: 2.0\n",
    "\n",
    "pipelines:\n",
    "  - pipeline: bronze_to_silver\n",
    "    nodes:\n",
    "      - name: load_sales\n",
    "        operation: read\n",
    "      - name: clean_sales\n",
    "        operation: transform\n",
    "        depends_on: [load_sales]\n",
    "\"\"\"\n",
    "\n",
    "# Define your schemas:\n",
    "class ConnectionConfig(BaseModel):\n",
    "    # TODO: Add fields\n",
    "    pass\n",
    "\n",
    "class RetryConfig(BaseModel):\n",
    "    # TODO: Add fields with validation\n",
    "    pass\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    # TODO: Add fields\n",
    "    pass\n",
    "\n",
    "class PipelineConfig(BaseModel):\n",
    "    # TODO: Add fields\n",
    "    pass\n",
    "\n",
    "class AppConfig(BaseModel):\n",
    "    # TODO: Add fields\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "data = yaml.safe_load(pipeline_yaml)\n",
    "config = AppConfig(**data)\n",
    "print(config.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Environment Variable Substitution\n",
    "\n",
    "Implement a function to load YAML with environment variable substitution AND defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def load_yaml_with_env(yaml_string: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load YAML and replace:\n",
    "    - ${VAR} with environment variable\n",
    "    - ${VAR:-default} with environment variable OR default if not set\n",
    "    \n",
    "    Example:\n",
    "      host: ${DB_HOST:-localhost}  # Uses localhost if DB_HOST not set\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "os.environ['DB_HOST'] = 'production.db.com'\n",
    "# DB_PORT not set - should use default\n",
    "\n",
    "test_yaml = \"\"\"\n",
    "database:\n",
    "  host: ${DB_HOST}\n",
    "  port: ${DB_PORT:-5432}\n",
    "  timeout: ${TIMEOUT:-30}\n",
    "\"\"\"\n",
    "\n",
    "config = load_yaml_with_env(test_yaml)\n",
    "assert config['database']['host'] == 'production.db.com'\n",
    "assert config['database']['port'] == 5432  # Default used\n",
    "assert config['database']['timeout'] == 30\n",
    "print(\"✅ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Multi-Environment Configs\n",
    "\n",
    "Create a system to load environment-specific configs (dev/staging/prod)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_config_for_env(base_path: Path, env: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load config with inheritance:\n",
    "    1. Load base.yaml (shared settings)\n",
    "    2. Load {env}.yaml (environment-specific)\n",
    "    3. Merge them (env overrides base)\n",
    "    \n",
    "    Example structure:\n",
    "      config/\n",
    "        base.yaml\n",
    "        dev.yaml\n",
    "        prod.yaml\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Hint: Use dict update or recursive merge\n",
    "    pass\n",
    "\n",
    "# Test (create test files first!)\n",
    "config_dir = Path('example_configs/multi_env')\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create base.yaml\n",
    "base_config = \"\"\"\n",
    "project: My Project\n",
    "timeout: 30\n",
    "log_level: INFO\n",
    "\"\"\"\n",
    "\n",
    "# Create prod.yaml\n",
    "prod_config = \"\"\"\n",
    "timeout: 60  # Override\n",
    "log_level: WARNING  # Override\n",
    "replicas: 3  # New field\n",
    "\"\"\"\n",
    "\n",
    "config = load_config_for_env(config_dir, 'prod')\n",
    "assert config['timeout'] == 60  # Overridden\n",
    "assert config['project'] == 'My Project'  # From base\n",
    "assert config['replicas'] == 3  # From prod\n",
    "print(\"✅ Multi-environment config works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Analyze Odibi Config\n",
    "\n",
    "Write code to extract insights from Odibi's YAML configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_odibi_config(yaml_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze an Odibi pipeline config and return:\n",
    "    - Total number of pipelines\n",
    "    - Total number of nodes\n",
    "    - Connection types used\n",
    "    - Formats used (csv, parquet, delta, etc.)\n",
    "    - Nodes with dependencies (depends_on)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test with real Odibi config\n",
    "odibi_path = r'c:\\Users\\hodibi\\OneDrive - Ingredion\\Desktop\\Repos\\Odibi\\examples\\example_delta_pipeline.yaml'\n",
    "analysis = analyze_odibi_config(odibi_path)\n",
    "\n",
    "print(\"Odibi Config Analysis:\")\n",
    "print(f\"  Pipelines: {analysis['total_pipelines']}\")\n",
    "print(f\"  Nodes: {analysis['total_nodes']}\")\n",
    "print(f\"  Connection types: {analysis['connection_types']}\")\n",
    "print(f\"  Formats: {analysis['formats']}\")\n",
    "print(f\"  Nodes with dependencies: {analysis['nodes_with_deps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Config Linter\n",
    "\n",
    "Build a linter that checks for common YAML config mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lint_yaml_config(yaml_path: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Check for common issues:\n",
    "    - Hardcoded passwords/secrets\n",
    "    - Missing required fields\n",
    "    - Inconsistent naming (snake_case vs camelCase)\n",
    "    - Unquoted values that might be misinterpreted (NO, YES, 1.0)\n",
    "    - TODO/FIXME comments\n",
    "    \n",
    "    Returns list of warnings.\n",
    "    \"\"\"\n",
    "    warnings = []\n",
    "    # TODO: Implement checks\n",
    "    return warnings\n",
    "\n",
    "# Test\n",
    "warnings = lint_yaml_config(Path('example_configs/basic.yaml'))\n",
    "for warning in warnings:\n",
    "    print(f\"⚠️  {warning}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
