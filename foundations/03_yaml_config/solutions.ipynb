{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAML Configuration - Solutions\n",
    "\n",
    "Solutions to exercises from [exercises.ipynb](exercises.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Fix the Broken YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "fixed_yaml = \"\"\"\n",
    "project: Data Pipeline\n",
    "version: \"1.0\"  # ‚úÖ Fixed: Quote to keep as string\n",
    "enabled: true   # ‚úÖ Fixed: Use lowercase true\n",
    "\n",
    "connections:    # ‚úÖ Fixed: Added colon\n",
    "  database:\n",
    "    host: localhost\n",
    "    port: 5432  # ‚úÖ Fixed: Removed quotes to make it int\n",
    "  storage:\n",
    "    type: local  # ‚úÖ Fixed: Proper indentation\n",
    "    path: /data\n",
    "\n",
    "countries:\n",
    "  - \"NO\"  # ‚úÖ Fixed: Quoted to prevent boolean conversion\n",
    "  - \"SE\"\n",
    "\"\"\"\n",
    "\n",
    "data = yaml.safe_load(fixed_yaml)\n",
    "print(\"‚úÖ Fixed YAML:\")\n",
    "print(yaml.dump(data, default_flow_style=False))\n",
    "\n",
    "# Verify fixes\n",
    "assert isinstance(data['version'], str)\n",
    "assert data['enabled'] is True\n",
    "assert isinstance(data['connections']['database']['port'], int)\n",
    "assert data['countries'][0] == 'NO'  # String, not False!\n",
    "print(\"\\n‚úÖ All assertions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Use Anchors to DRY Up Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_yaml = \"\"\"\n",
    "# Define defaults with anchor\n",
    "_defaults: &db_defaults\n",
    "  port: 5432\n",
    "  timeout: 30\n",
    "  pool_size: 5\n",
    "  retry_attempts: 3\n",
    "\n",
    "dev_database:\n",
    "  <<: *db_defaults  # Merge defaults\n",
    "  host: dev.db.com\n",
    "\n",
    "staging_database:\n",
    "  <<: *db_defaults\n",
    "  host: staging.db.com\n",
    "\n",
    "prod_database:\n",
    "  <<: *db_defaults\n",
    "  host: prod.db.com\n",
    "  timeout: 60    # Override for production\n",
    "  pool_size: 10  # Override for production\n",
    "\"\"\"\n",
    "\n",
    "data = yaml.safe_load(refactored_yaml)\n",
    "print(\"Dev database:\")\n",
    "print(data['dev_database'])\n",
    "print(\"\\nProd database:\")\n",
    "print(data['prod_database'])\n",
    "\n",
    "# Verify\n",
    "assert data['dev_database']['port'] == 5432\n",
    "assert data['dev_database']['timeout'] == 30\n",
    "assert data['prod_database']['timeout'] == 60  # Overridden\n",
    "assert data['prod_database']['pool_size'] == 10  # Overridden\n",
    "print(\"\\n‚úÖ Refactored successfully with 70% less duplication!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Create a Pydantic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional, Literal\n",
    "\n",
    "class ConnectionConfig(BaseModel):\n",
    "    type: Literal['local', 'azure_adls', 'azure_sql']\n",
    "    base_path: Optional[str] = None\n",
    "\n",
    "class RetryConfig(BaseModel):\n",
    "    max_attempts: int = Field(ge=1, le=10, description=\"Max retry attempts\")\n",
    "    backoff_seconds: float = Field(gt=0, description=\"Backoff time in seconds\")\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    name: str\n",
    "    operation: Literal['read', 'transform', 'write']\n",
    "    depends_on: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    @validator('name')\n",
    "    def validate_name(cls, v):\n",
    "        if not v.replace('_', '').isalnum():\n",
    "            raise ValueError('Node name must be alphanumeric + underscores')\n",
    "        return v\n",
    "\n",
    "class PipelineConfig(BaseModel):\n",
    "    pipeline: str\n",
    "    nodes: List[NodeConfig]\n",
    "    \n",
    "    @validator('nodes')\n",
    "    def validate_dependencies(cls, nodes):\n",
    "        \"\"\"Check that depends_on references valid nodes.\"\"\"\n",
    "        node_names = {n.name for n in nodes}\n",
    "        for node in nodes:\n",
    "            for dep in node.depends_on:\n",
    "                if dep not in node_names:\n",
    "                    raise ValueError(f\"Node '{node.name}' depends on unknown node '{dep}'\")\n",
    "        return nodes\n",
    "\n",
    "class AppConfig(BaseModel):\n",
    "    project: str\n",
    "    engine: Literal['pandas', 'spark'] = 'pandas'\n",
    "    connections: Dict[str, ConnectionConfig]\n",
    "    retry: RetryConfig\n",
    "    pipelines: List[PipelineConfig]\n",
    "\n",
    "# Test\n",
    "pipeline_yaml = \"\"\"\n",
    "project: Sales ETL\n",
    "engine: pandas\n",
    "\n",
    "connections:\n",
    "  data:\n",
    "    type: local\n",
    "    base_path: ./data\n",
    "\n",
    "retry:\n",
    "  max_attempts: 3\n",
    "  backoff_seconds: 2.0\n",
    "\n",
    "pipelines:\n",
    "  - pipeline: bronze_to_silver\n",
    "    nodes:\n",
    "      - name: load_sales\n",
    "        operation: read\n",
    "      - name: clean_sales\n",
    "        operation: transform\n",
    "        depends_on: [load_sales]\n",
    "\"\"\"\n",
    "\n",
    "data = yaml.safe_load(pipeline_yaml)\n",
    "config = AppConfig(**data)\n",
    "print(\"‚úÖ Valid config loaded!\")\n",
    "print(config.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Environment Variable Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def load_yaml_with_env(yaml_string: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load YAML with environment variable substitution.\n",
    "    Supports:\n",
    "    - ${VAR} - replace with env var (error if not set)\n",
    "    - ${VAR:-default} - replace with env var or default\n",
    "    \"\"\"\n",
    "    def replace_env(match):\n",
    "        full_match = match.group(1)\n",
    "        \n",
    "        # Check for default value syntax\n",
    "        if ':-' in full_match:\n",
    "            var_name, default = full_match.split(':-', 1)\n",
    "            return os.environ.get(var_name, default)\n",
    "        else:\n",
    "            # No default - must be set\n",
    "            var_name = full_match\n",
    "            if var_name not in os.environ:\n",
    "                raise ValueError(f\"Environment variable '{var_name}' not set\")\n",
    "            return os.environ[var_name]\n",
    "    \n",
    "    # Replace ${...} patterns\n",
    "    expanded = re.sub(r'\\$\\{([^}]+)\\}', replace_env, yaml_string)\n",
    "    return yaml.safe_load(expanded)\n",
    "\n",
    "# Test\n",
    "os.environ['DB_HOST'] = 'production.db.com'\n",
    "# DB_PORT not set - should use default\n",
    "\n",
    "test_yaml = \"\"\"\n",
    "database:\n",
    "  host: ${DB_HOST}\n",
    "  port: ${DB_PORT:-5432}\n",
    "  timeout: ${TIMEOUT:-30}\n",
    "\"\"\"\n",
    "\n",
    "config = load_yaml_with_env(test_yaml)\n",
    "print(\"Config with env vars:\")\n",
    "print(config)\n",
    "\n",
    "assert config['database']['host'] == 'production.db.com'\n",
    "assert config['database']['port'] == 5432  # Default used (string in YAML)\n",
    "assert config['database']['timeout'] == 30\n",
    "print(\"\\n‚úÖ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Multi-Environment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "def deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively merge override into base.\"\"\"\n",
    "    result = base.copy()\n",
    "    \n",
    "    for key, value in override.items():\n",
    "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "            result[key] = deep_merge(result[key], value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_config_for_env(base_path: Path, env: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load config with inheritance:\n",
    "    1. Load base.yaml\n",
    "    2. Load {env}.yaml\n",
    "    3. Merge (env overrides base)\n",
    "    \"\"\"\n",
    "    base_file = base_path / 'base.yaml'\n",
    "    env_file = base_path / f'{env}.yaml'\n",
    "    \n",
    "    # Load base\n",
    "    with open(base_file, 'r') as f:\n",
    "        base_config = yaml.safe_load(f) or {}\n",
    "    \n",
    "    # Load environment-specific\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            env_config = yaml.safe_load(f) or {}\n",
    "    else:\n",
    "        env_config = {}\n",
    "    \n",
    "    # Merge\n",
    "    return deep_merge(base_config, env_config)\n",
    "\n",
    "# Create test files\n",
    "config_dir = Path('example_configs/multi_env')\n",
    "config_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "base_yaml = \"\"\"\n",
    "project: My Project\n",
    "timeout: 30\n",
    "log_level: INFO\n",
    "database:\n",
    "  host: localhost\n",
    "  port: 5432\n",
    "\"\"\"\n",
    "\n",
    "prod_yaml = \"\"\"\n",
    "timeout: 60\n",
    "log_level: WARNING\n",
    "replicas: 3\n",
    "database:\n",
    "  host: prod.db.com  # Override\n",
    "  # port inherited from base\n",
    "\"\"\"\n",
    "\n",
    "(config_dir / 'base.yaml').write_text(base_yaml)\n",
    "(config_dir / 'prod.yaml').write_text(prod_yaml)\n",
    "\n",
    "# Test\n",
    "config = load_config_for_env(config_dir, 'prod')\n",
    "print(\"Merged prod config:\")\n",
    "print(yaml.dump(config, default_flow_style=False))\n",
    "\n",
    "assert config['timeout'] == 60  # Overridden\n",
    "assert config['project'] == 'My Project'  # From base\n",
    "assert config['replicas'] == 3  # From prod\n",
    "assert config['database']['host'] == 'prod.db.com'  # Overridden\n",
    "assert config['database']['port'] == 5432  # Inherited from base\n",
    "print(\"\\n‚úÖ Multi-environment config works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Analyze Odibi Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_odibi_config(yaml_path: str) -> dict:\n",
    "    \"\"\"Analyze an Odibi pipeline configuration.\"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    pipelines = config.get('pipelines', [])\n",
    "    connections = config.get('connections', {})\n",
    "    \n",
    "    # Count nodes\n",
    "    total_nodes = sum(len(p.get('nodes', [])) for p in pipelines)\n",
    "    \n",
    "    # Connection types\n",
    "    conn_types = [conn.get('type') for conn in connections.values()]\n",
    "    \n",
    "    # Formats used\n",
    "    formats = []\n",
    "    nodes_with_deps = 0\n",
    "    \n",
    "    for pipeline in pipelines:\n",
    "        for node in pipeline.get('nodes', []):\n",
    "            # Check for read/write formats\n",
    "            if 'read' in node:\n",
    "                fmt = node['read'].get('format')\n",
    "                if fmt:\n",
    "                    formats.append(fmt)\n",
    "            if 'write' in node:\n",
    "                fmt = node['write'].get('format')\n",
    "                if fmt:\n",
    "                    formats.append(fmt)\n",
    "            \n",
    "            # Check for dependencies\n",
    "            if node.get('depends_on'):\n",
    "                nodes_with_deps += 1\n",
    "    \n",
    "    return {\n",
    "        'total_pipelines': len(pipelines),\n",
    "        'total_nodes': total_nodes,\n",
    "        'connection_types': list(set(conn_types)),\n",
    "        'formats': dict(Counter(formats)),\n",
    "        'nodes_with_deps': nodes_with_deps,\n",
    "        'pipeline_names': [p.get('pipeline') for p in pipelines]\n",
    "    }\n",
    "\n",
    "# Test with real Odibi config\n",
    "odibi_path = r'c:\\Users\\hodibi\\OneDrive - Ingredion\\Desktop\\Repos\\Odibi\\examples\\example_delta_pipeline.yaml'\n",
    "analysis = analyze_odibi_config(odibi_path)\n",
    "\n",
    "print(\"üìä Odibi Config Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pipelines: {analysis['total_pipelines']}\")\n",
    "print(f\"  Names: {', '.join(analysis['pipeline_names'])}\")\n",
    "print(f\"\\nNodes: {analysis['total_nodes']}\")\n",
    "print(f\"  With dependencies: {analysis['nodes_with_deps']}\")\n",
    "print(f\"\\nConnection types: {', '.join(analysis['connection_types'])}\")\n",
    "print(f\"\\nFormats used:\")\n",
    "for fmt, count in analysis['formats'].items():\n",
    "    print(f\"  {fmt}: {count}x\")\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Solution: Config Linter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def lint_yaml_config(yaml_path: Path) -> List[str]:\n",
    "    \"\"\"Check YAML config for common issues.\"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Read raw YAML (before parsing)\n",
    "    content = yaml_path.read_text()\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Check 1: Hardcoded secrets\n",
    "    secret_patterns = [\n",
    "        r'password\\s*:\\s*[^\\s]+',\n",
    "        r'secret\\s*:\\s*[^\\s]+',\n",
    "        r'api_key\\s*:\\s*[^\\s]+',\n",
    "    ]\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        for pattern in secret_patterns:\n",
    "            if re.search(pattern, line, re.IGNORECASE):\n",
    "                if '${' not in line:  # Not an env var\n",
    "                    warnings.append(f\"Line {i}: Possible hardcoded secret\")\n",
    "    \n",
    "    # Check 2: Unquoted boolean-like values\n",
    "    risky_values = ['NO', 'YES', 'ON', 'OFF']\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        for val in risky_values:\n",
    "            if re.search(f':\\s*{val}\\s*$', line):\n",
    "                warnings.append(f\"Line {i}: Unquoted '{val}' may be misinterpreted\")\n",
    "    \n",
    "    # Check 3: Parse and check structure\n",
    "    try:\n",
    "        config = yaml.safe_load(content)\n",
    "        \n",
    "        # Check for TODO/FIXME\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            if 'TODO' in line or 'FIXME' in line:\n",
    "                warnings.append(f\"Line {i}: Contains TODO/FIXME\")\n",
    "        \n",
    "        # Check naming consistency\n",
    "        def check_keys(obj, path=''):\n",
    "            if isinstance(obj, dict):\n",
    "                for key, value in obj.items():\n",
    "                    # Check snake_case vs camelCase\n",
    "                    if re.search(r'[a-z][A-Z]', key):  # camelCase\n",
    "                        warnings.append(f\"{path}.{key}: Uses camelCase (prefer snake_case)\")\n",
    "                    check_keys(value, f\"{path}.{key}\" if path else key)\n",
    "            elif isinstance(obj, list):\n",
    "                for i, item in enumerate(obj):\n",
    "                    check_keys(item, f\"{path}[{i}]\")\n",
    "        \n",
    "        check_keys(config)\n",
    "        \n",
    "    except yaml.YAMLError as e:\n",
    "        warnings.append(f\"YAML parsing error: {e}\")\n",
    "    \n",
    "    return warnings\n",
    "\n",
    "# Test\n",
    "test_yaml = \"\"\"\n",
    "project: Test\n",
    "database:\n",
    "  host: localhost\n",
    "  password: hardcoded_secret_123  # Bad!\n",
    "  country: NO  # Unquoted!\n",
    "camelCaseKey: value  # Bad naming\n",
    "# TODO: fix this later\n",
    "\"\"\"\n",
    "\n",
    "test_file = Path('example_configs/test_lint.yaml')\n",
    "test_file.write_text(test_yaml)\n",
    "\n",
    "warnings = lint_yaml_config(test_file)\n",
    "print(\"üîç Linter Results:\")\n",
    "for warning in warnings:\n",
    "    print(f\"  ‚ö†Ô∏è  {warning}\")\n",
    "\n",
    "if not warnings:\n",
    "    print(\"  ‚úÖ No issues found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
