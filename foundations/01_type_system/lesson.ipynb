{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Python Type System & Validation\n",
    "\n",
    "**Focus**: Type hints, Pydantic models, and fail-fast validation for data engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Problem\n",
    "\n",
    "### Data Pipeline Configuration Without Types\n",
    "\n",
    "Imagine configuring a data pipeline with plain dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'connection': 'azure_blob', 'account_name': 'myaccount', 'container': 'data', 'port': '1433', 'retries': -5}\n"
     ]
    }
   ],
   "source": [
    "# What could go wrong?\n",
    "config = {\n",
    "    \"connection\": \"azure_blob\",\n",
    "    \"account_name\": \"myaccount\",\n",
    "    \"container\": \"data\",\n",
    "    \"port\": \"1433\",  # Should be int!\n",
    "    \"retries\": -5,    # Negative retries?\n",
    "    # Missing required fields?\n",
    "}\n",
    "\n",
    "# Errors discovered HOURS into execution üí•\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems**:\n",
    "1. ‚ùå No validation until runtime (expensive!)\n",
    "2. ‚ùå Type errors caught late\n",
    "3. ‚ùå Missing required fields silently ignored\n",
    "4. ‚ùå No IDE autocomplete\n",
    "5. ‚ùå Hard to document\n",
    "\n",
    "**Solution**: Type hints + Pydantic = Fail fast, self-document, validate early"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü¶â First Principles\n",
    "\n",
    "### Static vs Dynamic Typing\n",
    "\n",
    "**Dynamic Typing** (Python default):\n",
    "```python\n",
    "x = 5       # x is int\n",
    "x = \"hello\" # now x is str\n",
    "```\n",
    "\n",
    "**Static Typing** (Java, C++):\n",
    "```java\n",
    "int x = 5;\n",
    "x = \"hello\"; // Compile error!\n",
    "```\n",
    "\n",
    "**Gradual Typing** (Python 3.5+):\n",
    "```python\n",
    "x: int = 5       # Type hint\n",
    "x = \"hello\"      # Runs fine, but mypy catches it!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Hints: Documentation + Tooling\n",
    "\n",
    "Type hints are:\n",
    "- **Optional** (Python doesn't enforce them)\n",
    "- **For tools** (mypy, IDEs, linters)\n",
    "- **Self-documenting**\n",
    "- **Zero runtime cost** (ignored by interpreter)\n",
    "\n",
    "Pydantic adds:\n",
    "- **Runtime validation**\n",
    "- **Data parsing**\n",
    "- **Automatic coercion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° Minimal Examples\n",
    "\n",
    "### 1. Primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob, you are 25 years old\n"
     ]
    }
   ],
   "source": [
    "# Basic type hints\n",
    "age: int = 30\n",
    "name: str = \"Alice\"\n",
    "is_active: bool = True\n",
    "temperature: float = 98.6\n",
    "\n",
    "def greet(name: str, age: int) -> str:\n",
    "    return f\"Hello {name}, you are {age} years old\"\n",
    "\n",
    "print(greet(\"Bob\", 25))\n",
    "# mypy would catch: greet(123, \"invalid\")  # Wrong types!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers: [1, 2, 3, 4]\n",
      "Config: {'host': 'localhost', 'port': 8080}\n",
      "Coordinate: (40.7128, -74.006)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "# List of integers\n",
    "numbers: List[int] = [1, 2, 3, 4]\n",
    "\n",
    "# Dictionary: string keys, any values\n",
    "config: Dict[str, any] = {\"host\": \"localhost\", \"port\": 8080}\n",
    "\n",
    "# Set of strings\n",
    "tags: Set[str] = {\"python\", \"data\", \"engineering\"}\n",
    "\n",
    "# Tuple with specific types\n",
    "coordinate: Tuple[float, float] = (40.7128, -74.0060)\n",
    "\n",
    "print(f\"Numbers: {numbers}\")\n",
    "print(f\"Config: {config}\")\n",
    "print(f\"Coordinate: {coordinate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optional and Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 'sample'}\n",
      "{'data': 'sample', 'limit': 100}\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "# Optional[T] means T or None\n",
    "middle_name: Optional[str] = None  # Could be str or None\n",
    "\n",
    "# Union means one of several types\n",
    "identifier: Union[int, str] = \"USER123\"  # Could be int OR str\n",
    "\n",
    "def process_data(data: str, max_rows: Optional[int] = None) -> Dict[str, any]:\n",
    "    \"\"\"Process data with optional row limit.\"\"\"\n",
    "    result = {\"data\": data}\n",
    "    if max_rows is not None:\n",
    "        result[\"limit\"] = max_rows\n",
    "    return result\n",
    "\n",
    "print(process_data(\"sample\"))\n",
    "print(process_data(\"sample\", max_rows=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Literal (Constrained Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying to prod\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Only these exact values allowed\n",
    "Environment = Literal[\"dev\", \"staging\", \"prod\"]\n",
    "\n",
    "def deploy(env: Environment) -> None:\n",
    "    print(f\"Deploying to {env}\")\n",
    "\n",
    "deploy(\"prod\")  # ‚úÖ OK\n",
    "# deploy(\"production\")  # ‚ùå mypy error - not in Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Types with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Alice' age=30 email='alice@example.com'\n",
      "{'name': 'Alice', 'age': 30, 'email': 'alice@example.com'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel):\n",
    "    \"\"\"User model with validation.\"\"\"\n",
    "    name: str\n",
    "    age: int = Field(gt=0, lt=120)  # Constraints!\n",
    "    email: Optional[str] = None\n",
    "\n",
    "# ‚úÖ Valid user\n",
    "user = User(name=\"Alice\", age=30, email=\"alice@example.com\")\n",
    "print(user)\n",
    "print(user.model_dump())  # Convert to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1 validation error for User\n",
      "age\n",
      "  Input should be greater than 0 [type=greater_than, input_value=-5, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/greater_than\n"
     ]
    }
   ],
   "source": [
    "# ‚ùå Validation errors caught immediately\n",
    "try:\n",
    "    invalid_user = User(name=\"Bob\", age=-5)  # Negative age!\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Odibi Analysis\n",
    "\n",
    "Let's examine real production code from the Odibi framework.\n",
    "\n",
    "### Example 1: Enum for Type Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConnectionType.AZURE_BLOB\n",
      "azure_blob\n",
      "[<ConnectionType.LOCAL: 'local'>, <ConnectionType.AZURE_BLOB: 'azure_blob'>, <ConnectionType.DELTA: 'delta'>, <ConnectionType.SQL_SERVER: 'sql_server'>]\n"
     ]
    }
   ],
   "source": [
    "# From odibi_snippets.py\n",
    "from enum import Enum\n",
    "\n",
    "class ConnectionType(str, Enum):\n",
    "    \"\"\"Supported connection types.\"\"\"\n",
    "    LOCAL = \"local\"\n",
    "    AZURE_BLOB = \"azure_blob\"\n",
    "    DELTA = \"delta\"\n",
    "    SQL_SERVER = \"sql_server\"\n",
    "\n",
    "# Why Enum?\n",
    "# 1. Autocomplete in IDE\n",
    "# 2. Typo protection\n",
    "# 3. Clear documentation of valid values\n",
    "\n",
    "print(ConnectionType.AZURE_BLOB)\n",
    "print(ConnectionType.AZURE_BLOB.value)\n",
    "print(list(ConnectionType))  # All valid options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Connection Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"azure_blob\",\n",
      "  \"account_name\": \"myaccount\",\n",
      "  \"container\": \"data\",\n",
      "  \"auth\": {\n",
      "    \"key\": \"secret\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict\n",
    "\n",
    "class AzureBlobConnectionConfig(BaseModel):\n",
    "    \"\"\"Azure Blob Storage connection.\"\"\"\n",
    "    type: ConnectionType = ConnectionType.AZURE_BLOB\n",
    "    account_name: str  # Required!\n",
    "    container: str     # Required!\n",
    "    auth: Dict[str, str] = Field(default_factory=dict)  # Optional with default\n",
    "\n",
    "# ‚úÖ Valid config\n",
    "azure_config = AzureBlobConnectionConfig(\n",
    "    account_name=\"myaccount\",\n",
    "    container=\"data\",\n",
    "    auth={\"key\": \"secret\"}\n",
    ")\n",
    "print(azure_config.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1 validation error for AzureBlobConnectionConfig\n",
      "container\n",
      "  Field required [type=missing, input_value={'account_name': 'test'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/missing\n"
     ]
    }
   ],
   "source": [
    "# ‚ùå Missing required fields\n",
    "try:\n",
    "    bad_config = AzureBlobConnectionConfig(account_name=\"test\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Custom Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection='local' format='parquet' table=None path='/data/input.parquet'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import model_validator\n",
    "from typing import Optional\n",
    "\n",
    "class ReadConfig(BaseModel):\n",
    "    \"\"\"Configuration for reading data.\"\"\"\n",
    "    connection: str\n",
    "    format: str\n",
    "    table: Optional[str] = None\n",
    "    path: Optional[str] = None\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_table_or_path(self):\n",
    "        \"\"\"Ensure either table or path is provided.\"\"\"\n",
    "        if not self.table and not self.path:\n",
    "            raise ValueError(\"Either 'table' or 'path' must be provided\")\n",
    "        return self\n",
    "\n",
    "# ‚úÖ Valid - has path\n",
    "read_config = ReadConfig(\n",
    "    connection=\"local\",\n",
    "    format=\"parquet\",\n",
    "    path=\"/data/input.parquet\"\n",
    ")\n",
    "print(read_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1 validation error for ReadConfig\n",
      "  Value error, Either 'table' or 'path' must be provided [type=value_error, input_value={'connection': 'local', 'format': 'parquet'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n"
     ]
    }
   ],
   "source": [
    "# ‚ùå Invalid - missing both table and path\n",
    "try:\n",
    "    invalid_read = ReadConfig(\n",
    "        connection=\"local\",\n",
    "        format=\"parquet\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Field Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enabled=True max_attempts=5 backoff='linear'\n",
      "Error: 1 validation error for RetryConfig\n",
      "max_attempts\n",
      "  Input should be less than or equal to 10 [type=less_than_equal, input_value=100, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/less_than_equal\n"
     ]
    }
   ],
   "source": [
    "class RetryConfig(BaseModel):\n",
    "    \"\"\"Retry configuration with constraints.\"\"\"\n",
    "    enabled: bool = True\n",
    "    max_attempts: int = Field(default=3, ge=1, le=10)  # Between 1 and 10\n",
    "    backoff: str = Field(\n",
    "        default=\"exponential\",\n",
    "        pattern=\"^(exponential|linear|constant)$\"  # Regex pattern!\n",
    "    )\n",
    "\n",
    "# ‚úÖ Valid\n",
    "retry = RetryConfig(max_attempts=5, backoff=\"linear\")\n",
    "print(retry)\n",
    "\n",
    "# ‚ùå Invalid - too many attempts\n",
    "try:\n",
    "    bad_retry = RetryConfig(max_attempts=100)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Build It: Mini Config System\n",
    "\n",
    "Let's build a simplified data pipeline configuration system from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class DataFormat(str, Enum):\n",
    "    CSV = \"csv\"\n",
    "    PARQUET = \"parquet\"\n",
    "    JSON = \"json\"\n",
    "\n",
    "class ProcessingMode(str, Enum):\n",
    "    BATCH = \"batch\"\n",
    "    STREAMING = \"streaming\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Source Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"sales_data\",\n",
      "  \"path\": \"/data/sales.parquet\",\n",
      "  \"format\": \"parquet\",\n",
      "  \"options\": {\n",
      "    \"compression\": \"snappy\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class DataSource(BaseModel):\n",
    "    \"\"\"Configuration for a data source.\"\"\"\n",
    "    name: str = Field(description=\"Unique source name\")\n",
    "    path: str = Field(description=\"Path to data\")\n",
    "    format: DataFormat = DataFormat.PARQUET\n",
    "    options: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \n",
    "    @field_validator('name')\n",
    "    @classmethod\n",
    "    def validate_name(cls, v: str) -> str:\n",
    "        \"\"\"Ensure name is valid identifier.\"\"\"\n",
    "        if not v.isidentifier():\n",
    "            raise ValueError(f\"Name must be valid Python identifier: {v}\")\n",
    "        return v\n",
    "\n",
    "# Test it\n",
    "source = DataSource(\n",
    "    name=\"sales_data\",\n",
    "    path=\"/data/sales.parquet\",\n",
    "    format=DataFormat.PARQUET,\n",
    "    options={\"compression\": \"snappy\"}\n",
    ")\n",
    "print(source.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"daily_sales_etl\",\n",
      "  \"mode\": \"batch\",\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"name\": \"sales\",\n",
      "      \"path\": \"/data/sales.parquet\",\n",
      "      \"format\": \"parquet\",\n",
      "      \"options\": {}\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"customers\",\n",
      "      \"path\": \"/data/customers.csv\",\n",
      "      \"format\": \"csv\",\n",
      "      \"options\": {}\n",
      "    }\n",
      "  ],\n",
      "  \"max_workers\": 8,\n",
      "  \"tags\": [\n",
      "    \"daily\",\n",
      "    \"production\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Pipeline(BaseModel):\n",
    "    \"\"\"Complete pipeline configuration.\"\"\"\n",
    "    name: str\n",
    "    mode: ProcessingMode = ProcessingMode.BATCH\n",
    "    sources: List[DataSource] = Field(min_length=1)  # At least one source!\n",
    "    max_workers: int = Field(default=4, ge=1, le=32)\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    @field_validator('sources')\n",
    "    @classmethod\n",
    "    def check_unique_names(cls, sources: List[DataSource]) -> List[DataSource]:\n",
    "        \"\"\"Ensure all source names are unique.\"\"\"\n",
    "        names = [s.name for s in sources]\n",
    "        if len(names) != len(set(names)):\n",
    "            raise ValueError(f\"Duplicate source names: {names}\")\n",
    "        return sources\n",
    "\n",
    "# Build a pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"daily_sales_etl\",\n",
    "    mode=ProcessingMode.BATCH,\n",
    "    sources=[\n",
    "        DataSource(name=\"sales\", path=\"/data/sales.parquet\"),\n",
    "        DataSource(name=\"customers\", path=\"/data/customers.csv\", format=DataFormat.CSV)\n",
    "    ],\n",
    "    max_workers=8,\n",
    "    tags=[\"daily\", \"production\"]\n",
    ")\n",
    "\n",
    "print(pipeline.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Complex Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Valid pipeline created\n",
      "‚ùå Validation error: 1 validation error for AdvancedPipeline\n",
      "  Value error, output_format required when output_path is set [type=value_error, input_value={'name': 'test', 'sources...output_path': '/output'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n"
     ]
    }
   ],
   "source": [
    "from pydantic import model_validator\n",
    "\n",
    "class AdvancedPipeline(Pipeline):\n",
    "    \"\"\"Pipeline with cross-field validation.\"\"\"\n",
    "    output_path: Optional[str] = None\n",
    "    output_format: Optional[DataFormat] = None\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_output_consistency(self):\n",
    "        \"\"\"If output_path is set, output_format must be set.\"\"\"\n",
    "        if self.output_path and not self.output_format:\n",
    "            raise ValueError(\"output_format required when output_path is set\")\n",
    "        if self.output_format and not self.output_path:\n",
    "            raise ValueError(\"output_path required when output_format is set\")\n",
    "        return self\n",
    "\n",
    "# ‚úÖ Valid - both set\n",
    "valid = AdvancedPipeline(\n",
    "    name=\"test\",\n",
    "    sources=[DataSource(name=\"src\", path=\"/data\")],\n",
    "    output_path=\"/output\",\n",
    "    output_format=DataFormat.PARQUET\n",
    ")\n",
    "print(\"‚úÖ Valid pipeline created\")\n",
    "\n",
    "# ‚ùå Invalid - only path set\n",
    "try:\n",
    "    invalid = AdvancedPipeline(\n",
    "        name=\"test\",\n",
    "        sources=[DataSource(name=\"src\", path=\"/data\")],\n",
    "        output_path=\"/output\"  # Missing format!\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Test It\n",
    "\n",
    "Let's test our models systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Valid DataSource accepted\n",
      "‚úÖ Invalid name rejected: 1 validation error for DataSource\n",
      "name\n",
      "  Value error, Name must be valid Python identifier: invalid-name [type=value_error, input_value='invalid-name', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n",
      "‚úÖ Missing field caught: validation_error\n"
     ]
    }
   ],
   "source": [
    "def test_data_source_validation():\n",
    "    \"\"\"Test DataSource validation.\"\"\"\n",
    "    # Valid case\n",
    "    source = DataSource(name=\"test\", path=\"/data/test.parquet\")\n",
    "    assert source.name == \"test\"\n",
    "    print(\"‚úÖ Valid DataSource accepted\")\n",
    "    \n",
    "    # Invalid name\n",
    "    try:\n",
    "        DataSource(name=\"invalid-name\", path=\"/data\")  # Hyphen not allowed\n",
    "        assert False, \"Should have raised error\"\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úÖ Invalid name rejected: {e}\")\n",
    "    \n",
    "    # Missing required field\n",
    "    try:\n",
    "        DataSource(name=\"test\")  # Missing path!\n",
    "        assert False, \"Should have raised error\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Missing field caught: validation_error\")\n",
    "\n",
    "test_data_source_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Valid pipeline created\n",
      "‚úÖ Empty sources rejected\n",
      "‚úÖ Duplicate source names rejected\n"
     ]
    }
   ],
   "source": [
    "def test_pipeline_validation():\n",
    "    \"\"\"Test Pipeline validation.\"\"\"\n",
    "    # Valid pipeline\n",
    "    pipeline = Pipeline(\n",
    "        name=\"test\",\n",
    "        sources=[DataSource(name=\"s1\", path=\"/data\")]\n",
    "    )\n",
    "    assert len(pipeline.sources) == 1\n",
    "    print(\"‚úÖ Valid pipeline created\")\n",
    "    \n",
    "    # Empty sources\n",
    "    try:\n",
    "        Pipeline(name=\"test\", sources=[])\n",
    "        assert False, \"Should require at least one source\"\n",
    "    except Exception:\n",
    "        print(\"‚úÖ Empty sources rejected\")\n",
    "    \n",
    "    # Duplicate names\n",
    "    try:\n",
    "        Pipeline(\n",
    "            name=\"test\",\n",
    "            sources=[\n",
    "                DataSource(name=\"dup\", path=\"/a\"),\n",
    "                DataSource(name=\"dup\", path=\"/b\")\n",
    "            ]\n",
    "        )\n",
    "        assert False, \"Should reject duplicate names\"\n",
    "    except ValueError:\n",
    "        print(\"‚úÖ Duplicate source names rejected\")\n",
    "\n",
    "test_pipeline_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized to JSON:\n",
      "{\n",
      "  \"name\": \"daily_sales_etl\",\n",
      "  \"mode\": \"batch\",\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"name\": \"sales\",\n",
      "      \"path\": \"/data/sales.parquet\",\n",
      "      \"format\": \"parquet\",\n",
      "      \"options\": {}\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"customers\",\n",
      "      \"path\": \"/data/customers.csv\",\n",
      "      \"format\": \"csv\",\n",
      "      \"options\": {}\n",
      "    }\n",
      "  ],\n",
      "  \"max_workers\": 8,\n",
      "  \"tags\": [\n",
      "    \"daily\",\n",
      "    \"production\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "‚úÖ Reconstructed: daily_sales_etl\n"
     ]
    }
   ],
   "source": [
    "# Pydantic makes JSON serialization trivial\n",
    "pipeline_json = pipeline.model_dump_json(indent=2)\n",
    "print(\"Serialized to JSON:\")\n",
    "print(pipeline_json)\n",
    "\n",
    "# And deserialization\n",
    "import json\n",
    "data = json.loads(pipeline_json)\n",
    "reconstructed = Pipeline(**data)\n",
    "print(f\"\\n‚úÖ Reconstructed: {reconstructed.name}\")\n",
    "assert reconstructed.name == pipeline.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "Complete these TODOs to practice what you've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Database Connection Config\n",
    "\n",
    "Create a `DatabaseConfig` model with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host='localhost' port=5432 database='analytics' username='analyst' password=None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create DatabaseConfig with:\n",
    "# - host: str (required)\n",
    "# - port: int (default 5432, must be 1-65535)\n",
    "# - database: str (required)\n",
    "# - username: str (required)\n",
    "# - password: str (optional, for security reasons)\n",
    "# - ssl_enabled: bool (default True)\n",
    "\n",
    "class DatabaseConfig(BaseModel):\n",
    "    host: str\n",
    "    port: int = Field(default=5432, ge=1, le=65535)\n",
    "    database: str\n",
    "    username: str\n",
    "    password: Optional[str] = None\n",
    "\n",
    "# Test your implementation\n",
    "db = DatabaseConfig(\n",
    "    host=\"localhost\",\n",
    "    database=\"analytics\",\n",
    "    username=\"analyst\"\n",
    ")\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Custom Validator\n",
    "\n",
    "Add validation to ensure host is not 'localhost' in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Caught error: 1 validation error for ProductionDatabaseConfig\n",
      "  Value error, host cannot be `localhost` if environment = 'prod' [type=value_error, input_value={'host': 'localhost', 'en...db', 'username': 'user'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add an environment field and validator\n",
    "# - environment: Literal[\"dev\", \"staging\", \"prod\"]\n",
    "# - Add model_validator to ensure:\n",
    "#   - If environment is \"prod\", host cannot be \"localhost\"\n",
    "\n",
    "class ProductionDatabaseConfig(BaseModel):\n",
    "    host: str\n",
    "    port: int = Field(default=5432, ge=1, le=65535)\n",
    "    database: str\n",
    "    username: str\n",
    "    password: Optional[str] = None\n",
    "    environment: Literal[\"dev\", \"staging\", \"prod\"]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_env_and_host(self):\n",
    "        if self.environment == \"prod\" and self.host == \"localhost\":\n",
    "            raise ValueError(f\"host cannot be `localhost` if environment = 'prod'\")\n",
    "\n",
    "\n",
    "# This should fail:\n",
    "try:\n",
    "    bad_config = ProductionDatabaseConfig(\n",
    "        host=\"localhost\",\n",
    "        environment=\"prod\",\n",
    "        database=\"db\",\n",
    "        username=\"user\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Caught error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Transformation Config\n",
    "\n",
    "Create a model for SQL transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='clean_sales' sql=None description='Remove negative amounts' parameters=<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create TransformationConfig with:\n",
    "# - name: str (valid identifier)\n",
    "# - sql: str (required, non-empty)\n",
    "# - description: Optional[str]\n",
    "# - parameters: Dict[str, Any] (default empty dict)\n",
    "# Add validator to ensure sql is not just whitespace\n",
    "\n",
    "class TransformationConfig(BaseModel):\n",
    "    name: str\n",
    "    sql: str\n",
    "    description: Optional[str]\n",
    "    parameters: Dict[str, Any] = Field(default=dict)\n",
    "\n",
    "    @field_validator(\"sql\")\n",
    "    def check_non_empty_string(v):\n",
    "        if not v:\n",
    "            raise ValueError(\"sql must be non empty string\")\n",
    "\n",
    "# Test:\n",
    "transform = TransformationConfig(\n",
    "    name=\"clean_sales\",\n",
    "    sql=\"SELECT * FROM sales WHERE amount > 0\",\n",
    "    description=\"Remove negative amounts\"\n",
    ")\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Complete ETL Config\n",
    "\n",
    "Combine everything into a complete ETL configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     schedule: Optional[\u001b[38;5;28mstr\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create a complete ETL config:\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m etl \u001b[38;5;241m=\u001b[39m ETLConfig(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(etl\u001b[38;5;241m.\u001b[39mmodel_dump_json(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# TODO: Create ETLConfig that combines:\n",
    "# - source: DataSource\n",
    "# - transformations: List[TransformationConfig] (at least one)\n",
    "# - destination: DatabaseConfig\n",
    "# - schedule: Optional[str] (cron expression)\n",
    "# Add validation to ensure transformations list is not empty\n",
    "\n",
    "class ETLConfig(BaseModel):\n",
    "    source: DataSource\n",
    "    transformations: List[TransformationConfig] = Field(min_length=1)\n",
    "    destination: DatabaseConfig\n",
    "    schedule: Optional[str]\n",
    "\n",
    "# Create a complete ETL config:\n",
    "etl = ETLConfig()\n",
    "print(etl.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. ‚úÖ **Type hints**: Document and enable tooling\n",
    "2. ‚úÖ **Pydantic models**: Runtime validation + parsing\n",
    "3. ‚úÖ **Field constraints**: `ge`, `le`, `pattern`, `min_length`\n",
    "4. ‚úÖ **Custom validators**: `@field_validator`, `@model_validator`\n",
    "5. ‚úÖ **Enums**: Type-safe constants\n",
    "6. ‚úÖ **Fail-fast**: Catch errors at config time, not runtime\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Complete exercises.ipynb\n",
    "2. Check solutions.ipynb\n",
    "3. Explore odibi_snippets.py for more patterns\n",
    "4. Try running mypy on your code\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "> **In data engineering, failing fast with clear errors is better than failing hours into a pipeline run.**\n",
    ">\n",
    "> Type hints + Pydantic = Self-documenting, validated, maintainable configs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
