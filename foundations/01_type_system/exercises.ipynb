{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Python Type System & Validation\n",
    "\n",
    "Practice what you've learned by completing these exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "from typing import List, Dict, Optional, Union, Literal, Any\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Database Connection Config\n",
    "\n",
    "**Goal**: Create a `DatabaseConfig` model with proper types and constraints.\n",
    "\n",
    "**Requirements**:\n",
    "- `host`: str (required)\n",
    "- `port`: int (default 5432, must be between 1-65535)\n",
    "- `database`: str (required)\n",
    "- `username`: str (required)\n",
    "- `password`: Optional[str] (for security, make it optional)\n",
    "- `ssl_enabled`: bool (default True)\n",
    "- `timeout`: int (default 30, must be positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement DatabaseConfig\n",
    "class DatabaseConfig(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# db = DatabaseConfig(\n",
    "#     host=\"postgres.example.com\",\n",
    "#     database=\"analytics\",\n",
    "#     username=\"analyst\"\n",
    "# )\n",
    "# print(db.model_dump_json(indent=2))\n",
    "\n",
    "# Test validation - this should fail (port too high)\n",
    "# try:\n",
    "#     bad_db = DatabaseConfig(\n",
    "#         host=\"localhost\",\n",
    "#         port=99999,\n",
    "#         database=\"test\",\n",
    "#         username=\"user\"\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"âœ… Validation caught error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Production Environment Validation\n",
    "\n",
    "**Goal**: Extend `DatabaseConfig` to prevent localhost in production.\n",
    "\n",
    "**Requirements**:\n",
    "- Add `environment`: Literal[\"dev\", \"staging\", \"prod\"]\n",
    "- Add a `@model_validator` that ensures:\n",
    "  - If `environment == \"prod\"`, `host` cannot be \"localhost\" or \"127.0.0.1\"\n",
    "  - If `environment == \"prod\"`, `ssl_enabled` must be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement ProductionDatabaseConfig\n",
    "class ProductionDatabaseConfig(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Test - this should work\n",
    "# dev_db = ProductionDatabaseConfig(\n",
    "#     host=\"localhost\",\n",
    "#     database=\"test\",\n",
    "#     username=\"dev\",\n",
    "#     environment=\"dev\"\n",
    "# )\n",
    "# print(\"âœ… Dev with localhost OK\")\n",
    "\n",
    "# Test - this should fail (localhost in prod)\n",
    "# try:\n",
    "#     prod_db = ProductionDatabaseConfig(\n",
    "#         host=\"localhost\",\n",
    "#         database=\"prod_db\",\n",
    "#         username=\"prod_user\",\n",
    "#         environment=\"prod\"\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"âœ… Caught prod localhost error: {e}\")\n",
    "\n",
    "# Test - this should fail (SSL disabled in prod)\n",
    "# try:\n",
    "#     prod_db = ProductionDatabaseConfig(\n",
    "#         host=\"prod.example.com\",\n",
    "#         database=\"prod_db\",\n",
    "#         username=\"prod_user\",\n",
    "#         environment=\"prod\",\n",
    "#         ssl_enabled=False\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"âœ… Caught prod SSL error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: SQL Transformation Config\n",
    "\n",
    "**Goal**: Create a model for SQL transformations.\n",
    "\n",
    "**Requirements**:\n",
    "- `name`: str (must be valid Python identifier)\n",
    "- `sql`: str (required, cannot be empty or whitespace)\n",
    "- `description`: Optional[str]\n",
    "- `parameters`: Dict[str, Any] (default empty dict)\n",
    "- `enabled`: bool (default True)\n",
    "\n",
    "**Validators**:\n",
    "- Validate `name` is a valid Python identifier (use `str.isidentifier()`)\n",
    "- Validate `sql` is not empty or just whitespace (use `str.strip()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement TransformationConfig\n",
    "class TransformationConfig(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# transform = TransformationConfig(\n",
    "#     name=\"clean_sales\",\n",
    "#     sql=\"SELECT * FROM sales WHERE amount > 0\",\n",
    "#     description=\"Remove negative amounts\",\n",
    "#     parameters={\"min_amount\": 0}\n",
    "# )\n",
    "# print(transform.model_dump_json(indent=2))\n",
    "\n",
    "# Test - invalid name (has hyphen)\n",
    "# try:\n",
    "#     bad_transform = TransformationConfig(\n",
    "#         name=\"clean-sales\",\n",
    "#         sql=\"SELECT 1\"\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"âœ… Invalid name caught: {e}\")\n",
    "\n",
    "# Test - empty SQL\n",
    "# try:\n",
    "#     bad_transform = TransformationConfig(\n",
    "#         name=\"test\",\n",
    "#         sql=\"   \"  # Just whitespace\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"âœ… Empty SQL caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: File Format Config\n",
    "\n",
    "**Goal**: Create configs for different file formats with format-specific options.\n",
    "\n",
    "**Requirements**:\n",
    "- Create `FileFormat` enum: CSV, PARQUET, JSON, AVRO\n",
    "- Create `CompressionType` enum: NONE, GZIP, SNAPPY, LZ4\n",
    "- Create `FileConfig` with:\n",
    "  - `path`: str (required)\n",
    "  - `format`: FileFormat (required)\n",
    "  - `compression`: CompressionType (default NONE)\n",
    "  - `options`: Dict[str, Any] (default empty)\n",
    "  - Add validator: if format is CSV, options can have \"delimiter\" and \"header\"\n",
    "  - Add validator: PARQUET and AVRO cannot use GZIP (not supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement FileFormat, CompressionType, and FileConfig\n",
    "\n",
    "class FileFormat(str, Enum):\n",
    "    pass\n",
    "\n",
    "class CompressionType(str, Enum):\n",
    "    pass\n",
    "\n",
    "class FileConfig(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Test CSV with options\n",
    "# csv_file = FileConfig(\n",
    "#     path=\"/data/sales.csv\",\n",
    "#     format=FileFormat.CSV,\n",
    "#     options={\"delimiter\": \"|\", \"header\": True}\n",
    "# )\n",
    "# print(csv_file)\n",
    "\n",
    "# Test Parquet with compression\n",
    "# parquet_file = FileConfig(\n",
    "#     path=\"/data/sales.parquet\",\n",
    "#     format=FileFormat.PARQUET,\n",
    "#     compression=CompressionType.SNAPPY\n",
    "# )\n",
    "# print(parquet_file)\n",
    "\n",
    "# Test invalid combination (Parquet + GZIP)\n",
    "# try:\n",
    "#     bad_file = FileConfig(\n",
    "#         path=\"/data/test.parquet\",\n",
    "#         format=FileFormat.PARQUET,\n",
    "#         compression=CompressionType.GZIP\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"âœ… Invalid compression caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Data Quality Rules\n",
    "\n",
    "**Goal**: Create a model for data quality validation rules.\n",
    "\n",
    "**Requirements**:\n",
    "- `rule_name`: str (valid identifier)\n",
    "- `column`: str (required)\n",
    "- `rule_type`: Literal[\"not_null\", \"unique\", \"range\", \"regex\", \"custom\"]\n",
    "- `parameters`: Dict[str, Any] (default empty)\n",
    "- `severity`: Literal[\"warning\", \"error\"] (default \"error\")\n",
    "\n",
    "**Validators**:\n",
    "- If `rule_type == \"range\"`, parameters must have \"min\" or \"max\"\n",
    "- If `rule_type == \"regex\"`, parameters must have \"pattern\"\n",
    "- If `rule_type == \"custom\"`, parameters must have \"function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement DataQualityRule\n",
    "class DataQualityRule(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Test not_null rule\n",
    "# rule1 = DataQualityRule(\n",
    "#     rule_name=\"check_customer_id\",\n",
    "#     column=\"customer_id\",\n",
    "#     rule_type=\"not_null\"\n",
    "# )\n",
    "# print(rule1)\n",
    "\n",
    "# Test range rule\n",
    "# rule2 = DataQualityRule(\n",
    "#     rule_name=\"check_age_range\",\n",
    "#     column=\"age\",\n",
    "#     rule_type=\"range\",\n",
    "#     parameters={\"min\": 0, \"max\": 120}\n",
    "# )\n",
    "# print(rule2)\n",
    "\n",
    "# Test invalid range rule (missing parameters)\n",
    "# try:\n",
    "#     bad_rule = DataQualityRule(\n",
    "#         rule_name=\"bad_range\",\n",
    "#         column=\"value\",\n",
    "#         rule_type=\"range\"\n",
    "#         # Missing min/max!\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"âœ… Missing range parameters caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Complete ETL Pipeline Config\n",
    "\n",
    "**Goal**: Combine all previous models into a complete ETL configuration.\n",
    "\n",
    "**Requirements**:\n",
    "- `name`: str (required)\n",
    "- `source`: FileConfig (required)\n",
    "- `transformations`: List[TransformationConfig] (at least one required)\n",
    "- `quality_rules`: List[DataQualityRule] (default empty)\n",
    "- `destination`: DatabaseConfig (required)\n",
    "- `schedule`: Optional[str] (cron expression)\n",
    "- `enabled`: bool (default True)\n",
    "\n",
    "**Validators**:\n",
    "- Ensure `transformations` list has at least one item\n",
    "- Ensure all transformation names are unique\n",
    "- Ensure all quality rule names are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement ETLPipelineConfig\n",
    "class ETLPipelineConfig(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Create a complete ETL pipeline\n",
    "# etl = ETLPipelineConfig(\n",
    "#     name=\"daily_sales_etl\",\n",
    "#     source=FileConfig(\n",
    "#         path=\"/data/sales.csv\",\n",
    "#         format=FileFormat.CSV,\n",
    "#         options={\"delimiter\": \",\", \"header\": True}\n",
    "#     ),\n",
    "#     transformations=[\n",
    "#         TransformationConfig(\n",
    "#             name=\"filter_valid\",\n",
    "#             sql=\"SELECT * FROM source WHERE amount > 0\"\n",
    "#         ),\n",
    "#         TransformationConfig(\n",
    "#             name=\"add_timestamp\",\n",
    "#             sql=\"SELECT *, CURRENT_TIMESTAMP as processed_at FROM filtered\"\n",
    "#         )\n",
    "#     ],\n",
    "#     quality_rules=[\n",
    "#         DataQualityRule(\n",
    "#             rule_name=\"check_amount\",\n",
    "#             column=\"amount\",\n",
    "#             rule_type=\"range\",\n",
    "#             parameters={\"min\": 0}\n",
    "#         )\n",
    "#     ],\n",
    "#     destination=DatabaseConfig(\n",
    "#         host=\"warehouse.example.com\",\n",
    "#         database=\"analytics\",\n",
    "#         username=\"etl_user\"\n",
    "#     ),\n",
    "#     schedule=\"0 2 * * *\"  # Daily at 2 AM\n",
    "# )\n",
    "# print(etl.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Advanced - Union Types\n",
    "\n",
    "**Goal**: Handle multiple source types in one pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Create `APIConfig` model:\n",
    "  - `url`: str (required)\n",
    "  - `method`: Literal[\"GET\", \"POST\"] (default \"GET\")\n",
    "  - `headers`: Dict[str, str] (default empty)\n",
    "  - `timeout`: int (default 30, must be positive)\n",
    "- Create `Source` type alias as Union[FileConfig, DatabaseConfig, APIConfig]\n",
    "- Modify ETLPipelineConfig to accept `source: Source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement APIConfig and Source union\n",
    "class APIConfig(BaseModel):\n",
    "    pass\n",
    "\n",
    "# Source = Union[FileConfig, DatabaseConfig, APIConfig]\n",
    "\n",
    "# class FlexibleETLConfig(BaseModel):\n",
    "#     name: str\n",
    "#     source: Source  # Now accepts any source type!\n",
    "#     # ... rest of fields\n",
    "\n",
    "# Test with different source types\n",
    "# file_etl = FlexibleETLConfig(\n",
    "#     name=\"file_pipeline\",\n",
    "#     source=FileConfig(path=\"/data/file.csv\", format=FileFormat.CSV)\n",
    "# )\n",
    "\n",
    "# api_etl = FlexibleETLConfig(\n",
    "#     name=\"api_pipeline\",\n",
    "#     source=APIConfig(url=\"https://api.example.com/data\")\n",
    "# )\n",
    "\n",
    "# db_etl = FlexibleETLConfig(\n",
    "#     name=\"db_pipeline\",\n",
    "#     source=DatabaseConfig(host=\"db.example.com\", database=\"src\", username=\"reader\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Schema Migration\n",
    "\n",
    "**Challenge**: Create a model for database schema migrations.\n",
    "\n",
    "**Requirements**:\n",
    "- `version`: str (format: \"vX.Y.Z\" where X, Y, Z are integers)\n",
    "- `description`: str (required)\n",
    "- `up_sql`: str (SQL to apply migration)\n",
    "- `down_sql`: str (SQL to rollback migration)\n",
    "- `applied_at`: Optional[str] (ISO timestamp)\n",
    "- `checksum`: Optional[str] (MD5 hash of up_sql)\n",
    "\n",
    "**Validators**:\n",
    "- Validate version format with regex\n",
    "- Ensure up_sql and down_sql are not empty\n",
    "- Auto-compute checksum from up_sql if not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SchemaMigration\n",
    "# Hint: Use field_validator and model_validator\n",
    "# Hint: For checksum, use hashlib.md5\n",
    "\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "class SchemaMigration(BaseModel):\n",
    "    pass\n",
    "\n",
    "# migration = SchemaMigration(\n",
    "#     version=\"v1.0.0\",\n",
    "#     description=\"Add user_id column\",\n",
    "#     up_sql=\"ALTER TABLE orders ADD COLUMN user_id INTEGER\",\n",
    "#     down_sql=\"ALTER TABLE orders DROP COLUMN user_id\"\n",
    "# )\n",
    "# print(migration.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Completion Checklist\n",
    "\n",
    "- [ ] Exercise 1: DatabaseConfig with constraints\n",
    "- [ ] Exercise 2: Production environment validation\n",
    "- [ ] Exercise 3: SQL transformation config\n",
    "- [ ] Exercise 4: File format config with validators\n",
    "- [ ] Exercise 5: Data quality rules\n",
    "- [ ] Exercise 6: Complete ETL pipeline config\n",
    "- [ ] Exercise 7: Union types for multiple sources\n",
    "- [ ] Bonus: Schema migration model\n",
    "\n",
    "Once complete, check your solutions against `solutions.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
