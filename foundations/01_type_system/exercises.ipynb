{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Python Type System & Validation\n",
    "\n",
    "Practice what you've learned by completing these exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "from typing import List, Dict, Optional, Union, Literal, Any\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Database Connection Config\n",
    "\n",
    "**Goal**: Create a `DatabaseConfig` model with proper types and constraints.\n",
    "\n",
    "**Requirements**:\n",
    "- `host`: str (required)\n",
    "- `port`: int (default 5432, must be between 1-65535)\n",
    "- `database`: str (required)\n",
    "- `username`: str (required)\n",
    "- `password`: Optional[str] (for security, make it optional)\n",
    "- `ssl_enabled`: bool (default True)\n",
    "- `timeout`: int (default 30, must be positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"host\": \"postgres.example.com\",\n",
      "  \"port\": 5432,\n",
      "  \"database\": \"analytics\",\n",
      "  \"username\": \"analyst\",\n",
      "  \"password\": null,\n",
      "  \"ssl_enabled\": true,\n",
      "  \"timeout\": 30\n",
      "}\n",
      "âœ… Validation caught error: 1 validation error for DatabaseConfig\n",
      "port\n",
      "  Input should be less than or equal to 65535 [type=less_than_equal, input_value=99999, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/less_than_equal\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement DatabaseConfig\n",
    "class DatabaseConfig(BaseModel):\n",
    "    host: str\n",
    "    port: int = Field(default=5432, ge=1, le=65535)\n",
    "    database: str\n",
    "    username: str\n",
    "    password: Optional[str] = None\n",
    "    ssl_enabled: bool = Field(default=True)\n",
    "    timeout: int = Field(default=30,ge=0)\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "db = DatabaseConfig(\n",
    "    host=\"postgres.example.com\",\n",
    "    database=\"analytics\",\n",
    "    username=\"analyst\"\n",
    ")\n",
    "print(db.model_dump_json(indent=2))\n",
    "\n",
    "# Test validation - this should fail (port too high)\n",
    "try:\n",
    "    bad_db = DatabaseConfig(\n",
    "        host=\"localhost\",\n",
    "        port=99999,\n",
    "        database=\"test\",\n",
    "        username=\"user\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âœ… Validation caught error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Production Environment Validation\n",
    "\n",
    "**Goal**: Extend `DatabaseConfig` to prevent localhost in production.\n",
    "\n",
    "**Requirements**:\n",
    "- Add `environment`: Literal[\"dev\", \"staging\", \"prod\"]\n",
    "- Add a `@model_validator` that ensures:\n",
    "  - If `environment == \"prod\"`, `host` cannot be \"localhost\" or \"127.0.0.1\"\n",
    "  - If `environment == \"prod\"`, `ssl_enabled` must be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dev with localhost OK\n",
      "âœ… Caught prod localhost error: 1 validation error for ProductionDatabaseConfig\n",
      "  Value error, host cannot be 'localhost' or '127.0.0.1' [type=value_error, input_value={'host': 'localhost', 'da..., 'environment': 'prod'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n",
      "âœ… Caught prod SSL error: 1 validation error for ProductionDatabaseConfig\n",
      "  Value error, ssl_enabled must be set to True [type=value_error, input_value={'host': 'prod.example.co...', 'ssl_enabled': False}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ProductionDatabaseConfig\n",
    "class ProductionDatabaseConfig(BaseModel):\n",
    "    host: str\n",
    "    port: int = Field(default=5432, ge=1, le=65535)\n",
    "    database: str\n",
    "    username: str\n",
    "    password: Optional[str] = None\n",
    "    ssl_enabled: bool = Field(default=True)\n",
    "    timeout: int = Field(default=30,ge=0)\n",
    "    environment: Literal[\"dev\",\"staging\",\"prod\"]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_production(self):\n",
    "        if self.environment == \"prod\":\n",
    "            if self.host == \"localhost\" or self.host == \"127.0.0.1\":\n",
    "                raise ValueError(\"host cannot be 'localhost' or '127.0.0.1'\")\n",
    "            elif self.ssl_enabled == False:\n",
    "                raise ValueError(\"ssl_enabled must be set to True\")\n",
    "        return self\n",
    "\n",
    "# Test - this should work\n",
    "dev_db = ProductionDatabaseConfig(\n",
    "    host=\"localhost\",\n",
    "    database=\"test\",\n",
    "    username=\"dev\",\n",
    "    environment=\"dev\"\n",
    ")\n",
    "print(\"âœ… Dev with localhost OK\")\n",
    "\n",
    "# Test - this should fail (localhost in prod)\n",
    "try:\n",
    "    prod_db = ProductionDatabaseConfig(\n",
    "        host=\"localhost\",\n",
    "        database=\"prod_db\",\n",
    "        username=\"prod_user\",\n",
    "        environment=\"prod\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Caught prod localhost error: {e}\")\n",
    "\n",
    "# Test - this should fail (SSL disabled in prod)\n",
    "try:\n",
    "    prod_db = ProductionDatabaseConfig(\n",
    "        host=\"prod.example.com\",\n",
    "        database=\"prod_db\",\n",
    "        username=\"prod_user\",\n",
    "        environment=\"prod\",\n",
    "        ssl_enabled=False\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Caught prod SSL error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: SQL Transformation Config\n",
    "\n",
    "**Goal**: Create a model for SQL transformations.\n",
    "\n",
    "**Requirements**:\n",
    "- `name`: str (must be valid Python identifier)\n",
    "- `sql`: str (required, cannot be empty or whitespace)\n",
    "- `description`: Optional[str]\n",
    "- `parameters`: Dict[str, Any] (default empty dict)\n",
    "- `enabled`: bool (default True)\n",
    "\n",
    "**Validators**:\n",
    "- Validate `name` is a valid Python identifier (use `str.isidentifier()`)\n",
    "- Validate `sql` is not empty or just whitespace (use `str.strip()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": null,\n",
      "  \"sql\": null,\n",
      "  \"description\": \"Remove negative amounts\",\n",
      "  \"parameters\": {\n",
      "    \"min_amount\": 0\n",
      "  },\n",
      "  \"enabled\": true\n",
      "}\n",
      "âœ… Invalid name caught: 2 validation errors for TransformationConfig\n",
      "name\n",
      "  Value error, name needs to be a valid python identifier [type=value_error, input_value='clean-sales', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n",
      "description\n",
      "  Field required [type=missing, input_value={'name': 'clean-sales', 'sql': 'SELECT 1'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/missing\n",
      "âœ… Empty SQL caught: 2 validation errors for TransformationConfig\n",
      "sql\n",
      "  Value error, sql cannot be empty or have whitespace [type=value_error, input_value='   ', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n",
      "description\n",
      "  Field required [type=missing, input_value={'name': 'test', 'sql': '   '}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/missing\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement TransformationConfig\n",
    "class TransformationConfig(BaseModel):\n",
    "    name: str\n",
    "    sql: str\n",
    "    description: Optional[str]\n",
    "    parameters: Dict[str, Any] = Field(default=dict)\n",
    "    enabled: bool = True\n",
    "\n",
    "    @field_validator(\"name\")\n",
    "    @classmethod\n",
    "    def validate_name(cls,v: str) -> str:\n",
    "        if not v.isidentifier():\n",
    "            raise ValueError(\n",
    "                \"name needs to be a valid python identifier\"\n",
    "            )\n",
    "    @field_validator(\"sql\")\n",
    "    @classmethod\n",
    "    def validate_sql(cls,v: str) -> str:\n",
    "        if not v.strip():\n",
    "            raise ValueError(\n",
    "                \"sql cannot be empty or have whitespace\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "transform = TransformationConfig(\n",
    "    name=\"clean_sales\",\n",
    "    sql=\"SELECT * FROM sales WHERE amount > 0\",\n",
    "    description=\"Remove negative amounts\",\n",
    "    parameters={\"min_amount\": 0}\n",
    ")\n",
    "print(transform.model_dump_json(indent=2))\n",
    "\n",
    "# Test - invalid name (has hyphen)\n",
    "try:\n",
    "    bad_transform = TransformationConfig(\n",
    "        name=\"clean-sales\",\n",
    "        sql=\"SELECT 1\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Invalid name caught: {e}\")\n",
    "\n",
    "# Test - empty SQL\n",
    "try:\n",
    "    bad_transform = TransformationConfig(\n",
    "        name=\"test\",\n",
    "        sql=\"   \"  # Just whitespace\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Empty SQL caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: File Format Config\n",
    "\n",
    "**Goal**: Create configs for different file formats with format-specific options.\n",
    "\n",
    "**Requirements**:\n",
    "- Create `FileFormat` enum: CSV, PARQUET, JSON, AVRO\n",
    "- Create `CompressionType` enum: NONE, GZIP, SNAPPY, LZ4\n",
    "- Create `FileConfig` with:\n",
    "  - `path`: str (required)\n",
    "  - `format`: FileFormat (required)\n",
    "  - `compression`: CompressionType (default NONE)\n",
    "  - `options`: Dict[str, Any] (default empty)\n",
    "  - Add validator: if format is CSV, options can have \"delimiter\" and \"header\"\n",
    "  - Add validator: PARQUET and AVRO cannot use GZIP (not supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path='/data/sales.csv' format=<FileFormat.CSV: 'csv'> compression=<CompressionType.NONE: 'none'> options={'delimiter': '|', 'header': True}\n",
      "path='/data/sales.parquet' format=<FileFormat.PARQUET: 'parquet'> compression=<CompressionType.SNAPPY: 'snappy'> options={}\n",
      "âœ… Invalid compression caught: 1 validation error for FileConfig\n",
      "  Value error, parquet format does not support GZIP compression. Use SNAPPY or LZ4 instead. [type=value_error, input_value={'path': '/data/test.parq...ssionType.GZIP: 'gzip'>}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FileFormat, CompressionType, and FileConfig\n",
    "\n",
    "class FileFormat(str, Enum):\n",
    "    CSV = \"csv\"\n",
    "    PARQUET = \"parquet\"\n",
    "    JSON = \"json\"\n",
    "    AVRO = \"avro\"\n",
    "\n",
    "class CompressionType(str, Enum):\n",
    "    NONE = \"none\"\n",
    "    GZIP = \"gzip\"\n",
    "    SNAPPY = \"snappy\"\n",
    "    LZ4 = \"lz4\"\n",
    "\n",
    "class FileConfig(BaseModel):\n",
    "    path: str\n",
    "    format: FileFormat\n",
    "    compression: CompressionType = CompressionType.NONE\n",
    "    options: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_compression_compatibility(self):\n",
    "        \"\"\"Ensure compression is compatible with format.\"\"\"\n",
    "        # GZIP not supported for PARQUET and AVRO (they have their own)\n",
    "        if self.format in [FileFormat.PARQUET, FileFormat.AVRO]:\n",
    "            if self.compression == CompressionType.GZIP:\n",
    "                raise ValueError(\n",
    "                    f\"{self.format.value} format does not support GZIP compression. \"\n",
    "                    \"Use SNAPPY or LZ4 instead.\"\n",
    "                )\n",
    "        elif self.format == FileFormat.CSV:\n",
    "            if not all(k in self.options for k in [\"delimiter\", \"header\"]):\n",
    "                raise ValueError(\"Must set delimiter and header when using CSV\")\n",
    "\n",
    "\n",
    "\n",
    "        return self\n",
    "# Test CSV with options\n",
    "csv_file = FileConfig(\n",
    "    path=\"/data/sales.csv\",\n",
    "    format=FileFormat.CSV,\n",
    "    options={\"delimiter\": \"|\", \"header\": True}\n",
    ")\n",
    "print(csv_file)\n",
    "\n",
    "# Test Parquet with compression\n",
    "parquet_file = FileConfig(\n",
    "    path=\"/data/sales.parquet\",\n",
    "    format=FileFormat.PARQUET,\n",
    "    compression=CompressionType.SNAPPY\n",
    ")\n",
    "print(parquet_file)\n",
    "\n",
    "# Test invalid combination (Parquet + GZIP)\n",
    "try:\n",
    "    bad_file = FileConfig(\n",
    "        path=\"/data/test.parquet\",\n",
    "        format=FileFormat.PARQUET,\n",
    "        compression=CompressionType.GZIP\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Invalid compression caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Data Quality Rules\n",
    "\n",
    "**Goal**: Create a model for data quality validation rules.\n",
    "\n",
    "**Requirements**:\n",
    "- `rule_name`: str (valid identifier)\n",
    "- `column`: str (required)\n",
    "- `rule_type`: Literal[\"not_null\", \"unique\", \"range\", \"regex\", \"custom\"]\n",
    "- `parameters`: Dict[str, Any] (default empty)\n",
    "- `severity`: Literal[\"warning\", \"error\"] (default \"error\")\n",
    "\n",
    "**Validators**:\n",
    "- If `rule_type == \"range\"`, parameters must have \"min\" or \"max\"\n",
    "- If `rule_type == \"regex\"`, parameters must have \"pattern\"\n",
    "- If `rule_type == \"custom\"`, parameters must have \"function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_name='check_customer_id' column='customer_id' rule_type='not_null' parameters={} severity='error'\n",
      "rule_name='check_age_range' column='age' rule_type='range' parameters={'min': 0, 'max': 120} severity='error'\n",
      "âœ… Missing range parameters caught: 1 validation error for DataQualityRule\n",
      "  Value error, Range rule requires at least 'min' or 'max' in parameters [type=value_error, input_value={'rule_name': 'bad_range'...', 'rule_type': 'range'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement DataQualityRule\n",
    "class DataQualityRule(BaseModel):\n",
    "    rule_name: str\n",
    "    column: str\n",
    "    rule_type: Literal[\"not_null\", \"unique\", \"range\", \"regex\", \"custom\"]\n",
    "    parameters: Dict[str, Any] = Field(default_factory=dict)\n",
    "    severity: Literal[\"warning\", \"error\"] = Field(default=\"error\")\n",
    "\n",
    "    @field_validator(\"rule_type\")\n",
    "    @classmethod\n",
    "    def validate_rule_name(cls, v: str) -> str:\n",
    "        \"\"\"Ensure rule name is valid identifier.\"\"\"\n",
    "        if not v.isidentifier():\n",
    "            raise ValueError(f\"Rule name must be valid identifier: {v}\")\n",
    "        return v\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_parameters(self):\n",
    "        \"\"\"Validate parameters based on rule type.\"\"\"\n",
    "        if self.rule_type == \"range\":\n",
    "            if \"min\" not in self.parameters and \"max\" not in self.parameters:\n",
    "                raise ValueError(\n",
    "                    \"Range rule requires at least 'min' or 'max' in parameters\"\n",
    "                )\n",
    "        \n",
    "        elif self.rule_type == \"regex\":\n",
    "            if \"pattern\" not in self.parameters:\n",
    "                raise ValueError(\n",
    "                    \"Regex rule requires 'pattern' in parameters\"\n",
    "                )\n",
    "        \n",
    "        elif self.rule_type == \"custom\":\n",
    "            if \"function\" not in self.parameters:\n",
    "                raise ValueError(\n",
    "                    \"Custom rule requires 'function' in parameters\"\n",
    "                )\n",
    "        \n",
    "        return self\n",
    "# Test not_null rule\n",
    "rule1 = DataQualityRule(\n",
    "    rule_name=\"check_customer_id\",\n",
    "    column=\"customer_id\",\n",
    "    rule_type=\"not_null\"\n",
    ")\n",
    "print(rule1)\n",
    "\n",
    "# Test range rule\n",
    "rule2 = DataQualityRule(\n",
    "    rule_name=\"check_age_range\",\n",
    "    column=\"age\",\n",
    "    rule_type=\"range\",\n",
    "    parameters={\"min\": 0, \"max\": 120}\n",
    ")\n",
    "print(rule2)\n",
    "\n",
    "# Test invalid range rule (missing parameters)\n",
    "try:\n",
    "    bad_rule = DataQualityRule(\n",
    "        rule_name=\"bad_range\",\n",
    "        column=\"value\",\n",
    "        rule_type=\"range\"\n",
    "        # Missing min/max!\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Missing range parameters caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Complete ETL Pipeline Config\n",
    "\n",
    "**Goal**: Combine all previous models into a complete ETL configuration.\n",
    "\n",
    "**Requirements**:\n",
    "- `name`: str (required)\n",
    "- `source`: FileConfig (required)\n",
    "- `transformations`: List[TransformationConfig] (at least one required)\n",
    "- `quality_rules`: List[DataQualityRule] (default empty)\n",
    "- `destination`: DatabaseConfig (required)\n",
    "- `schedule`: Optional[str] (cron expression)\n",
    "- `enabled`: bool (default True)\n",
    "\n",
    "**Validators**:\n",
    "- Ensure `transformations` list has at least one item\n",
    "- Ensure all transformation names are unique\n",
    "- Ensure all quality rule names are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"daily_sales_etl\",\n",
      "  \"source\": {\n",
      "    \"path\": \"/data/sales.csv\",\n",
      "    \"format\": \"csv\",\n",
      "    \"compression\": \"none\",\n",
      "    \"options\": {\n",
      "      \"delimiter\": \",\",\n",
      "      \"header\": true\n",
      "    }\n",
      "  },\n",
      "  \"transformations\": [\n",
      "    {\n",
      "      \"name\": \"filter_valid\",\n",
      "      \"sql\": \"SELECT * FROM source WHERE amount > 0\",\n",
      "      \"description\": \"Remove invalid sales\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"add_timestamp\",\n",
      "      \"sql\": \"SELECT *, CURRENT_TIMESTAMP as processed_at FROM filtered\",\n",
      "      \"description\": \"Add processing timestamp\"\n",
      "    }\n",
      "  ],\n",
      "  \"quality_rules\": [\n",
      "    {\n",
      "      \"rule_name\": \"check_amount\",\n",
      "      \"column\": \"amount\",\n",
      "      \"rule_type\": \"range\",\n",
      "      \"parameters\": {\n",
      "        \"min\": 0\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"destination\": {\n",
      "    \"host\": \"warehouse.example.com\",\n",
      "    \"database\": \"analytics\",\n",
      "    \"username\": \"etl_user\",\n",
      "    \"password\": null\n",
      "  },\n",
      "  \"schedule\": \"0 2 * * *\",\n",
      "  \"enabled\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ETLPipelineConfig\n",
    "class ETLPipelineConfig(BaseModel):\n",
    "    name: str\n",
    "    source: FileConfig\n",
    "    transformations: List[TransformationConfig] = Field(min_length=1)\n",
    "    quality_rules: List[DataQualityRule] = Field(default_factory=list)\n",
    "    destination: DatabaseConfig\n",
    "    schedule: Optional[str] = None\n",
    "    enabled: bool = Field(default=True)\n",
    "\n",
    "    @field_validator('transformations')\n",
    "    @classmethod\n",
    "    def check_unique_transformation_names(cls, transformations: List[TransformationConfig]):\n",
    "        \"\"\"Ensure all transformation names are unique.\"\"\"\n",
    "        names = [t.name for t in transformations]\n",
    "        if len(names) != len(set(names)):\n",
    "            duplicates = [name for name in names if names.count(name) > 1]\n",
    "            raise ValueError(f\"Duplicate transformation names: {set(duplicates)}\")\n",
    "        return transformations\n",
    "    \n",
    "    @field_validator('quality_rules')\n",
    "    @classmethod\n",
    "    def check_unique_rule_names(cls, rules: List[DataQualityRule]):\n",
    "        \"\"\"Ensure all quality rule names are unique.\"\"\"\n",
    "        if not rules:\n",
    "            return rules\n",
    "        names = [r.rule_name for r in rules]\n",
    "        if len(names) != len(set(names)):\n",
    "            duplicates = [name for name in names if names.count(name) > 1]\n",
    "            raise ValueError(f\"Duplicate quality rule names: {set(duplicates)}\")\n",
    "        return rules\n",
    "# Create a complete ETL pipeline\n",
    "etl = ETLPipelineConfig(\n",
    "    name=\"daily_sales_etl\",\n",
    "    source=FileConfig(\n",
    "        path=\"/data/sales.csv\",\n",
    "        format=FileFormat.CSV,\n",
    "        options={\"delimiter\": \",\", \"header\": True}\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(\n",
    "            name=\"filter_valid\",\n",
    "            sql=\"SELECT * FROM source WHERE amount > 0\",\n",
    "            description=\"Remove invalid sales\"\n",
    "\n",
    "        ),\n",
    "        TransformationConfig(\n",
    "            name=\"add_timestamp\",\n",
    "            sql=\"SELECT *, CURRENT_TIMESTAMP as processed_at FROM filtered\",\n",
    "            description=\"Add processing timestamp\"\n",
    "        )\n",
    "    ],\n",
    "    quality_rules=[\n",
    "        DataQualityRule(\n",
    "            rule_name=\"check_amount\",\n",
    "            column=\"amount\",\n",
    "            rule_type=\"range\",\n",
    "            parameters={\"min\": 0}\n",
    "        )\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.example.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl_user\"\n",
    "    ),\n",
    "    schedule=\"0 2 * * *\"  # Daily at 2 AM\n",
    ")\n",
    "print(etl.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Advanced - Union Types\n",
    "\n",
    "**Goal**: Handle multiple source types in one pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Create `APIConfig` model:\n",
    "  - `url`: str (required)\n",
    "  - `method`: Literal[\"GET\", \"POST\"] (default \"GET\")\n",
    "  - `headers`: Dict[str, str] (default empty)\n",
    "  - `timeout`: int (default 30, must be positive)\n",
    "- Create `Source` type alias as Union[FileConfig, DatabaseConfig, APIConfig]\n",
    "- Modify ETLPipelineConfig to accept `source: Source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File source ETL\n",
      "âœ… API source ETL\n",
      "âœ… Database source ETL\n",
      "\n",
      "âœ… All source types work with FlexibleETLConfig!\n"
     ]
    }
   ],
   "source": [
    "class APIConfig(BaseModel):\n",
    "    \"\"\"API data source configuration.\"\"\"\n",
    "    url: str\n",
    "    method: Literal[\"GET\", \"POST\"] = \"GET\"\n",
    "    headers: Dict[str, str] = Field(default_factory=dict)\n",
    "    timeout: int = Field(default=30, gt=0)\n",
    "    \n",
    "    @field_validator('url')\n",
    "    @classmethod\n",
    "    def validate_url(cls, v: str) -> str:\n",
    "        \"\"\"Ensure URL starts with http:// or https://.\"\"\"\n",
    "        if not v.startswith((\"http://\", \"https://\")):\n",
    "            raise ValueError(\"URL must start with http:// or https://\")\n",
    "        return v\n",
    "\n",
    "# Union type for flexible sources\n",
    "Source = Union[FileConfig, DatabaseConfig, APIConfig]\n",
    "\n",
    "class FlexibleETLConfig(BaseModel):\n",
    "    \"\"\"ETL config that accepts any source type.\"\"\"\n",
    "    name: str\n",
    "    source: Source\n",
    "    transformations: List[TransformationConfig] = Field(min_length=1)\n",
    "    quality_rules: List[DataQualityRule] = Field(default_factory=list)\n",
    "    destination: DatabaseConfig\n",
    "    schedule: Optional[str] = None\n",
    "    enabled: bool = True\n",
    "\n",
    "# Test with file source\n",
    "file_etl = FlexibleETLConfig(\n",
    "    name=\"file_pipeline\",\n",
    "    source=FileConfig(\n",
    "        path=\"/data/file.csv\",\n",
    "        format=FileFormat.PARQUET\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(name=\"transform1\", sql=\"SELECT * FROM source\")\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl\"\n",
    "    )\n",
    ")\n",
    "print(\"âœ… File source ETL\")\n",
    "\n",
    "# Test with API source\n",
    "api_etl = FlexibleETLConfig(\n",
    "    name=\"api_pipeline\",\n",
    "    source=APIConfig(\n",
    "        url=\"https://api.example.com/data\",\n",
    "        method=\"GET\",\n",
    "        headers={\"Authorization\": \"Bearer token\"}\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(name=\"parse_json\", sql=\"SELECT * FROM json_table(source)\")\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl\"\n",
    "    )\n",
    ")\n",
    "print(\"âœ… API source ETL\")\n",
    "\n",
    "# Test with database source\n",
    "db_etl = FlexibleETLConfig(\n",
    "    name=\"db_pipeline\",\n",
    "    source=DatabaseConfig(\n",
    "        host=\"source-db.example.com\",\n",
    "        database=\"production\",\n",
    "        username=\"reader\"\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(name=\"aggregate\", sql=\"SELECT category, SUM(amount) FROM source GROUP BY category\")\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl\"\n",
    "    )\n",
    ")\n",
    "print(\"âœ… Database source ETL\")\n",
    "\n",
    "print(\"\\nâœ… All source types work with FlexibleETLConfig!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Schema Migration\n",
    "\n",
    "**Challenge**: Create a model for database schema migrations.\n",
    "\n",
    "**Requirements**:\n",
    "- `version`: str (format: \"vX.Y.Z\" where X, Y, Z are integers)\n",
    "- `description`: str (required)\n",
    "- `up_sql`: str (SQL to apply migration)\n",
    "- `down_sql`: str (SQL to rollback migration)\n",
    "- `applied_at`: Optional[str] (ISO timestamp)\n",
    "- `checksum`: Optional[str] (MD5 hash of up_sql)\n",
    "\n",
    "**Validators**:\n",
    "- Validate version format with regex\n",
    "- Ensure up_sql and down_sql are not empty\n",
    "- Auto-compute checksum from up_sql if not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": \"v1.0.0\",\n",
      "  \"description\": \"Add user_id column to orders table\",\n",
      "  \"up_sql\": \"ALTER TABLE orders ADD COLUMN user_id INTEGER NOT NULL\",\n",
      "  \"down_sql\": \"ALTER TABLE orders DROP COLUMN user_id\",\n",
      "  \"applied_at\": null,\n",
      "  \"checksum\": \"935995d44a3d3b96e8c38a14e15e877b\"\n",
      "}\n",
      "\n",
      "âœ… Checksum auto-computed: 935995d44a3d3b96e8c38a14e15e877b\n",
      "\n",
      "âœ… Invalid version caught: 1 validation error for SchemaMigration\n",
      "version\n",
      "  Value error, Version must be in format vX.Y.Z (e.g., v1.0.0), got: 1.0.0 [type=value_error, input_value='1.0.0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n",
      "\n",
      "âœ… Manual checksum preserved: custom_checksum_123\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement SchemaMigration\n",
    "# Hint: Use field_validator and model_validator\n",
    "# Hint: For checksum, use hashlib.md5\n",
    "\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "class SchemaMigration(BaseModel):\n",
    "    \"\"\"Database schema migration model.\"\"\"\n",
    "    version: str\n",
    "    description: str\n",
    "    up_sql: str\n",
    "    down_sql: str\n",
    "    applied_at: Optional[str] = None\n",
    "    checksum: Optional[str] = None\n",
    "    \n",
    "    @field_validator('version')\n",
    "    @classmethod\n",
    "    def validate_version(cls, v: str) -> str:\n",
    "        \"\"\"Ensure version follows vX.Y.Z format.\"\"\"\n",
    "        pattern = r'^v\\d+\\.\\d+\\.\\d+$'\n",
    "        if not re.match(pattern, v):\n",
    "            raise ValueError(\n",
    "                f\"Version must be in format vX.Y.Z (e.g., v1.0.0), got: {v}\"\n",
    "            )\n",
    "        return v\n",
    "    \n",
    "    @field_validator('up_sql', 'down_sql')\n",
    "    @classmethod\n",
    "    def validate_sql_not_empty(cls, v: str) -> str:\n",
    "        \"\"\"Ensure SQL is not empty or whitespace.\"\"\"\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"SQL cannot be empty or whitespace\")\n",
    "        return v\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def compute_checksum(self):\n",
    "        \"\"\"Auto-compute checksum from up_sql if not provided.\"\"\"\n",
    "        if self.checksum is None:\n",
    "            self.checksum = hashlib.md5(self.up_sql.encode()).hexdigest()\n",
    "        return self\n",
    "\n",
    "# Test migration\n",
    "migration = SchemaMigration(\n",
    "    version=\"v1.0.0\",\n",
    "    description=\"Add user_id column to orders table\",\n",
    "    up_sql=\"ALTER TABLE orders ADD COLUMN user_id INTEGER NOT NULL\",\n",
    "    down_sql=\"ALTER TABLE orders DROP COLUMN user_id\"\n",
    ")\n",
    "print(migration.model_dump_json(indent=2))\n",
    "print(f\"\\nâœ… Checksum auto-computed: {migration.checksum}\")\n",
    "\n",
    "# Test invalid version\n",
    "try:\n",
    "    bad_migration = SchemaMigration(\n",
    "        version=\"1.0.0\",  # Missing 'v' prefix\n",
    "        description=\"Test\",\n",
    "        up_sql=\"SELECT 1\",\n",
    "        down_sql=\"SELECT 0\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"\\nâœ… Invalid version caught: {e}\")\n",
    "\n",
    "# Test manual checksum\n",
    "migration_with_checksum = SchemaMigration(\n",
    "    version=\"v2.0.0\",\n",
    "    description=\"Add index\",\n",
    "    up_sql=\"CREATE INDEX idx_user_id ON orders(user_id)\",\n",
    "    down_sql=\"DROP INDEX idx_user_id\",\n",
    "    checksum=\"custom_checksum_123\"\n",
    ")\n",
    "print(f\"\\nâœ… Manual checksum preserved: {migration_with_checksum.checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Completion Checklist\n",
    "\n",
    "- [ ] Exercise 1: DatabaseConfig with constraints\n",
    "- [ ] Exercise 2: Production environment validation\n",
    "- [ ] Exercise 3: SQL transformation config\n",
    "- [ ] Exercise 4: File format config with validators\n",
    "- [ ] Exercise 5: Data quality rules\n",
    "- [ ] Exercise 6: Complete ETL pipeline config\n",
    "- [ ] Exercise 7: Union types for multiple sources\n",
    "- [ ] Bonus: Schema migration model\n",
    "\n",
    "Once complete, check your solutions against `solutions.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
