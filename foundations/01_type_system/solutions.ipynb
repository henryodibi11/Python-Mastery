{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Python Type System & Validation\n",
    "\n",
    "Complete solutions to all exercises with explanations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "from typing import List, Dict, Optional, Union, Literal, Any\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Database Connection Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConfig(BaseModel):\n",
    "    \"\"\"Database connection configuration.\"\"\"\n",
    "    host: str\n",
    "    port: int = Field(default=5432, ge=1, le=65535)\n",
    "    database: str\n",
    "    username: str\n",
    "    password: Optional[str] = None\n",
    "    ssl_enabled: bool = True\n",
    "    timeout: int = Field(default=30, gt=0)\n",
    "\n",
    "# Test\n",
    "db = DatabaseConfig(\n",
    "    host=\"postgres.example.com\",\n",
    "    database=\"analytics\",\n",
    "    username=\"analyst\"\n",
    ")\n",
    "print(db.model_dump_json(indent=2))\n",
    "\n",
    "# Test validation\n",
    "try:\n",
    "    bad_db = DatabaseConfig(\n",
    "        host=\"localhost\",\n",
    "        port=99999,  # Too high!\n",
    "        database=\"test\",\n",
    "        username=\"user\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âœ… Validation caught error: Port constraint violated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- `ge=1, le=65535` ensures valid port range\n",
    "- `Optional[str]` allows password to be None\n",
    "- `gt=0` ensures timeout is positive\n",
    "- Default values reduce required fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Production Environment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDatabaseConfig(BaseModel):\n",
    "    \"\"\"Database config with production safety checks.\"\"\"\n",
    "    host: str\n",
    "    port: int = Field(default=5432, ge=1, le=65535)\n",
    "    database: str\n",
    "    username: str\n",
    "    password: Optional[str] = None\n",
    "    ssl_enabled: bool = True\n",
    "    timeout: int = Field(default=30, gt=0)\n",
    "    environment: Literal[\"dev\", \"staging\", \"prod\"] = \"dev\"\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_production_safety(self):\n",
    "        \"\"\"Ensure production environment is configured safely.\"\"\"\n",
    "        if self.environment == \"prod\":\n",
    "            # No localhost in production\n",
    "            if self.host in [\"localhost\", \"127.0.0.1\"]:\n",
    "                raise ValueError(\n",
    "                    \"Production environment cannot use localhost. \"\n",
    "                    \"Use a proper hostname or IP address.\"\n",
    "                )\n",
    "            # SSL must be enabled in production\n",
    "            if not self.ssl_enabled:\n",
    "                raise ValueError(\n",
    "                    \"Production environment must have SSL enabled for security.\"\n",
    "                )\n",
    "        return self\n",
    "\n",
    "# Test - dev with localhost is OK\n",
    "dev_db = ProductionDatabaseConfig(\n",
    "    host=\"localhost\",\n",
    "    database=\"test\",\n",
    "    username=\"dev\",\n",
    "    environment=\"dev\"\n",
    ")\n",
    "print(\"âœ… Dev with localhost OK\")\n",
    "\n",
    "# Test - prod with localhost fails\n",
    "try:\n",
    "    prod_db = ProductionDatabaseConfig(\n",
    "        host=\"localhost\",\n",
    "        database=\"prod_db\",\n",
    "        username=\"prod_user\",\n",
    "        environment=\"prod\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Caught prod localhost error: {e}\")\n",
    "\n",
    "# Test - prod without SSL fails\n",
    "try:\n",
    "    prod_db = ProductionDatabaseConfig(\n",
    "        host=\"prod.example.com\",\n",
    "        database=\"prod_db\",\n",
    "        username=\"prod_user\",\n",
    "        environment=\"prod\",\n",
    "        ssl_enabled=False\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Caught prod SSL error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- `@model_validator(mode=\"after\")` validates after all fields are set\n",
    "- Can access multiple fields in validation logic\n",
    "- Clear error messages help users fix issues\n",
    "- Different rules for different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: SQL Transformation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformationConfig(BaseModel):\n",
    "    \"\"\"Configuration for SQL transformations.\"\"\"\n",
    "    name: str\n",
    "    sql: str\n",
    "    description: Optional[str] = None\n",
    "    parameters: Dict[str, Any] = Field(default_factory=dict)\n",
    "    enabled: bool = True\n",
    "    \n",
    "    @field_validator('name')\n",
    "    @classmethod\n",
    "    def validate_name(cls, v: str) -> str:\n",
    "        \"\"\"Ensure name is valid Python identifier.\"\"\"\n",
    "        if not v.isidentifier():\n",
    "            raise ValueError(\n",
    "                f\"Name '{v}' must be a valid Python identifier \"\n",
    "                \"(letters, digits, underscore; cannot start with digit)\"\n",
    "            )\n",
    "        return v\n",
    "    \n",
    "    @field_validator('sql')\n",
    "    @classmethod\n",
    "    def validate_sql(cls, v: str) -> str:\n",
    "        \"\"\"Ensure SQL is not empty or whitespace.\"\"\"\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"SQL cannot be empty or whitespace\")\n",
    "        return v\n",
    "\n",
    "# Test valid transformation\n",
    "transform = TransformationConfig(\n",
    "    name=\"clean_sales\",\n",
    "    sql=\"SELECT * FROM sales WHERE amount > 0\",\n",
    "    description=\"Remove negative amounts\",\n",
    "    parameters={\"min_amount\": 0}\n",
    ")\n",
    "print(transform.model_dump_json(indent=2))\n",
    "\n",
    "# Test invalid name\n",
    "try:\n",
    "    bad_transform = TransformationConfig(\n",
    "        name=\"clean-sales\",  # Hyphen not allowed\n",
    "        sql=\"SELECT 1\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Invalid name caught: {e}\")\n",
    "\n",
    "# Test empty SQL\n",
    "try:\n",
    "    bad_transform = TransformationConfig(\n",
    "        name=\"test\",\n",
    "        sql=\"   \"  # Just whitespace\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Empty SQL caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- `@field_validator` validates individual fields\n",
    "- `@classmethod` required for field validators\n",
    "- `str.isidentifier()` checks for valid Python identifier\n",
    "- `str.strip()` removes whitespace for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: File Format Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileFormat(str, Enum):\n",
    "    \"\"\"Supported file formats.\"\"\"\n",
    "    CSV = \"csv\"\n",
    "    PARQUET = \"parquet\"\n",
    "    JSON = \"json\"\n",
    "    AVRO = \"avro\"\n",
    "\n",
    "class CompressionType(str, Enum):\n",
    "    \"\"\"Supported compression types.\"\"\"\n",
    "    NONE = \"none\"\n",
    "    GZIP = \"gzip\"\n",
    "    SNAPPY = \"snappy\"\n",
    "    LZ4 = \"lz4\"\n",
    "\n",
    "class FileConfig(BaseModel):\n",
    "    \"\"\"File configuration with format-specific validation.\"\"\"\n",
    "    path: str\n",
    "    format: FileFormat\n",
    "    compression: CompressionType = CompressionType.NONE\n",
    "    options: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_compression_compatibility(self):\n",
    "        \"\"\"Ensure compression is compatible with format.\"\"\"\n",
    "        # GZIP not supported for PARQUET and AVRO (they have their own)\n",
    "        if self.format in [FileFormat.PARQUET, FileFormat.AVRO]:\n",
    "            if self.compression == CompressionType.GZIP:\n",
    "                raise ValueError(\n",
    "                    f\"{self.format.value} format does not support GZIP compression. \"\n",
    "                    \"Use SNAPPY or LZ4 instead.\"\n",
    "                )\n",
    "        return self\n",
    "\n",
    "# Test CSV with options\n",
    "csv_file = FileConfig(\n",
    "    path=\"/data/sales.csv\",\n",
    "    format=FileFormat.CSV,\n",
    "    options={\"delimiter\": \"|\", \"header\": True}\n",
    ")\n",
    "print(csv_file.model_dump_json(indent=2))\n",
    "\n",
    "# Test Parquet with valid compression\n",
    "parquet_file = FileConfig(\n",
    "    path=\"/data/sales.parquet\",\n",
    "    format=FileFormat.PARQUET,\n",
    "    compression=CompressionType.SNAPPY\n",
    ")\n",
    "print(\"âœ… Parquet with SNAPPY OK\")\n",
    "\n",
    "# Test invalid combination\n",
    "try:\n",
    "    bad_file = FileConfig(\n",
    "        path=\"/data/test.parquet\",\n",
    "        format=FileFormat.PARQUET,\n",
    "        compression=CompressionType.GZIP\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Invalid compression caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- Enums provide type-safe constants\n",
    "- Model validator checks compatibility between fields\n",
    "- Clear error messages guide users to correct combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Data Quality Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityRule(BaseModel):\n",
    "    \"\"\"Data quality validation rule.\"\"\"\n",
    "    rule_name: str\n",
    "    column: str\n",
    "    rule_type: Literal[\"not_null\", \"unique\", \"range\", \"regex\", \"custom\"]\n",
    "    parameters: Dict[str, Any] = Field(default_factory=dict)\n",
    "    severity: Literal[\"warning\", \"error\"] = \"error\"\n",
    "    \n",
    "    @field_validator('rule_name')\n",
    "    @classmethod\n",
    "    def validate_rule_name(cls, v: str) -> str:\n",
    "        \"\"\"Ensure rule name is valid identifier.\"\"\"\n",
    "        if not v.isidentifier():\n",
    "            raise ValueError(f\"Rule name must be valid identifier: {v}\")\n",
    "        return v\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_parameters(self):\n",
    "        \"\"\"Validate parameters based on rule type.\"\"\"\n",
    "        if self.rule_type == \"range\":\n",
    "            if \"min\" not in self.parameters and \"max\" not in self.parameters:\n",
    "                raise ValueError(\n",
    "                    \"Range rule requires at least 'min' or 'max' in parameters\"\n",
    "                )\n",
    "        \n",
    "        elif self.rule_type == \"regex\":\n",
    "            if \"pattern\" not in self.parameters:\n",
    "                raise ValueError(\n",
    "                    \"Regex rule requires 'pattern' in parameters\"\n",
    "                )\n",
    "        \n",
    "        elif self.rule_type == \"custom\":\n",
    "            if \"function\" not in self.parameters:\n",
    "                raise ValueError(\n",
    "                    \"Custom rule requires 'function' in parameters\"\n",
    "                )\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Test not_null rule\n",
    "rule1 = DataQualityRule(\n",
    "    rule_name=\"check_customer_id\",\n",
    "    column=\"customer_id\",\n",
    "    rule_type=\"not_null\"\n",
    ")\n",
    "print(rule1.model_dump_json(indent=2))\n",
    "\n",
    "# Test range rule\n",
    "rule2 = DataQualityRule(\n",
    "    rule_name=\"check_age_range\",\n",
    "    column=\"age\",\n",
    "    rule_type=\"range\",\n",
    "    parameters={\"min\": 0, \"max\": 120},\n",
    "    severity=\"warning\"\n",
    ")\n",
    "print(\"âœ… Range rule with parameters OK\")\n",
    "\n",
    "# Test regex rule\n",
    "rule3 = DataQualityRule(\n",
    "    rule_name=\"check_email_format\",\n",
    "    column=\"email\",\n",
    "    rule_type=\"regex\",\n",
    "    parameters={\"pattern\": r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"}\n",
    ")\n",
    "print(\"âœ… Regex rule OK\")\n",
    "\n",
    "# Test invalid range rule\n",
    "try:\n",
    "    bad_rule = DataQualityRule(\n",
    "        rule_name=\"bad_range\",\n",
    "        column=\"value\",\n",
    "        rule_type=\"range\"\n",
    "        # Missing min/max!\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ… Missing range parameters caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- Different validation logic for different rule types\n",
    "- Model validator checks parameter requirements\n",
    "- Severity levels allow warnings vs errors\n",
    "- Flexible parameter dictionary for extensibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Complete ETL Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipelineConfig(BaseModel):\n",
    "    \"\"\"Complete ETL pipeline configuration.\"\"\"\n",
    "    name: str\n",
    "    source: FileConfig\n",
    "    transformations: List[TransformationConfig] = Field(min_length=1)\n",
    "    quality_rules: List[DataQualityRule] = Field(default_factory=list)\n",
    "    destination: DatabaseConfig\n",
    "    schedule: Optional[str] = None\n",
    "    enabled: bool = True\n",
    "    \n",
    "    @field_validator('transformations')\n",
    "    @classmethod\n",
    "    def check_unique_transformation_names(cls, transformations: List[TransformationConfig]):\n",
    "        \"\"\"Ensure all transformation names are unique.\"\"\"\n",
    "        names = [t.name for t in transformations]\n",
    "        if len(names) != len(set(names)):\n",
    "            duplicates = [name for name in names if names.count(name) > 1]\n",
    "            raise ValueError(f\"Duplicate transformation names: {set(duplicates)}\")\n",
    "        return transformations\n",
    "    \n",
    "    @field_validator('quality_rules')\n",
    "    @classmethod\n",
    "    def check_unique_rule_names(cls, rules: List[DataQualityRule]):\n",
    "        \"\"\"Ensure all quality rule names are unique.\"\"\"\n",
    "        if not rules:\n",
    "            return rules\n",
    "        names = [r.rule_name for r in rules]\n",
    "        if len(names) != len(set(names)):\n",
    "            duplicates = [name for name in names if names.count(name) > 1]\n",
    "            raise ValueError(f\"Duplicate quality rule names: {set(duplicates)}\")\n",
    "        return rules\n",
    "\n",
    "# Create complete ETL pipeline\n",
    "etl = ETLPipelineConfig(\n",
    "    name=\"daily_sales_etl\",\n",
    "    source=FileConfig(\n",
    "        path=\"/data/sales.csv\",\n",
    "        format=FileFormat.CSV,\n",
    "        options={\"delimiter\": \",\", \"header\": True}\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(\n",
    "            name=\"filter_valid\",\n",
    "            sql=\"SELECT * FROM source WHERE amount > 0\",\n",
    "            description=\"Remove invalid sales\"\n",
    "        ),\n",
    "        TransformationConfig(\n",
    "            name=\"add_timestamp\",\n",
    "            sql=\"SELECT *, CURRENT_TIMESTAMP as processed_at FROM filtered\",\n",
    "            description=\"Add processing timestamp\"\n",
    "        )\n",
    "    ],\n",
    "    quality_rules=[\n",
    "        DataQualityRule(\n",
    "            rule_name=\"check_amount\",\n",
    "            column=\"amount\",\n",
    "            rule_type=\"range\",\n",
    "            parameters={\"min\": 0}\n",
    "        ),\n",
    "        DataQualityRule(\n",
    "            rule_name=\"check_customer_id\",\n",
    "            column=\"customer_id\",\n",
    "            rule_type=\"not_null\"\n",
    "        )\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.example.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl_user\"\n",
    "    ),\n",
    "    schedule=\"0 2 * * *\"  # Daily at 2 AM\n",
    ")\n",
    "\n",
    "print(etl.model_dump_json(indent=2))\n",
    "print(\"\\nâœ… Complete ETL pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- Composition of multiple Pydantic models\n",
    "- `min_length=1` ensures at least one transformation\n",
    "- Unique name validation prevents conflicts\n",
    "- Complete type safety across entire config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 7: Union Types for Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIConfig(BaseModel):\n",
    "    \"\"\"API data source configuration.\"\"\"\n",
    "    url: str\n",
    "    method: Literal[\"GET\", \"POST\"] = \"GET\"\n",
    "    headers: Dict[str, str] = Field(default_factory=dict)\n",
    "    timeout: int = Field(default=30, gt=0)\n",
    "    \n",
    "    @field_validator('url')\n",
    "    @classmethod\n",
    "    def validate_url(cls, v: str) -> str:\n",
    "        \"\"\"Ensure URL starts with http:// or https://.\"\"\"\n",
    "        if not v.startswith((\"http://\", \"https://\")):\n",
    "            raise ValueError(\"URL must start with http:// or https://\")\n",
    "        return v\n",
    "\n",
    "# Union type for flexible sources\n",
    "Source = Union[FileConfig, DatabaseConfig, APIConfig]\n",
    "\n",
    "class FlexibleETLConfig(BaseModel):\n",
    "    \"\"\"ETL config that accepts any source type.\"\"\"\n",
    "    name: str\n",
    "    source: Source\n",
    "    transformations: List[TransformationConfig] = Field(min_length=1)\n",
    "    quality_rules: List[DataQualityRule] = Field(default_factory=list)\n",
    "    destination: DatabaseConfig\n",
    "    schedule: Optional[str] = None\n",
    "    enabled: bool = True\n",
    "\n",
    "# Test with file source\n",
    "file_etl = FlexibleETLConfig(\n",
    "    name=\"file_pipeline\",\n",
    "    source=FileConfig(\n",
    "        path=\"/data/file.csv\",\n",
    "        format=FileFormat.CSV\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(name=\"transform1\", sql=\"SELECT * FROM source\")\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl\"\n",
    "    )\n",
    ")\n",
    "print(\"âœ… File source ETL\")\n",
    "\n",
    "# Test with API source\n",
    "api_etl = FlexibleETLConfig(\n",
    "    name=\"api_pipeline\",\n",
    "    source=APIConfig(\n",
    "        url=\"https://api.example.com/data\",\n",
    "        method=\"GET\",\n",
    "        headers={\"Authorization\": \"Bearer token\"}\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(name=\"parse_json\", sql=\"SELECT * FROM json_table(source)\")\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl\"\n",
    "    )\n",
    ")\n",
    "print(\"âœ… API source ETL\")\n",
    "\n",
    "# Test with database source\n",
    "db_etl = FlexibleETLConfig(\n",
    "    name=\"db_pipeline\",\n",
    "    source=DatabaseConfig(\n",
    "        host=\"source-db.example.com\",\n",
    "        database=\"production\",\n",
    "        username=\"reader\"\n",
    "    ),\n",
    "    transformations=[\n",
    "        TransformationConfig(name=\"aggregate\", sql=\"SELECT category, SUM(amount) FROM source GROUP BY category\")\n",
    "    ],\n",
    "    destination=DatabaseConfig(\n",
    "        host=\"warehouse.com\",\n",
    "        database=\"analytics\",\n",
    "        username=\"etl\"\n",
    "    )\n",
    ")\n",
    "print(\"âœ… Database source ETL\")\n",
    "\n",
    "print(\"\\nâœ… All source types work with FlexibleETLConfig!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- `Union` types allow multiple valid types for a field\n",
    "- Pydantic automatically discriminates based on fields\n",
    "- Enables flexible architecture while maintaining type safety\n",
    "- Each source type has its own validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Solution: Schema Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaMigration(BaseModel):\n",
    "    \"\"\"Database schema migration model.\"\"\"\n",
    "    version: str\n",
    "    description: str\n",
    "    up_sql: str\n",
    "    down_sql: str\n",
    "    applied_at: Optional[str] = None\n",
    "    checksum: Optional[str] = None\n",
    "    \n",
    "    @field_validator('version')\n",
    "    @classmethod\n",
    "    def validate_version(cls, v: str) -> str:\n",
    "        \"\"\"Ensure version follows vX.Y.Z format.\"\"\"\n",
    "        pattern = r'^v\\d+\\.\\d+\\.\\d+$'\n",
    "        if not re.match(pattern, v):\n",
    "            raise ValueError(\n",
    "                f\"Version must be in format vX.Y.Z (e.g., v1.0.0), got: {v}\"\n",
    "            )\n",
    "        return v\n",
    "    \n",
    "    @field_validator('up_sql', 'down_sql')\n",
    "    @classmethod\n",
    "    def validate_sql_not_empty(cls, v: str) -> str:\n",
    "        \"\"\"Ensure SQL is not empty or whitespace.\"\"\"\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"SQL cannot be empty or whitespace\")\n",
    "        return v\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def compute_checksum(self):\n",
    "        \"\"\"Auto-compute checksum from up_sql if not provided.\"\"\"\n",
    "        if self.checksum is None:\n",
    "            self.checksum = hashlib.md5(self.up_sql.encode()).hexdigest()\n",
    "        return self\n",
    "\n",
    "# Test migration\n",
    "migration = SchemaMigration(\n",
    "    version=\"v1.0.0\",\n",
    "    description=\"Add user_id column to orders table\",\n",
    "    up_sql=\"ALTER TABLE orders ADD COLUMN user_id INTEGER NOT NULL\",\n",
    "    down_sql=\"ALTER TABLE orders DROP COLUMN user_id\"\n",
    ")\n",
    "print(migration.model_dump_json(indent=2))\n",
    "print(f\"\\nâœ… Checksum auto-computed: {migration.checksum}\")\n",
    "\n",
    "# Test invalid version\n",
    "try:\n",
    "    bad_migration = SchemaMigration(\n",
    "        version=\"1.0.0\",  # Missing 'v' prefix\n",
    "        description=\"Test\",\n",
    "        up_sql=\"SELECT 1\",\n",
    "        down_sql=\"SELECT 0\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"\\nâœ… Invalid version caught: {e}\")\n",
    "\n",
    "# Test manual checksum\n",
    "migration_with_checksum = SchemaMigration(\n",
    "    version=\"v2.0.0\",\n",
    "    description=\"Add index\",\n",
    "    up_sql=\"CREATE INDEX idx_user_id ON orders(user_id)\",\n",
    "    down_sql=\"DROP INDEX idx_user_id\",\n",
    "    checksum=\"custom_checksum_123\"\n",
    ")\n",
    "print(f\"\\nâœ… Manual checksum preserved: {migration_with_checksum.checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- Regex validation for version format\n",
    "- Multiple fields validated by same validator\n",
    "- Model validator computes derived fields\n",
    "- Preserves manual values if provided\n",
    "- Real-world schema migration pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Learning Summary\n",
    "\n",
    "### Patterns You Mastered\n",
    "\n",
    "1. **Field Constraints**\n",
    "   - `ge`, `le`, `gt`, `lt` for numeric bounds\n",
    "   - `min_length`, `max_length` for collections\n",
    "   - `pattern` for regex validation\n",
    "\n",
    "2. **Validators**\n",
    "   - `@field_validator` for single field validation\n",
    "   - `@model_validator(mode=\"after\")` for cross-field validation\n",
    "   - Computing derived fields\n",
    "\n",
    "3. **Type Safety**\n",
    "   - Enums for constrained values\n",
    "   - `Literal` for exact value sets\n",
    "   - `Union` for multiple valid types\n",
    "   - `Optional` for nullable fields\n",
    "\n",
    "4. **Composition**\n",
    "   - Nested Pydantic models\n",
    "   - Lists of models\n",
    "   - Inheritance and base classes\n",
    "\n",
    "5. **Production Patterns**\n",
    "   - Environment-specific validation\n",
    "   - Security checks\n",
    "   - Unique constraints\n",
    "   - Auto-computed fields\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Apply these patterns to your own projects\n",
    "2. Run `mypy` on your code for static type checking\n",
    "3. Explore Pydantic's advanced features (discriminated unions, custom types)\n",
    "4. Study the Odibi codebase for more real-world examples\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "> **Type hints + Pydantic = Self-documenting, validated, maintainable code**\n",
    ">\n",
    "> Fail fast at config time, not hours into a data pipeline run!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"
  },\n",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
