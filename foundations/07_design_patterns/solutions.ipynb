{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Patterns Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Validator Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable, Any\n",
    "import re\n",
    "\n",
    "class ValidatorRegistry:\n",
    "    _validators: Dict[str, Callable] = {}\n",
    "    \n",
    "    @classmethod\n",
    "    def register(cls, name: str):\n",
    "        def decorator(func: Callable):\n",
    "            cls._validators[name] = func\n",
    "            return func\n",
    "        return decorator\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls, name: str) -> Callable:\n",
    "        if name not in cls._validators:\n",
    "            raise ValueError(f\"Validator '{name}' not registered\")\n",
    "        return cls._validators[name]\n",
    "    \n",
    "    @classmethod\n",
    "    def list_validators(cls) -> list[str]:\n",
    "        return list(cls._validators.keys())\n",
    "\n",
    "@ValidatorRegistry.register(\"not_null\")\n",
    "def not_null_validator(value: Any) -> bool:\n",
    "    return value is not None and value != \"\"\n",
    "\n",
    "@ValidatorRegistry.register(\"range_check\")\n",
    "def range_check_validator(value: Any, min_val: float = 0, max_val: float = 100) -> bool:\n",
    "    return min_val <= value <= max_val\n",
    "\n",
    "@ValidatorRegistry.register(\"email_format\")\n",
    "def email_format_validator(value: str) -> bool:\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, value))\n",
    "\n",
    "# Test\n",
    "validator = ValidatorRegistry.get(\"email_format\")\n",
    "print(f\"Valid email: {validator('test@example.com')}\")  # True\n",
    "print(f\"Invalid email: {validator('not-an-email')}\")  # False\n",
    "\n",
    "range_validator = ValidatorRegistry.get(\"range_check\")\n",
    "print(f\"In range: {range_validator(50, min_val=0, max_val=100)}\")  # True\n",
    "print(f\"Out of range: {range_validator(150, min_val=0, max_val=100)}\")  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Data Source Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "class DataSource(ABC):\n",
    "    @abstractmethod\n",
    "    def read(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class CSVSource(DataSource):\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        self.path = path\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def read(self) -> pd.DataFrame:\n",
    "        print(f\"Reading CSV from {self.path}\")\n",
    "        return pd.DataFrame({\"source\": [\"csv\"], \"path\": [self.path]})\n",
    "\n",
    "class ParquetSource(DataSource):\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        self.path = path\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def read(self) -> pd.DataFrame:\n",
    "        print(f\"Reading Parquet from {self.path}\")\n",
    "        return pd.DataFrame({\"source\": [\"parquet\"], \"path\": [self.path]})\n",
    "\n",
    "class DatabaseSource(DataSource):\n",
    "    def __init__(self, connection_string: str, query: str, **kwargs):\n",
    "        self.connection_string = connection_string\n",
    "        self.query = query\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def read(self) -> pd.DataFrame:\n",
    "        print(f\"Executing query on {self.connection_string}\")\n",
    "        return pd.DataFrame({\n",
    "            \"source\": [\"database\"],\n",
    "            \"query\": [self.query]\n",
    "        })\n",
    "\n",
    "class DataSourceFactory:\n",
    "    @staticmethod\n",
    "    def create(source_type: str, **config) -> DataSource:\n",
    "        if source_type == \"csv\":\n",
    "            return CSVSource(path=config[\"path\"], **config)\n",
    "        elif source_type == \"parquet\":\n",
    "            return ParquetSource(path=config[\"path\"], **config)\n",
    "        elif source_type == \"database\":\n",
    "            return DatabaseSource(\n",
    "                connection_string=config[\"connection_string\"],\n",
    "                query=config[\"query\"],\n",
    "                **config\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown source type: {source_type}\")\n",
    "\n",
    "# Test\n",
    "csv_source = DataSourceFactory.create(\"csv\", path=\"data.csv\")\n",
    "df = csv_source.read()\n",
    "print(df)\n",
    "\n",
    "db_source = DataSourceFactory.create(\n",
    "    \"database\",\n",
    "    connection_string=\"postgresql://localhost/db\",\n",
    "    query=\"SELECT * FROM users\"\n",
    ")\n",
    "df = db_source.read()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Serialization Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "class Serializer(ABC):\n",
    "    @abstractmethod\n",
    "    def serialize(self, data: Any) -> bytes:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def deserialize(self, data: bytes) -> Any:\n",
    "        pass\n",
    "\n",
    "class JSONSerializer(Serializer):\n",
    "    def serialize(self, data: Any) -> bytes:\n",
    "        return json.dumps(data).encode('utf-8')\n",
    "    \n",
    "    def deserialize(self, data: bytes) -> Any:\n",
    "        return json.loads(data.decode('utf-8'))\n",
    "\n",
    "class PickleSerializer(Serializer):\n",
    "    def serialize(self, data: Any) -> bytes:\n",
    "        return pickle.dumps(data)\n",
    "    \n",
    "    def deserialize(self, data: bytes) -> Any:\n",
    "        return pickle.loads(data)\n",
    "\n",
    "class ParquetSerializer(Serializer):\n",
    "    def serialize(self, data: pd.DataFrame) -> bytes:\n",
    "        return data.to_parquet()\n",
    "    \n",
    "    def deserialize(self, data: bytes) -> pd.DataFrame:\n",
    "        import io\n",
    "        return pd.read_parquet(io.BytesIO(data))\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self, serializer: Serializer):\n",
    "        self.serializer = serializer\n",
    "        self._storage: dict[str, bytes] = {}\n",
    "    \n",
    "    def save(self, key: str, data: Any) -> None:\n",
    "        self._storage[key] = self.serializer.serialize(data)\n",
    "    \n",
    "    def load(self, key: str) -> Any:\n",
    "        if key not in self._storage:\n",
    "            raise KeyError(f\"Key '{key}' not found\")\n",
    "        return self.serializer.deserialize(self._storage[key])\n",
    "\n",
    "# Test with different serializers\n",
    "data = {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "# JSON\n",
    "json_store = DataStore(JSONSerializer())\n",
    "json_store.save(\"user\", data)\n",
    "loaded = json_store.load(\"user\")\n",
    "print(f\"JSON: {loaded}\")\n",
    "\n",
    "# Pickle\n",
    "pickle_store = DataStore(PickleSerializer())\n",
    "pickle_store.save(\"user\", data)\n",
    "loaded = pickle_store.load(\"user\")\n",
    "print(f\"Pickle: {loaded}\")\n",
    "\n",
    "# Parquet\n",
    "df = pd.DataFrame({\"x\": [1, 2, 3]})\n",
    "parquet_store = DataStore(ParquetSerializer())\n",
    "parquet_store.save(\"data\", df)\n",
    "loaded_df = parquet_store.load(\"data\")\n",
    "print(f\"Parquet:\\n{loaded_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Query Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    def __init__(self):\n",
    "        self._select_cols: list[str] = []\n",
    "        self._from: str | None = None\n",
    "        self._where: list[str] = []\n",
    "        self._group_by: list[str] = []\n",
    "        self._order_by: list[str] = []\n",
    "        self._limit: int | None = None\n",
    "    \n",
    "    def select(self, *columns: str) -> 'QueryBuilder':\n",
    "        self._select_cols.extend(columns)\n",
    "        return self\n",
    "    \n",
    "    def from_table(self, table: str) -> 'QueryBuilder':\n",
    "        self._from = table\n",
    "        return self\n",
    "    \n",
    "    def where(self, condition: str) -> 'QueryBuilder':\n",
    "        self._where.append(condition)\n",
    "        return self\n",
    "    \n",
    "    def group_by(self, *columns: str) -> 'QueryBuilder':\n",
    "        self._group_by.extend(columns)\n",
    "        return self\n",
    "    \n",
    "    def order_by(self, *columns: str) -> 'QueryBuilder':\n",
    "        self._order_by.extend(columns)\n",
    "        return self\n",
    "    \n",
    "    def limit(self, count: int) -> 'QueryBuilder':\n",
    "        self._limit = count\n",
    "        return self\n",
    "    \n",
    "    def build(self) -> str:\n",
    "        if not self._select_cols:\n",
    "            raise ValueError(\"SELECT columns required\")\n",
    "        if not self._from:\n",
    "            raise ValueError(\"FROM table required\")\n",
    "        \n",
    "        parts = []\n",
    "        parts.append(f\"SELECT {', '.join(self._select_cols)}\")\n",
    "        parts.append(f\"FROM {self._from}\")\n",
    "        \n",
    "        if self._where:\n",
    "            parts.append(f\"WHERE {' AND '.join(self._where)}\")\n",
    "        \n",
    "        if self._group_by:\n",
    "            parts.append(f\"GROUP BY {', '.join(self._group_by)}\")\n",
    "        \n",
    "        if self._order_by:\n",
    "            parts.append(f\"ORDER BY {', '.join(self._order_by)}\")\n",
    "        \n",
    "        if self._limit:\n",
    "            parts.append(f\"LIMIT {self._limit}\")\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "\n",
    "# Test\n",
    "query = (\n",
    "    QueryBuilder()\n",
    "    .select(\"name\", \"COUNT(*) as count\")\n",
    "    .from_table(\"users\")\n",
    "    .where(\"age > 25\")\n",
    "    .where(\"active = true\")\n",
    "    .group_by(\"name\")\n",
    "    .order_by(\"count DESC\")\n",
    "    .limit(10)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Transform Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "class Logger:\n",
    "    def log(self, message: str) -> None:\n",
    "        print(f\"[LOG] {message}\")\n",
    "\n",
    "# Strategy: Transform interface\n",
    "class Transform(ABC):\n",
    "    @abstractmethod\n",
    "    def apply(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "# Registry: Transform registry\n",
    "class TransformRegistry:\n",
    "    _transforms: Dict[str, type[Transform]] = {}\n",
    "    \n",
    "    @classmethod\n",
    "    def register(cls, name: str):\n",
    "        def decorator(transform_class: type[Transform]):\n",
    "            cls._transforms[name] = transform_class\n",
    "            return transform_class\n",
    "        return decorator\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls, name: str) -> type[Transform]:\n",
    "        return cls._transforms[name]\n",
    "\n",
    "# Factory: Transform factory\n",
    "class TransformFactory:\n",
    "    @staticmethod\n",
    "    def create(name: str, **config) -> Transform:\n",
    "        transform_class = TransformRegistry.get(name)\n",
    "        return transform_class(**config)\n",
    "\n",
    "# Builder: Pipeline builder\n",
    "class PipelineBuilder:\n",
    "    def __init__(self):\n",
    "        self._steps: list[tuple[str, dict]] = []\n",
    "        self._logger: Logger | None = None\n",
    "    \n",
    "    def add_transform(self, name: str, **config) -> 'PipelineBuilder':\n",
    "        self._steps.append((name, config))\n",
    "        return self\n",
    "    \n",
    "    def with_logger(self, logger: Logger) -> 'PipelineBuilder':\n",
    "        self._logger = logger\n",
    "        return self\n",
    "    \n",
    "    def build(self) -> 'Pipeline':\n",
    "        transforms = [\n",
    "            TransformFactory.create(name, **config)\n",
    "            for name, config in self._steps\n",
    "        ]\n",
    "        return Pipeline(transforms, logger=self._logger or Logger())\n",
    "\n",
    "# Dependency Injection: Pipeline with logger\n",
    "class Pipeline:\n",
    "    def __init__(self, transforms: list[Transform], logger: Logger):\n",
    "        self.transforms = transforms\n",
    "        self.logger = logger\n",
    "    \n",
    "    def execute(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df\n",
    "        for i, transform in enumerate(self.transforms, 1):\n",
    "            self.logger.log(f\"Step {i}: {transform.__class__.__name__}\")\n",
    "            result = transform.apply(result)\n",
    "        self.logger.log(\"Pipeline complete\")\n",
    "        return result\n",
    "\n",
    "# Define transforms\n",
    "@TransformRegistry.register(\"filter\")\n",
    "class FilterTransform(Transform):\n",
    "    def __init__(self, condition: str):\n",
    "        self.condition = condition\n",
    "    \n",
    "    def apply(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.query(self.condition)\n",
    "\n",
    "@TransformRegistry.register(\"uppercase\")\n",
    "class UppercaseTransform(Transform):\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "    \n",
    "    def apply(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        result[self.column] = result[self.column].str.upper()\n",
    "        return result\n",
    "\n",
    "@TransformRegistry.register(\"add_column\")\n",
    "class AddColumnTransform(Transform):\n",
    "    def __init__(self, column: str, expression: str):\n",
    "        self.column = column\n",
    "        self.expression = expression\n",
    "    \n",
    "    def apply(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        result[self.column] = result.eval(self.expression)\n",
    "        return result\n",
    "\n",
    "# Build and test pipeline\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"alice\", \"bob\", \"charlie\"],\n",
    "    \"last_name\": [\"smith\", \"jones\", \"brown\"],\n",
    "    \"age\": [30, 20, 35]\n",
    "})\n",
    "\n",
    "pipeline = (\n",
    "    PipelineBuilder()\n",
    "    .with_logger(Logger())\n",
    "    .add_transform(\"filter\", condition=\"age > 25\")\n",
    "    .add_transform(\"uppercase\", column=\"name\")\n",
    "    .add_transform(\"add_column\", column=\"full_name\", expression=\"name + ' ' + last_name\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "result = pipeline.execute(df)\n",
    "print(\"\\nResult:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Pattern Recognition in Odibi\n",
    "\n",
    "**1. Registry pattern examples:**\n",
    "- Transform registry for registering data transformations\n",
    "- Function registry for custom UDFs\n",
    "- Connector/adapter registry for different data sources\n",
    "\n",
    "**2. Factory pattern in `create_context()`:**\n",
    "- Takes configuration and creates appropriate execution context (Spark, Pandas, etc.)\n",
    "- Hides complex initialization of different engine types\n",
    "- Returns common interface regardless of underlying engine\n",
    "\n",
    "**3. Strategy pattern classes:**\n",
    "- Different execution engines (SparkEngine, PandasEngine, PolarsEngine)\n",
    "- Different serialization formats (JSON, Parquet, CSV writers)\n",
    "- Different validation strategies\n",
    "\n",
    "**4. Dependency Injection examples:**\n",
    "- Context object passed to transforms\n",
    "- Config passed to pipeline components\n",
    "- Connection objects passed to data readers/writers\n",
    "\n",
    "**5. Singletons:**\n",
    "- Check if any global state managers exist\n",
    "- If found, consider refactoring to dependency injection\n",
    "- Singletons make testing harder and create hidden dependencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
