{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Patterns for Data Engineering Frameworks\n",
    "\n",
    "## ðŸŽ¯ Problem: Code Reusability and Extensibility\n",
    "\n",
    "### The Framework Challenge\n",
    "\n",
    "When building data engineering frameworks like Odibi, you need:\n",
    "- **Extensibility**: Add new transforms, connectors, engines without modifying core code\n",
    "- **Consistency**: Same interface for different implementations\n",
    "- **Discoverability**: Find and register components dynamically\n",
    "- **Testability**: Mock and test components in isolation\n",
    "\n",
    "Design patterns solve these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¦‰ First Principles\n",
    "\n",
    "1. **Design for Change**: Code will evolve; make change easy\n",
    "2. **Favor Composition**: Combine small pieces over inheritance hierarchies\n",
    "3. **Program to Interfaces**: Depend on contracts, not implementations\n",
    "4. **Open/Closed Principle**: Open for extension, closed for modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Registry Pattern\n",
    "\n",
    "### âš¡ Example: Transform Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable, Any\n",
    "import pandas as pd\n",
    "\n",
    "class TransformRegistry:\n",
    "    \"\"\"Central registry for data transforms.\"\"\"\n",
    "    \n",
    "    _transforms: Dict[str, Callable] = {}\n",
    "    \n",
    "    @classmethod\n",
    "    def register(cls, name: str):\n",
    "        \"\"\"Decorator to register a transform.\"\"\"\n",
    "        def decorator(func: Callable):\n",
    "            cls._transforms[name] = func\n",
    "            return func\n",
    "        return decorator\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls, name: str) -> Callable:\n",
    "        \"\"\"Get a registered transform.\"\"\"\n",
    "        if name not in cls._transforms:\n",
    "            raise ValueError(f\"Transform '{name}' not registered\")\n",
    "        return cls._transforms[name]\n",
    "    \n",
    "    @classmethod\n",
    "    def list_transforms(cls) -> list[str]:\n",
    "        \"\"\"List all registered transforms.\"\"\"\n",
    "        return list(cls._transforms.keys())\n",
    "\n",
    "# Register transforms using decorator\n",
    "@TransformRegistry.register(\"uppercase\")\n",
    "def uppercase_transform(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[column] = df[column].str.upper()\n",
    "    return df\n",
    "\n",
    "@TransformRegistry.register(\"add_prefix\")\n",
    "def add_prefix_transform(df: pd.DataFrame, column: str, prefix: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[column] = prefix + df[column]\n",
    "    return df\n",
    "\n",
    "# Use registered transforms\n",
    "df = pd.DataFrame({\"name\": [\"alice\", \"bob\"]})\n",
    "\n",
    "transform = TransformRegistry.get(\"uppercase\")\n",
    "result = transform(df, \"name\")\n",
    "print(result)\n",
    "\n",
    "print(f\"\\nAvailable transforms: {TransformRegistry.list_transforms()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Odibi Analysis: Registry Pattern\n",
    "\n",
    "**Where used:**\n",
    "- Transform registry for data transformations\n",
    "- Function registry for custom operations\n",
    "- Connector registry for different data sources\n",
    "\n",
    "**Benefits:**\n",
    "- Add new transforms without modifying core\n",
    "- Discover available transforms at runtime\n",
    "- Decouple registration from usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Factory Pattern\n",
    "\n",
    "### âš¡ Example: Connection Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "class Connection(ABC):\n",
    "    \"\"\"Abstract connection interface.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def connect(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, query: str) -> Any:\n",
    "        pass\n",
    "\n",
    "class PostgresConnection(Connection):\n",
    "    def __init__(self, host: str, database: str):\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "    \n",
    "    def connect(self) -> None:\n",
    "        print(f\"Connecting to PostgreSQL: {self.host}/{self.database}\")\n",
    "    \n",
    "    def execute(self, query: str) -> Any:\n",
    "        return f\"PostgreSQL executing: {query}\"\n",
    "\n",
    "class SnowflakeConnection(Connection):\n",
    "    def __init__(self, account: str, warehouse: str):\n",
    "        self.account = account\n",
    "        self.warehouse = warehouse\n",
    "    \n",
    "    def connect(self) -> None:\n",
    "        print(f\"Connecting to Snowflake: {self.account}/{self.warehouse}\")\n",
    "    \n",
    "    def execute(self, query: str) -> Any:\n",
    "        return f\"Snowflake executing: {query}\"\n",
    "\n",
    "class ConnectionFactory:\n",
    "    \"\"\"Factory for creating connections.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_connection(conn_type: str, **config) -> Connection:\n",
    "        \"\"\"Create connection based on type.\"\"\"\n",
    "        if conn_type == \"postgres\":\n",
    "            return PostgresConnection(\n",
    "                host=config.get(\"host\", \"localhost\"),\n",
    "                database=config[\"database\"]\n",
    "            )\n",
    "        elif conn_type == \"snowflake\":\n",
    "            return SnowflakeConnection(\n",
    "                account=config[\"account\"],\n",
    "                warehouse=config[\"warehouse\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown connection type: {conn_type}\")\n",
    "\n",
    "# Use factory\n",
    "conn = ConnectionFactory.create_connection(\n",
    "    \"postgres\",\n",
    "    host=\"db.example.com\",\n",
    "    database=\"analytics\"\n",
    ")\n",
    "conn.connect()\n",
    "print(conn.execute(\"SELECT * FROM users\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Odibi Analysis: Factory Pattern\n",
    "\n",
    "**Where used:**\n",
    "- `create_context()` - Creates execution context based on config\n",
    "- Connection factory - Creates database connections\n",
    "- Engine factory - Creates compute engines (Spark, Pandas, Polars)\n",
    "\n",
    "**Benefits:**\n",
    "- Centralize object creation logic\n",
    "- Hide complex initialization\n",
    "- Return interface, not concrete type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 3: Strategy Pattern\n",
    "\n",
    "### âš¡ Example: Execution Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class ExecutionEngine(ABC):\n",
    "    \"\"\"Strategy for executing data operations.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def filter(self, data: Any, condition: str) -> Any:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def aggregate(self, data: Any, columns: list[str]) -> Any:\n",
    "        pass\n",
    "\n",
    "class PandasEngine(ExecutionEngine):\n",
    "    def filter(self, data: pd.DataFrame, condition: str) -> pd.DataFrame:\n",
    "        return data.query(condition)\n",
    "    \n",
    "    def aggregate(self, data: pd.DataFrame, columns: list[str]) -> pd.DataFrame:\n",
    "        return data.groupby(columns).size().reset_index(name='count')\n",
    "\n",
    "class PolarsEngine(ExecutionEngine):\n",
    "    def filter(self, data: Any, condition: str) -> Any:\n",
    "        # Simplified - real implementation would parse condition\n",
    "        return f\"Polars filter: {condition}\"\n",
    "    \n",
    "    def aggregate(self, data: Any, columns: list[str]) -> Any:\n",
    "        return f\"Polars groupby: {columns}\"\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"Context that uses an execution engine.\"\"\"\n",
    "    \n",
    "    def __init__(self, engine: ExecutionEngine):\n",
    "        self.engine = engine\n",
    "    \n",
    "    def process(self, data: Any) -> Any:\n",
    "        \"\"\"Process using the configured engine.\"\"\"\n",
    "        filtered = self.engine.filter(data, \"age > 25\")\n",
    "        aggregated = self.engine.aggregate(filtered, [\"city\"])\n",
    "        return aggregated\n",
    "\n",
    "# Use with different strategies\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [30, 20, 35],\n",
    "    \"city\": [\"NYC\", \"LA\", \"NYC\"]\n",
    "})\n",
    "\n",
    "# Same pipeline, different engine\n",
    "pandas_pipeline = DataPipeline(PandasEngine())\n",
    "result = pandas_pipeline.process(df)\n",
    "print(\"Pandas result:\")\n",
    "print(result)\n",
    "\n",
    "polars_pipeline = DataPipeline(PolarsEngine())\n",
    "print(f\"\\nPolars result: {polars_pipeline.process(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Odibi Analysis: Strategy Pattern\n",
    "\n",
    "**Where used:**\n",
    "- Different execution engines (Spark, Pandas, Polars) with same interface\n",
    "- Different serialization strategies (JSON, Parquet, CSV)\n",
    "- Different validation strategies\n",
    "\n",
    "**Benefits:**\n",
    "- Swap implementations without changing client code\n",
    "- Test with mock strategies\n",
    "- Configure behavior at runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 4: Builder Pattern\n",
    "\n",
    "### âš¡ Example: Config Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Pipeline configuration.\"\"\"\n",
    "    name: str\n",
    "    source: Optional[str] = None\n",
    "    target: Optional[str] = None\n",
    "    transforms: list[str] = field(default_factory=list)\n",
    "    validation_enabled: bool = True\n",
    "    retry_count: int = 3\n",
    "\n",
    "class PipelineConfigBuilder:\n",
    "    \"\"\"Builder for pipeline configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self._config = PipelineConfig(name=name)\n",
    "    \n",
    "    def with_source(self, source: str) -> 'PipelineConfigBuilder':\n",
    "        self._config.source = source\n",
    "        return self\n",
    "    \n",
    "    def with_target(self, target: str) -> 'PipelineConfigBuilder':\n",
    "        self._config.target = target\n",
    "        return self\n",
    "    \n",
    "    def add_transform(self, transform: str) -> 'PipelineConfigBuilder':\n",
    "        self._config.transforms.append(transform)\n",
    "        return self\n",
    "    \n",
    "    def with_validation(self, enabled: bool) -> 'PipelineConfigBuilder':\n",
    "        self._config.validation_enabled = enabled\n",
    "        return self\n",
    "    \n",
    "    def with_retries(self, count: int) -> 'PipelineConfigBuilder':\n",
    "        self._config.retry_count = count\n",
    "        return self\n",
    "    \n",
    "    def build(self) -> PipelineConfig:\n",
    "        \"\"\"Build the final config.\"\"\"\n",
    "        if not self._config.source:\n",
    "            raise ValueError(\"Source is required\")\n",
    "        if not self._config.target:\n",
    "            raise ValueError(\"Target is required\")\n",
    "        return self._config\n",
    "\n",
    "# Build configuration fluently\n",
    "config = (\n",
    "    PipelineConfigBuilder(\"user_pipeline\")\n",
    "    .with_source(\"postgres://users\")\n",
    "    .with_target(\"s3://analytics/users\")\n",
    "    .add_transform(\"clean_names\")\n",
    "    .add_transform(\"filter_active\")\n",
    "    .with_validation(True)\n",
    "    .with_retries(5)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Odibi Analysis: Builder Pattern\n",
    "\n",
    "**Where used:**\n",
    "- Config building with fluent interface\n",
    "- Query builders for different engines\n",
    "- Pipeline construction\n",
    "\n",
    "**Benefits:**\n",
    "- Readable, fluent API\n",
    "- Validate during construction\n",
    "- Handle complex object creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 5: Singleton Pattern\n",
    "\n",
    "### âš¡ Example: Config Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "    \"\"\"Singleton config manager.\"\"\"\n",
    "    \n",
    "    _instance: Optional['ConfigManager'] = None\n",
    "    _config: dict = {}\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def set(self, key: str, value: Any) -> None:\n",
    "        self._config[key] = value\n",
    "    \n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        return self._config.get(key, default)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear config (useful for testing).\"\"\"\n",
    "        self._config.clear()\n",
    "\n",
    "# Same instance everywhere\n",
    "config1 = ConfigManager()\n",
    "config1.set(\"database\", \"analytics\")\n",
    "\n",
    "config2 = ConfigManager()\n",
    "print(f\"Same instance: {config1 is config2}\")  # True\n",
    "print(f\"Database: {config2.get('database')}\")  # analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš ï¸ Singleton Anti-Pattern\n",
    "\n",
    "**Problems with Singleton:**\n",
    "- Makes testing harder (global state)\n",
    "- Hidden dependencies\n",
    "- Tight coupling\n",
    "\n",
    "**Better Alternative: Dependency Injection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 6: Dependency Injection\n",
    "\n",
    "### âš¡ Example: Explicit Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def log(self, message: str) -> None:\n",
    "        print(f\"LOG: {message}\")\n",
    "\n",
    "class MetricsCollector:\n",
    "    def record(self, metric: str, value: float) -> None:\n",
    "        print(f\"METRIC: {metric} = {value}\")\n",
    "\n",
    "# BAD: Hidden dependencies (Singleton)\n",
    "class PipelineBad:\n",
    "    def run(self):\n",
    "        logger = ConfigManager().get(\"logger\")  # Hidden!\n",
    "        logger.log(\"Starting\")\n",
    "\n",
    "# GOOD: Explicit dependencies (Dependency Injection)\n",
    "class Pipeline:\n",
    "    def __init__(self, logger: Logger, metrics: MetricsCollector):\n",
    "        self.logger = logger\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.logger.log(\"Starting pipeline\")\n",
    "        self.metrics.record(\"rows\", len(data))\n",
    "        \n",
    "        # Process data\n",
    "        result = data.copy()\n",
    "        \n",
    "        self.logger.log(\"Pipeline complete\")\n",
    "        return result\n",
    "\n",
    "# Easy to test - inject mocks\n",
    "logger = Logger()\n",
    "metrics = MetricsCollector()\n",
    "pipeline = Pipeline(logger=logger, metrics=metrics)\n",
    "\n",
    "df = pd.DataFrame({\"x\": [1, 2, 3]})\n",
    "result = pipeline.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Odibi Analysis: Dependency Injection\n",
    "\n",
    "**Where used:**\n",
    "- Execution context passed explicitly\n",
    "- Config injected into components\n",
    "- Connections passed to transforms\n",
    "\n",
    "**Benefits:**\n",
    "- Clear dependencies\n",
    "- Easy to test\n",
    "- Flexible composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Build: Mini Plugin System\n",
    "\n",
    "Combine patterns to build an extensible plugin system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Callable, Any\n",
    "\n",
    "# Strategy Pattern: Plugin interface\n",
    "class Plugin(ABC):\n",
    "    @abstractmethod\n",
    "    def execute(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "# Registry Pattern: Plugin registry\n",
    "class PluginRegistry:\n",
    "    _plugins: Dict[str, type[Plugin]] = {}\n",
    "    \n",
    "    @classmethod\n",
    "    def register(cls, name: str):\n",
    "        def decorator(plugin_class: type[Plugin]):\n",
    "            cls._plugins[name] = plugin_class\n",
    "            return plugin_class\n",
    "        return decorator\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls, name: str) -> type[Plugin]:\n",
    "        return cls._plugins[name]\n",
    "\n",
    "# Factory Pattern: Plugin factory\n",
    "class PluginFactory:\n",
    "    @staticmethod\n",
    "    def create(name: str, **config) -> Plugin:\n",
    "        plugin_class = PluginRegistry.get(name)\n",
    "        return plugin_class(**config)\n",
    "\n",
    "# Builder Pattern: Pipeline builder\n",
    "class PipelineBuilder:\n",
    "    def __init__(self):\n",
    "        self._steps: list[tuple[str, dict]] = []\n",
    "    \n",
    "    def add_step(self, plugin_name: str, **config) -> 'PipelineBuilder':\n",
    "        self._steps.append((plugin_name, config))\n",
    "        return self\n",
    "    \n",
    "    def build(self) -> 'PluginPipeline':\n",
    "        plugins = [\n",
    "            PluginFactory.create(name, **config)\n",
    "            for name, config in self._steps\n",
    "        ]\n",
    "        return PluginPipeline(plugins)\n",
    "\n",
    "# Dependency Injection: Pipeline with explicit dependencies\n",
    "class PluginPipeline:\n",
    "    def __init__(self, plugins: list[Plugin]):\n",
    "        self.plugins = plugins\n",
    "    \n",
    "    def execute(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = data\n",
    "        for plugin in self.plugins:\n",
    "            result = plugin.execute(result)\n",
    "        return result\n",
    "\n",
    "# Define plugins\n",
    "@PluginRegistry.register(\"uppercase\")\n",
    "class UppercasePlugin(Plugin):\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "    \n",
    "    def execute(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "        result = data.copy()\n",
    "        result[self.column] = result[self.column].str.upper()\n",
    "        return result\n",
    "\n",
    "@PluginRegistry.register(\"filter\")\n",
    "class FilterPlugin(Plugin):\n",
    "    def __init__(self, condition: str):\n",
    "        self.condition = condition\n",
    "    \n",
    "    def execute(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "        return data.query(self.condition)\n",
    "\n",
    "# Use the plugin system\n",
    "pipeline = (\n",
    "    PipelineBuilder()\n",
    "    .add_step(\"uppercase\", column=\"name\")\n",
    "    .add_step(\"filter\", condition=\"age > 25\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"alice\", \"bob\", \"charlie\"],\n",
    "    \"age\": [30, 20, 35]\n",
    "})\n",
    "\n",
    "result = pipeline.execute(df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Test: Pattern Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plugin system\n",
    "def test_plugin_pipeline():\n",
    "    # Arrange\n",
    "    df = pd.DataFrame({\n",
    "        \"name\": [\"alice\", \"bob\"],\n",
    "        \"age\": [30, 20]\n",
    "    })\n",
    "    \n",
    "    pipeline = (\n",
    "        PipelineBuilder()\n",
    "        .add_step(\"uppercase\", column=\"name\")\n",
    "        .build()\n",
    "    )\n",
    "    \n",
    "    # Act\n",
    "    result = pipeline.execute(df)\n",
    "    \n",
    "    # Assert\n",
    "    assert result[\"name\"].tolist() == [\"ALICE\", \"BOB\"]\n",
    "    print(\"âœ“ Plugin pipeline test passed\")\n",
    "\n",
    "test_plugin_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "\n",
    "| Pattern | Purpose | Odibi Usage |\n",
    "|---------|---------|-------------|\n",
    "| Registry | Discover & register components | Transform/function registry |\n",
    "| Factory | Create objects by type | `create_context()`, connections |\n",
    "| Strategy | Swap implementations | Different engines (Spark/Pandas) |\n",
    "| Builder | Fluent construction | Config building, query building |\n",
    "| Singleton | Single instance (avoid!) | - |\n",
    "| Dependency Injection | Explicit dependencies | Context, config, connections |\n",
    "\n",
    "**Key Takeaway**: Use patterns to solve real problems, not for their own sake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
