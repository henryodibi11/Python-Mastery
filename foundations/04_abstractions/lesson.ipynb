{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Abstractions: Building Flexible Systems\n",
    "\n",
    "**Learning Goal**: Master abstraction patterns that enable swappable implementations and extensible architecture.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Problem: Why Abstractions?\n",
    "\n",
    "### The Multi-Engine Challenge\n",
    "\n",
    "Imagine building a data processing library that needs to support:\n",
    "- **Pandas** for small/medium datasets on a single machine\n",
    "- **Spark** for large-scale distributed processing\n",
    "- **Polars** for high-performance single-machine workloads\n",
    "\n",
    "**Without abstractions:**\n",
    "```python\n",
    "def read_data(engine_type: str, path: str):\n",
    "    if engine_type == 'pandas':\n",
    "        return pd.read_csv(path)\n",
    "    elif engine_type == 'spark':\n",
    "        return spark.read.csv(path)\n",
    "    elif engine_type == 'polars':\n",
    "        return pl.read_csv(path)\n",
    "    # What happens when we add DuckDB? Dask? Modin?\n",
    "```\n",
    "\n",
    "Problems:\n",
    "- âŒ If/else chains everywhere\n",
    "- âŒ Tight coupling to specific implementations\n",
    "- âŒ Can't add new engines without modifying existing code\n",
    "- âŒ No compile-time guarantee that all engines implement all operations\n",
    "\n",
    "**With abstractions:**\n",
    "```python\n",
    "class Engine(ABC):\n",
    "    @abstractmethod\n",
    "    def read(self, path: str) -> DataFrame: ...\n",
    "\n",
    "def read_data(engine: Engine, path: str):\n",
    "    return engine.read(path)  # Works with ANY engine!\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- âœ… Single interface, multiple implementations\n",
    "- âœ… Add new engines without changing existing code\n",
    "- âœ… Type-safe contracts enforced at definition time\n",
    "- âœ… Easy to test with mock implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¦‰ First Principles: Depend on Abstractions, Not Concretions\n",
    "\n",
    "### The Dependency Inversion Principle\n",
    "\n",
    "**High-level modules should not depend on low-level modules. Both should depend on abstractions.**\n",
    "\n",
    "```python\n",
    "# âŒ BAD: Depends on concretion\n",
    "class DataPipeline:\n",
    "    def __init__(self):\n",
    "        self.engine = PandasEngine()  # Locked to Pandas!\n",
    "\n",
    "# âœ… GOOD: Depends on abstraction\n",
    "class DataPipeline:\n",
    "    def __init__(self, engine: Engine):  # Works with any Engine!\n",
    "        self.engine = engine\n",
    "```\n",
    "\n",
    "### Python's Abstraction Tools\n",
    "\n",
    "1. **ABC (Abstract Base Classes)**: Explicit contracts with inheritance\n",
    "2. **Protocol**: Structural subtyping (duck typing with type safety)\n",
    "3. **Duck Typing**: \"If it walks like a duck...\"\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Pattern | Use When | Example |\n",
    "|---------|----------|----------|\n",
    "| **ABC** | You control implementations & want enforced contracts | Odibi's Engine (internal library) |\n",
    "| **Protocol** | Working with external types or want structural typing | File-like objects, Iterables |\n",
    "| **Duck Typing** | Maximum flexibility, runtime checking acceptable | Quick scripts, exploratory code |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Examples: ABC vs Protocol\n",
    "\n",
    "### Example 1: ABC for Explicit Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "class DataReader(ABC):\n",
    "    \"\"\"Abstract interface for data readers.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def read(self, source: str) -> Any:\n",
    "        \"\"\"Read data from source.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_schema(self) -> dict:\n",
    "        \"\"\"Return schema information.\"\"\"\n",
    "        pass\n",
    "\n",
    "# âŒ This will fail instantly - can't instantiate abstract class\n",
    "# reader = DataReader()  # TypeError!\n",
    "\n",
    "class CSVReader(DataReader):\n",
    "    def read(self, source: str) -> list[dict]:\n",
    "        # Actual implementation\n",
    "        import csv\n",
    "        with open(source) as f:\n",
    "            return list(csv.DictReader(f))\n",
    "    \n",
    "    def get_schema(self) -> dict:\n",
    "        return {\"type\": \"csv\", \"columns\": []}\n",
    "\n",
    "# âœ… This works - all abstract methods implemented\n",
    "reader = CSVReader()\n",
    "print(f\"Created: {type(reader).__name__}\")\n",
    "print(f\"Is DataReader? {isinstance(reader, DataReader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Protocol for Structural Subtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, runtime_checkable\n",
    "\n",
    "@runtime_checkable\n",
    "class Readable(Protocol):\n",
    "    \"\"\"Protocol: anything with a read() method.\"\"\"\n",
    "    def read(self, source: str) -> Any: ...\n",
    "\n",
    "# No inheritance needed!\n",
    "class JSONReader:\n",
    "    def read(self, source: str) -> dict:\n",
    "        import json\n",
    "        with open(source) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "class ParquetReader:\n",
    "    def read(self, source: str) -> Any:\n",
    "        # Pretend implementation\n",
    "        return {\"data\": \"from parquet\"}\n",
    "\n",
    "# âœ… Both satisfy the protocol automatically\n",
    "def process_data(reader: Readable, source: str):\n",
    "    return reader.read(source)\n",
    "\n",
    "json_reader = JSONReader()\n",
    "parquet_reader = ParquetReader()\n",
    "\n",
    "print(f\"JSONReader is Readable? {isinstance(json_reader, Readable)}\")\n",
    "print(f\"ParquetReader is Readable? {isinstance(parquet_reader, Readable)}\")\n",
    "\n",
    "# Type checker accepts both even though they don't inherit from Readable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Composition Over Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# âŒ Inheritance approach - rigid hierarchy\n",
    "class DatabaseReader(DataReader):\n",
    "    def connect(self): ...\n",
    "    def read(self, query: str): ...\n",
    "    def get_schema(self): ...\n",
    "\n",
    "class CachedDatabaseReader(DatabaseReader):\n",
    "    def read(self, query: str):  # Must override\n",
    "        # Check cache, then call super().read()\n",
    "        pass\n",
    "\n",
    "# âœ… Composition approach - flexible and testable\n",
    "@dataclass\n",
    "class CachedReader:\n",
    "    \"\"\"Wraps any reader with caching.\"\"\"\n",
    "    reader: DataReader\n",
    "    cache: dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.cache is None:\n",
    "            self.cache = {}\n",
    "    \n",
    "    def read(self, source: str) -> Any:\n",
    "        if source not in self.cache:\n",
    "            self.cache[source] = self.reader.read(source)\n",
    "        return self.cache[source]\n",
    "    \n",
    "    def get_schema(self) -> dict:\n",
    "        return self.reader.get_schema()\n",
    "\n",
    "# Works with ANY DataReader implementation\n",
    "csv_reader = CSVReader()\n",
    "cached_csv = CachedReader(csv_reader)\n",
    "\n",
    "print(\"Composition allows mixing behaviors freely!\")\n",
    "print(f\"Base reader: {type(cached_csv.reader).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Odibi Analysis: Production Abstraction Patterns\n",
    "\n",
    "Let's analyze how Odibi uses ABC to enable multi-engine support.\n",
    "\n",
    "### 1. Engine Abstraction\n",
    "\n",
    "**File**: `odibi/engine/base.py`\n",
    "\n",
    "**Pattern**: ABC with 9 abstract methods defining complete engine contract\n",
    "\n",
    "```python\n",
    "class Engine(ABC):\n",
    "    @abstractmethod\n",
    "    def read(self, connection, format, table=None, path=None, options=None) -> Any: ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def write(self, df, connection, format, table=None, path=None, mode=\"overwrite\", options=None) -> None: ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute_sql(self, sql: str, context: Context) -> Any: ...\n",
    "    \n",
    "    # ... 6 more abstract methods\n",
    "```\n",
    "\n",
    "**Why ABC instead of Protocol?**\n",
    "- âœ… Odibi controls both the interface AND implementations (PandasEngine, SparkEngine)\n",
    "- âœ… Wants compile-time errors if implementation forgets a method\n",
    "- âœ… Clear documentation of the complete contract\n",
    "- âœ… Prevents accidental instantiation of incomplete engines\n",
    "\n",
    "**Key Design Decisions**:\n",
    "1. **Return `Any` instead of concrete types**: Pandas returns `pd.DataFrame`, Spark returns `pyspark.sql.DataFrame`\n",
    "2. **Consistent method signatures**: All engines implement same interface\n",
    "3. **Context passing**: Enables cross-engine state management\n",
    "\n",
    "### 2. Connection Abstraction\n",
    "\n",
    "**File**: `odibi/connections/base.py`\n",
    "\n",
    "```python\n",
    "class BaseConnection(ABC):\n",
    "    @abstractmethod\n",
    "    def get_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Resolve relative path to full path.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate connection config.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "**Why such a minimal interface?**\n",
    "- Different connection types have vastly different needs:\n",
    "  - `FileConnection`: Just needs a base directory\n",
    "  - `DatabaseConnection`: Needs host, port, credentials, connection pool\n",
    "  - `CloudConnection`: Needs auth tokens, regions, buckets\n",
    "- Only abstracts the **common operations** all connections must provide\n",
    "- Specific connection types extend with their own methods\n",
    "\n",
    "### 3. Context Abstraction\n",
    "\n",
    "**File**: `odibi/context.py`\n",
    "\n",
    "```python\n",
    "class Context(ABC):\n",
    "    @abstractmethod\n",
    "    def register(self, name: str, df: Any) -> None: ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get(self, name: str) -> Any: ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def has(self, name: str) -> bool: ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def list_names(self) -> list[str]: ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def clear(self) -> None: ...\n",
    "```\n",
    "\n",
    "**Implementation Difference**:\n",
    "- `PandasContext`: Stores DataFrames in dictionary (`self._data`)\n",
    "- `SparkContext`: Registers temp views in Spark catalog (`createOrReplaceTempView`)\n",
    "\n",
    "**Why this matters**: SQL queries need to reference DataFrames by name. Each engine handles this differently:\n",
    "```python\n",
    "# Pandas: DuckDB queries an in-memory dict\n",
    "duckdb.query(\"SELECT * FROM sales\", context._data)\n",
    "\n",
    "# Spark: Queries use temp views in catalog\n",
    "spark.sql(\"SELECT * FROM sales\")  # Looks up temp view\n",
    "```\n",
    "\n",
    "Same interface, completely different storage mechanisms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Build: Abstract Data Reader\n",
    "\n",
    "Let's build a realistic abstraction layer for data reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Protocol\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataSource:\n",
    "    \"\"\"Configuration for a data source.\"\"\"\n",
    "    path: str\n",
    "    format: str\n",
    "    options: dict[str, Any] | None = None\n",
    "\n",
    "class AbstractReader(ABC):\n",
    "    \"\"\"Base class for all data readers.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def read(self, source: DataSource) -> Any:\n",
    "        \"\"\"Read data from source.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def supports_format(self, format: str) -> bool:\n",
    "        \"\"\"Check if this reader supports the given format.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metadata(self, source: DataSource) -> dict:\n",
    "        \"\"\"Get metadata without reading full data.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation 1: Pandas Reader\n",
    "import pandas as pd\n",
    "\n",
    "class PandasReader(AbstractReader):\n",
    "    \"\"\"Pandas-based data reader.\"\"\"\n",
    "    \n",
    "    SUPPORTED_FORMATS = {\"csv\", \"parquet\", \"json\", \"excel\"}\n",
    "    \n",
    "    def read(self, source: DataSource) -> pd.DataFrame:\n",
    "        options = source.options or {}\n",
    "        \n",
    "        if source.format == \"csv\":\n",
    "            return pd.read_csv(source.path, **options)\n",
    "        elif source.format == \"parquet\":\n",
    "            return pd.read_parquet(source.path, **options)\n",
    "        elif source.format == \"json\":\n",
    "            return pd.read_json(source.path, **options)\n",
    "        elif source.format == \"excel\":\n",
    "            return pd.read_excel(source.path, **options)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {source.format}\")\n",
    "    \n",
    "    def supports_format(self, format: str) -> bool:\n",
    "        return format in self.SUPPORTED_FORMATS\n",
    "    \n",
    "    def get_metadata(self, source: DataSource) -> dict:\n",
    "        # Read first row to get schema\n",
    "        df = self.read(source)\n",
    "        return {\n",
    "            \"columns\": list(df.columns),\n",
    "            \"dtypes\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "            \"row_count\": len(df),\n",
    "            \"file_size\": Path(source.path).stat().st_size if Path(source.path).exists() else 0\n",
    "        }\n",
    "\n",
    "# Test it\n",
    "reader = PandasReader()\n",
    "print(f\"Supports CSV? {reader.supports_format('csv')}\")\n",
    "print(f\"Supports XML? {reader.supports_format('xml')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation 2: Simulated Spark Reader\n",
    "\n",
    "class SparkReader(AbstractReader):\n",
    "    \"\"\"Spark-based data reader (simulated).\"\"\"\n",
    "    \n",
    "    SUPPORTED_FORMATS = {\"csv\", \"parquet\", \"json\", \"delta\", \"orc\"}\n",
    "    \n",
    "    def __init__(self, spark_session=None):\n",
    "        self.spark = spark_session  # Would be real SparkSession\n",
    "    \n",
    "    def read(self, source: DataSource) -> dict:  # Simulating Spark DataFrame\n",
    "        options = source.options or {}\n",
    "        \n",
    "        # In real implementation: self.spark.read.format(source.format).options(**options).load(source.path)\n",
    "        return {\n",
    "            \"type\": \"spark_dataframe\",\n",
    "            \"format\": source.format,\n",
    "            \"path\": source.path,\n",
    "            \"options\": options\n",
    "        }\n",
    "    \n",
    "    def supports_format(self, format: str) -> bool:\n",
    "        return format in self.SUPPORTED_FORMATS\n",
    "    \n",
    "    def get_metadata(self, source: DataSource) -> dict:\n",
    "        # In real implementation: df.schema, df.count(), etc.\n",
    "        return {\n",
    "            \"format\": source.format,\n",
    "            \"path\": source.path,\n",
    "            \"distributed\": True\n",
    "        }\n",
    "\n",
    "# Test it\n",
    "spark_reader = SparkReader()\n",
    "print(f\"Supports Delta? {spark_reader.supports_format('delta')}\")\n",
    "print(f\"Supports Excel? {spark_reader.supports_format('excel')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory Pattern: Choose reader based on scale\n",
    "\n",
    "class ReaderFactory:\n",
    "    \"\"\"Factory for creating appropriate readers.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_reader(engine: str) -> AbstractReader:\n",
    "        if engine == \"pandas\":\n",
    "            return PandasReader()\n",
    "        elif engine == \"spark\":\n",
    "            return SparkReader()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown engine: {engine}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_select(file_size_mb: float) -> AbstractReader:\n",
    "        \"\"\"Automatically select reader based on data size.\"\"\"\n",
    "        if file_size_mb < 1000:  # Less than 1GB\n",
    "            return PandasReader()\n",
    "        else:\n",
    "            return SparkReader()\n",
    "\n",
    "# Usage: Client code doesn't care about implementation\n",
    "def load_data(source: DataSource, engine: str):\n",
    "    reader = ReaderFactory.create_reader(engine)\n",
    "    \n",
    "    if not reader.supports_format(source.format):\n",
    "        raise ValueError(f\"{engine} doesn't support {source.format}\")\n",
    "    \n",
    "    return reader.read(source)\n",
    "\n",
    "# Test with both engines\n",
    "source = DataSource(path=\"data.csv\", format=\"csv\")\n",
    "\n",
    "pandas_data = load_data(source, engine=\"pandas\")\n",
    "print(f\"Loaded with Pandas: {type(pandas_data)}\")\n",
    "\n",
    "spark_data = load_data(source, engine=\"spark\")\n",
    "print(f\"Loaded with Spark: {spark_data['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Testing Abstract Implementations\n",
    "\n",
    "Abstractions make testing incredibly easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock implementation for testing\n",
    "class MockReader(AbstractReader):\n",
    "    \"\"\"Test double for data readers.\"\"\"\n",
    "    \n",
    "    def __init__(self, mock_data: Any):\n",
    "        self.mock_data = mock_data\n",
    "        self.read_count = 0\n",
    "    \n",
    "    def read(self, source: DataSource) -> Any:\n",
    "        self.read_count += 1\n",
    "        return self.mock_data\n",
    "    \n",
    "    def supports_format(self, format: str) -> bool:\n",
    "        return True  # Mock supports everything\n",
    "    \n",
    "    def get_metadata(self, source: DataSource) -> dict:\n",
    "        return {\"mock\": True}\n",
    "\n",
    "# Use mock in tests\n",
    "def process_pipeline(reader: AbstractReader, source: DataSource):\n",
    "    \"\"\"Example pipeline that works with any reader.\"\"\"\n",
    "    metadata = reader.get_metadata(source)\n",
    "    data = reader.read(source)\n",
    "    return {\"metadata\": metadata, \"data\": data}\n",
    "\n",
    "# Test without hitting real files\n",
    "mock_reader = MockReader(mock_data={\"test\": \"data\"})\n",
    "result = process_pipeline(mock_reader, DataSource(\"fake.csv\", \"csv\"))\n",
    "\n",
    "print(f\"Read called {mock_reader.read_count} times\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### When to Use ABC\n",
    "- âœ… You control the implementations\n",
    "- âœ… Want compile-time contract enforcement\n",
    "- âœ… Clear inheritance hierarchy\n",
    "- âœ… Need to prevent incomplete implementations\n",
    "\n",
    "### When to Use Protocol\n",
    "- âœ… Working with external types\n",
    "- âœ… Want structural subtyping\n",
    "- âœ… Retrofitting interfaces to existing code\n",
    "- âœ… Type checking without inheritance\n",
    "\n",
    "### Design Principles\n",
    "1. **Depend on abstractions, not concretions**\n",
    "2. **Keep interfaces minimal** (fewer methods = easier to implement)\n",
    "3. **Use composition over inheritance** (more flexible)\n",
    "4. **Factory patterns** for creating implementations\n",
    "5. **Mock implementations** for testing\n",
    "\n",
    "### Odibi's Architecture\n",
    "- **Engine ABC**: Enforces complete contract for Pandas/Spark\n",
    "- **Connection ABC**: Minimal interface for path resolution\n",
    "- **Context ABC**: Different storage strategies per engine\n",
    "- **Factory functions**: `create_context(engine)` selects implementation\n",
    "\n",
    "**The Pattern**: Define once, implement many times, use everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **exercises.ipynb**: Build storage abstraction, validator abstraction\n",
    "2. **odibi_abstractions.md**: Deep dive into Odibi's full engine implementations\n",
    "3. **Advanced topics**: Multiple inheritance, MRO, mixin patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
