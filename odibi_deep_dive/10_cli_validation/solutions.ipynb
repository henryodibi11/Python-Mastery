{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: CLI Validation\n",
    "\n",
    "**Complete solutions with explanations**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Build `odibi list` Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import fnmatch\n",
    "\n",
    "@click.group()\n",
    "def odibi():\n",
    "    \"\"\"Odibi Data Pipeline Framework.\"\"\"\n",
    "    pass\n",
    "\n",
    "@odibi.group()\n",
    "def list_cmd():\n",
    "    \"\"\"List available resources.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Rename to 'list' for CLI\n",
    "odibi.add_command(list_cmd, name='list')\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load and parse config file.\"\"\"\n",
    "    with open(config_path) as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def filter_items(items: Dict[str, Any], pattern: str) -> Dict[str, Any]:\n",
    "    \"\"\"Filter items by name pattern.\"\"\"\n",
    "    if not pattern:\n",
    "        return items\n",
    "    return {k: v for k, v in items.items() if fnmatch.fnmatch(k, pattern)}\n",
    "\n",
    "def format_output(data: Any, format_type: str) -> str:\n",
    "    \"\"\"Format data for output.\"\"\"\n",
    "    if format_type == 'json':\n",
    "        return json.dumps(data, indent=2)\n",
    "    elif format_type == 'yaml':\n",
    "        return yaml.dump(data, default_flow_style=False)\n",
    "    else:  # table\n",
    "        return data  # Already formatted as string\n",
    "\n",
    "@list_cmd.command()\n",
    "@click.argument('config', type=click.Path(exists=True))\n",
    "@click.option('--format', type=click.Choice(['table', 'json', 'yaml']), default='table')\n",
    "@click.option('--filter', 'filter_pattern', help='Filter by name pattern')\n",
    "def connections(config: str, format: str, filter_pattern: str):\n",
    "    \"\"\"List all connections.\"\"\"\n",
    "    cfg = load_config(config)\n",
    "    conns = cfg.get('connections', {})\n",
    "    conns = filter_items(conns, filter_pattern)\n",
    "    \n",
    "    if format == 'table':\n",
    "        click.secho(\"Connections:\", fg='cyan', bold=True)\n",
    "        if not conns:\n",
    "            click.echo(\"  (none)\")\n",
    "        else:\n",
    "            for name, details in conns.items():\n",
    "                conn_type = details.get('type', 'unknown')\n",
    "                click.echo(f\"  â€¢ {name} ({conn_type})\")\n",
    "                if 'catalog' in details:\n",
    "                    click.echo(f\"    Catalog: {details['catalog']}\")\n",
    "                if 'schema' in details:\n",
    "                    click.echo(f\"    Schema: {details['schema']}\")\n",
    "    else:\n",
    "        output = format_output(conns, format)\n",
    "        click.echo(output)\n",
    "\n",
    "@list_cmd.command()\n",
    "@click.argument('config', type=click.Path(exists=True))\n",
    "@click.option('--format', type=click.Choice(['table', 'json', 'yaml']), default='table')\n",
    "@click.option('--filter', 'filter_pattern', help='Filter by name pattern')\n",
    "def pipelines(config: str, format: str, filter_pattern: str):\n",
    "    \"\"\"List all pipelines.\"\"\"\n",
    "    cfg = load_config(config)\n",
    "    pipes = cfg.get('pipelines', {})\n",
    "    pipes = filter_items(pipes, filter_pattern)\n",
    "    \n",
    "    if format == 'table':\n",
    "        click.secho(\"Pipelines:\", fg='cyan', bold=True)\n",
    "        if not pipes:\n",
    "            click.echo(\"  (none)\")\n",
    "        else:\n",
    "            for name, details in pipes.items():\n",
    "                steps = details.get('steps', [])\n",
    "                step_count = len(steps)\n",
    "                click.echo(f\"  â€¢ {name} ({step_count} steps)\")\n",
    "                \n",
    "                if 'description' in details:\n",
    "                    click.echo(f\"    {details['description']}\")\n",
    "                \n",
    "                if 'connection' in details:\n",
    "                    click.echo(f\"    Connection: {details['connection']}\")\n",
    "    else:\n",
    "        output = format_output(pipes, format)\n",
    "        click.echo(output)\n",
    "\n",
    "@list_cmd.command()\n",
    "@click.argument('config', type=click.Path(exists=True))\n",
    "@click.option('--format', type=click.Choice(['table', 'json', 'yaml']), default='table')\n",
    "def schemas(config: str, format: str):\n",
    "    \"\"\"List available schemas from connections.\"\"\"\n",
    "    cfg = load_config(config)\n",
    "    conns = cfg.get('connections', {})\n",
    "    \n",
    "    schemas_by_conn = {}\n",
    "    for name, details in conns.items():\n",
    "        if 'schema' in details:\n",
    "            schemas_by_conn[name] = details['schema']\n",
    "        elif 'catalog' in details:\n",
    "            schemas_by_conn[name] = f\"{details['catalog']}.default\"\n",
    "    \n",
    "    if format == 'table':\n",
    "        click.secho(\"Schemas:\", fg='cyan', bold=True)\n",
    "        if not schemas_by_conn:\n",
    "            click.echo(\"  (none configured)\")\n",
    "        else:\n",
    "            for conn, schema in schemas_by_conn.items():\n",
    "                click.echo(f\"  â€¢ {conn}: {schema}\")\n",
    "    else:\n",
    "        output = format_output(schemas_by_conn, format)\n",
    "        click.echo(output)\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    from click.testing import CliRunner\n",
    "    \n",
    "    SAMPLE_CONFIG = \"\"\"\n",
    "connections:\n",
    "  prod_db:\n",
    "    type: databricks\n",
    "    catalog: main\n",
    "  staging_db:\n",
    "    type: databricks\n",
    "    catalog: staging\n",
    "  warehouse:\n",
    "    type: snowflake\n",
    "    account: abc123\n",
    "\n",
    "pipelines:\n",
    "  sales_etl:\n",
    "    connection: prod_db\n",
    "    description: \"Extract and transform sales data\"\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales\"\n",
    "      - transform:\n",
    "          code: \"df['revenue'] = df['price'] * df['quantity']\"\n",
    "      - load:\n",
    "          table: \"sales_clean\"\n",
    "  \n",
    "  sales_report:\n",
    "    connection: prod_db\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales_clean\"\n",
    "      - aggregate:\n",
    "          groupby: [\"region\"]\n",
    "      - transform:\n",
    "          code: \"df['margin'] = df['revenue'] - df['cost']\"\n",
    "      - validate:\n",
    "          checks: [\"no_nulls\"]\n",
    "      - load:\n",
    "          table: \"sales_summary\"\n",
    "  \n",
    "  customer_analysis:\n",
    "    connection: warehouse\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM customers\"\n",
    "      - transform:\n",
    "          code: \"df['lifetime_value'] = df['total_purchases'] * df['avg_order']\"\n",
    "\"\"\"\n",
    "    \n",
    "    runner = CliRunner()\n",
    "    \n",
    "    with runner.isolated_filesystem():\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write(SAMPLE_CONFIG)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Test 1: List connections\")\n",
    "        print(\"=\" * 60)\n",
    "        result = runner.invoke(odibi, ['list', 'connections', 'config.yaml'])\n",
    "        print(result.output)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Test 2: List pipelines with filter\")\n",
    "        print(\"=\" * 60)\n",
    "        result = runner.invoke(odibi, ['list', 'pipelines', 'config.yaml', '--filter', 'sales*'])\n",
    "        print(result.output)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Test 3: List connections as JSON\")\n",
    "        print(\"=\" * 60)\n",
    "        result = runner.invoke(odibi, ['list', 'connections', 'config.yaml', '--format', 'json'])\n",
    "        print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 2: Add Progress Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import time\n",
    "import yaml\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class StepResult:\n",
    "    \"\"\"Result from executing a step.\"\"\"\n",
    "    name: str\n",
    "    duration: float\n",
    "    rows_processed: int\n",
    "    success: bool = True\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Mock pipeline for testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, steps: List[Dict[str, Any]]):\n",
    "        self.name = name\n",
    "        self.steps = steps\n",
    "    \n",
    "    def run_step(self, step: Dict[str, Any], step_name: str) -> StepResult:\n",
    "        \"\"\"Simulate step execution.\"\"\"\n",
    "        import random\n",
    "        duration = random.uniform(0.3, 1.5)\n",
    "        time.sleep(duration)\n",
    "        \n",
    "        return StepResult(\n",
    "            name=step_name,\n",
    "            duration=duration,\n",
    "            rows_processed=random.randint(1000, 50000)\n",
    "        )\n",
    "\n",
    "@click.command()\n",
    "@click.argument('config', type=click.Path(exists=True))\n",
    "@click.option('--pipeline', '-p', help='Pipeline name to run')\n",
    "@click.option('--quiet', '-q', is_flag=True, help='No progress output')\n",
    "@click.option('--summary', is_flag=True, help='Show summary statistics')\n",
    "def run(\n",
    "    config: str,\n",
    "    pipeline: str,\n",
    "    quiet: bool,\n",
    "    summary: bool\n",
    ") -> None:\n",
    "    \"\"\"Execute pipeline with progress tracking.\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    with open(config) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    \n",
    "    # Get pipeline\n",
    "    pipelines = cfg.get('pipelines', {})\n",
    "    if not pipelines:\n",
    "        click.secho(\"No pipelines found\", fg='red', err=True)\n",
    "        raise click.Abort()\n",
    "    \n",
    "    # Select pipeline\n",
    "    if pipeline:\n",
    "        if pipeline not in pipelines:\n",
    "            click.secho(f\"Pipeline '{pipeline}' not found\", fg='red', err=True)\n",
    "            raise click.Abort()\n",
    "        pipeline_name = pipeline\n",
    "    else:\n",
    "        # Use first pipeline\n",
    "        pipeline_name = list(pipelines.keys())[0]\n",
    "    \n",
    "    pipeline_config = pipelines[pipeline_name]\n",
    "    steps = pipeline_config.get('steps', [])\n",
    "    \n",
    "    if not quiet:\n",
    "        click.secho(f\"\\nRunning pipeline: {pipeline_name}\", fg='cyan', bold=True)\n",
    "        click.echo()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipe = Pipeline(pipeline_name, steps)\n",
    "    \n",
    "    # Track statistics\n",
    "    start_time = time.time()\n",
    "    results: List[StepResult] = []\n",
    "    \n",
    "    # Execute with progress bar\n",
    "    if quiet:\n",
    "        # No progress bar\n",
    "        for i, step in enumerate(steps):\n",
    "            step_name = list(step.keys())[0]\n",
    "            result = pipe.run_step(step, step_name)\n",
    "            results.append(result)\n",
    "    else:\n",
    "        # With progress bar\n",
    "        with click.progressbar(\n",
    "            steps,\n",
    "            label='Progress',\n",
    "            show_eta=True,\n",
    "            item_show_func=lambda s: list(s.keys())[0] if s else ''\n",
    "        ) as bar:\n",
    "            for step in bar:\n",
    "                step_name = list(step.keys())[0]\n",
    "                step_start = time.time()\n",
    "                result = pipe.run_step(step, step_name)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Show step completion\n",
    "                click.echo(f\"  âœ“ {step_name} ({result.duration:.2f}s)\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Show completion\n",
    "    if not quiet:\n",
    "        click.echo()\n",
    "        click.secho(\"âœ… Pipeline completed successfully\", fg='green', bold=True)\n",
    "    \n",
    "    # Show summary\n",
    "    if summary:\n",
    "        click.echo()\n",
    "        click.secho(\"Summary:\", fg='cyan', bold=True)\n",
    "        click.echo(f\"  Pipeline: {pipeline_name}\")\n",
    "        click.echo(f\"  Total time: {total_time:.2f}s\")\n",
    "        click.echo(f\"  Steps completed: {len(results)}\")\n",
    "        \n",
    "        total_rows = sum(r.rows_processed for r in results)\n",
    "        click.echo(f\"  Rows processed: {total_rows:,}\")\n",
    "        \n",
    "        click.echo()\n",
    "        click.secho(\"  Step timings:\", fg='cyan')\n",
    "        for result in results:\n",
    "            click.echo(f\"    â€¢ {result.name}: {result.duration:.2f}s ({result.rows_processed:,} rows)\")\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    from click.testing import CliRunner\n",
    "    \n",
    "    test_config = \"\"\"\n",
    "connections:\n",
    "  db:\n",
    "    type: databricks\n",
    "\n",
    "pipelines:\n",
    "  test_pipeline:\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM source\"\n",
    "      - transform:\n",
    "          code: \"df['new_col'] = df['old_col'] * 2\"\n",
    "      - validate:\n",
    "          checks: [\"no_nulls\"]\n",
    "      - load:\n",
    "          table: \"output\"\n",
    "\"\"\"\n",
    "    \n",
    "    runner = CliRunner()\n",
    "    \n",
    "    with runner.isolated_filesystem():\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write(test_config)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Test: Run with summary\")\n",
    "        print(\"=\" * 60)\n",
    "        result = runner.invoke(run, ['config.yaml', '--summary'])\n",
    "        print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 3: Config Linter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "import re\n",
    "from typing import List, Dict, Any, Set\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LintResult:\n",
    "    severity: str  # 'error', 'warning', 'info'\n",
    "    location: str\n",
    "    message: str\n",
    "    rule: str\n",
    "    fixable: bool = False\n",
    "\n",
    "class ConfigLinter:\n",
    "    \"\"\"Lints Odibi configuration files.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.results: List[LintResult] = []\n",
    "    \n",
    "    def lint(self) -> List[LintResult]:\n",
    "        \"\"\"Run all lint checks.\"\"\"\n",
    "        self.check_unused_connections()\n",
    "        self.check_missing_descriptions()\n",
    "        self.check_missing_explanations()\n",
    "        self.check_missing_validation()\n",
    "        self.check_long_pipelines()\n",
    "        self.check_hardcoded_values()\n",
    "        return self.results\n",
    "    \n",
    "    def check_unused_connections(self):\n",
    "        \"\"\"Check for connections that are never used.\"\"\"\n",
    "        connections = set(self.config.get('connections', {}).keys())\n",
    "        used_connections: Set[str] = set()\n",
    "        \n",
    "        # Find used connections\n",
    "        for pipeline_name, pipeline in self.config.get('pipelines', {}).items():\n",
    "            if 'connection' in pipeline:\n",
    "                used_connections.add(pipeline['connection'])\n",
    "        \n",
    "        # Report unused\n",
    "        unused = connections - used_connections\n",
    "        for conn_name in unused:\n",
    "            self.results.append(LintResult(\n",
    "                severity='warning',\n",
    "                location=f\"connections.{conn_name}\",\n",
    "                message=f\"Connection '{conn_name}' is defined but never used\",\n",
    "                rule='W001',\n",
    "                fixable=True\n",
    "            ))\n",
    "    \n",
    "    def check_missing_descriptions(self):\n",
    "        \"\"\"Check for pipelines without descriptions.\"\"\"\n",
    "        for pipeline_name, pipeline in self.config.get('pipelines', {}).items():\n",
    "            if 'description' not in pipeline:\n",
    "                self.results.append(LintResult(\n",
    "                    severity='info',\n",
    "                    location=f\"pipelines.{pipeline_name}\",\n",
    "                    message=f\"Pipeline '{pipeline_name}' missing description\",\n",
    "                    rule='I001'\n",
    "                ))\n",
    "    \n",
    "    def check_missing_explanations(self):\n",
    "        \"\"\"Check for steps without explanations.\"\"\"\n",
    "        for pipeline_name, pipeline in self.config.get('pipelines', {}).items():\n",
    "            steps = pipeline.get('steps', [])\n",
    "            \n",
    "            for i, step in enumerate(steps):\n",
    "                step_type = list(step.keys())[0]\n",
    "                step_config = step[step_type]\n",
    "                \n",
    "                if isinstance(step_config, dict):\n",
    "                    if 'explanation' not in step_config:\n",
    "                        self.results.append(LintResult(\n",
    "                            severity='error',\n",
    "                            location=f\"pipelines.{pipeline_name}.steps[{i}].{step_type}\",\n",
    "                            message=f\"Step missing explanation\",\n",
    "                            rule='E001'\n",
    "                        ))\n",
    "    \n",
    "    def check_missing_validation(self):\n",
    "        \"\"\"Check for pipelines without validation steps.\"\"\"\n",
    "        for pipeline_name, pipeline in self.config.get('pipelines', {}).items():\n",
    "            steps = pipeline.get('steps', [])\n",
    "            \n",
    "            has_validation = any('validate' in step for step in steps)\n",
    "            \n",
    "            if not has_validation and len(steps) > 2:\n",
    "                self.results.append(LintResult(\n",
    "                    severity='warning',\n",
    "                    location=f\"pipelines.{pipeline_name}\",\n",
    "                    message=f\"Pipeline has no validation step\",\n",
    "                    rule='W002'\n",
    "                ))\n",
    "    \n",
    "    def check_long_pipelines(self, max_steps: int = 10):\n",
    "        \"\"\"Check for overly long pipelines.\"\"\"\n",
    "        for pipeline_name, pipeline in self.config.get('pipelines', {}).items():\n",
    "            steps = pipeline.get('steps', [])\n",
    "            \n",
    "            if len(steps) > max_steps:\n",
    "                self.results.append(LintResult(\n",
    "                    severity='warning',\n",
    "                    location=f\"pipelines.{pipeline_name}\",\n",
    "                    message=f\"Long pipeline ({len(steps)} steps, max recommended: {max_steps})\",\n",
    "                    rule='W003'\n",
    "                ))\n",
    "    \n",
    "    def check_hardcoded_values(self):\n",
    "        \"\"\"Check for hardcoded dates, numbers in SQL.\"\"\"\n",
    "        date_pattern = r\"\\d{4}-\\d{2}-\\d{2}\"\n",
    "        \n",
    "        for pipeline_name, pipeline in self.config.get('pipelines', {}).items():\n",
    "            steps = pipeline.get('steps', [])\n",
    "            \n",
    "            for i, step in enumerate(steps):\n",
    "                if 'extract' in step:\n",
    "                    extract_config = step['extract']\n",
    "                    if isinstance(extract_config, dict) and 'sql' in extract_config:\n",
    "                        sql = extract_config['sql']\n",
    "                        \n",
    "                        # Check for hardcoded dates\n",
    "                        if re.search(date_pattern, sql):\n",
    "                            dates = re.findall(date_pattern, sql)\n",
    "                            self.results.append(LintResult(\n",
    "                                severity='warning',\n",
    "                                location=f\"pipelines.{pipeline_name}.steps[{i}].extract.sql\",\n",
    "                                message=f\"Hardcoded date in SQL: {dates[0]}\",\n",
    "                                rule='W004'\n",
    "                            ))\n",
    "\n",
    "@click.command()\n",
    "@click.argument('config', type=click.Path(exists=True))\n",
    "@click.option('--fix', is_flag=True, help='Auto-fix issues where possible')\n",
    "@click.option('--strict', is_flag=True, help='Treat warnings as errors')\n",
    "def lint(config: str, fix: bool, strict: bool) -> None:\n",
    "    \"\"\"Lint configuration file for quality issues.\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    with open(config) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    \n",
    "    # Run linter\n",
    "    linter = ConfigLinter(cfg)\n",
    "    results = linter.lint()\n",
    "    \n",
    "    if not results:\n",
    "        click.secho(\"âœ… No issues found\", fg='green', bold=True)\n",
    "        return\n",
    "    \n",
    "    # Group by severity\n",
    "    errors = [r for r in results if r.severity == 'error']\n",
    "    warnings = [r for r in results if r.severity == 'warning']\n",
    "    info = [r for r in results if r.severity == 'info']\n",
    "    \n",
    "    # Display errors\n",
    "    if errors:\n",
    "        click.echo()\n",
    "        click.secho(f\"âŒ Errors ({len(errors)}):\", fg='red', bold=True)\n",
    "        for result in errors:\n",
    "            click.echo(f\"  â€¢ {result.location}: {result.message} [{result.rule}]\")\n",
    "    \n",
    "    # Display warnings\n",
    "    if warnings:\n",
    "        click.echo()\n",
    "        click.secho(f\"âš ï¸  Warnings ({len(warnings)}):\", fg='yellow', bold=True)\n",
    "        for result in warnings:\n",
    "            fixable = \" (fixable)\" if result.fixable else \"\"\n",
    "            click.echo(f\"  â€¢ {result.location}: {result.message} [{result.rule}]{fixable}\")\n",
    "    \n",
    "    # Display info\n",
    "    if info:\n",
    "        click.echo()\n",
    "        click.secho(f\"â„¹ï¸  Suggestions ({len(info)}):\", fg='blue', bold=True)\n",
    "        for result in info:\n",
    "            click.echo(f\"  â€¢ {result.location}: {result.message} [{result.rule}]\")\n",
    "    \n",
    "    # Auto-fix\n",
    "    if fix:\n",
    "        fixable_count = sum(1 for r in results if r.fixable)\n",
    "        if fixable_count > 0:\n",
    "            click.echo()\n",
    "            click.secho(f\"ðŸ”§ Auto-fixing {fixable_count} issue(s)...\", fg='cyan')\n",
    "            # Implementation would modify config and save\n",
    "            click.secho(\"Note: Auto-fix not fully implemented\", fg='yellow')\n",
    "    \n",
    "    # Exit code\n",
    "    if errors:\n",
    "        raise click.Abort()\n",
    "    \n",
    "    if strict and warnings:\n",
    "        click.echo()\n",
    "        click.secho(\"Failing due to --strict mode\", fg='red')\n",
    "        raise click.Abort()\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    from click.testing import CliRunner\n",
    "    \n",
    "    test_config = \"\"\"\n",
    "connections:\n",
    "  prod_db:\n",
    "    type: databricks\n",
    "  old_db:\n",
    "    type: postgres\n",
    "\n",
    "pipelines:\n",
    "  sales_etl:\n",
    "    connection: prod_db\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales WHERE date = '2024-01-01'\"\n",
    "      - transform:\n",
    "          code: \"df['x'] = 1\"\n",
    "      - load:\n",
    "          table: \"output\"\n",
    "\"\"\"\n",
    "    \n",
    "    runner = CliRunner()\n",
    "    \n",
    "    with runner.isolated_filesystem():\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write(test_config)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Test: Lint config\")\n",
    "        print(\"=\" * 60)\n",
    "        result = runner.invoke(lint, ['config.yaml'])\n",
    "        print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 4: Dry-Run Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "from typing import Dict, Any, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class StepPlan:\n",
    "    \"\"\"Execution plan for a single step.\"\"\"\n",
    "    name: str\n",
    "    type: str\n",
    "    operations: List[str]\n",
    "    estimated_time: float\n",
    "    estimated_rows: int\n",
    "    details: Dict[str, Any]\n",
    "\n",
    "class ExecutionPlanner:\n",
    "    \"\"\"Plans pipeline execution without running.\"\"\"\n",
    "    \n",
    "    # Historical averages (in reality, would come from metrics)\n",
    "    STEP_TIME_ESTIMATES = {\n",
    "        'extract': 2.0,\n",
    "        'transform': 1.0,\n",
    "        'aggregate': 1.5,\n",
    "        'validate': 0.5,\n",
    "        'load': 1.2,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "    \n",
    "    def plan(self, pipeline_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate execution plan.\"\"\"\n",
    "        pipeline = self.config['pipelines'][pipeline_name]\n",
    "        steps = pipeline.get('steps', [])\n",
    "        \n",
    "        step_plans = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            step_type = list(step.keys())[0]\n",
    "            step_config = step[step_type]\n",
    "            \n",
    "            plan = self._plan_step(step_type, step_config, i)\n",
    "            step_plans.append(plan)\n",
    "            total_time += plan.estimated_time\n",
    "        \n",
    "        return {\n",
    "            'pipeline_name': pipeline_name,\n",
    "            'connection': pipeline.get('connection', 'default'),\n",
    "            'steps': step_plans,\n",
    "            'total_time': total_time,\n",
    "            'data_flow': self.get_data_flow(pipeline_name, steps)\n",
    "        }\n",
    "    \n",
    "    def _plan_step(self, step_type: str, config: Any, index: int) -> StepPlan:\n",
    "        \"\"\"Plan a single step.\"\"\"\n",
    "        estimated_time = self.estimate_step_time(step_type, config)\n",
    "        estimated_rows = self.estimate_row_count(step_type, config)\n",
    "        operations = self.analyze_operations(step_type, config)\n",
    "        \n",
    "        details = {}\n",
    "        if isinstance(config, dict):\n",
    "            if 'sql' in config:\n",
    "                details['sql'] = config['sql'][:100] + '...' if len(config['sql']) > 100 else config['sql']\n",
    "            if 'table' in config:\n",
    "                details['target'] = config['table']\n",
    "            if 'checks' in config:\n",
    "                details['checks'] = config['checks']\n",
    "        \n",
    "        return StepPlan(\n",
    "            name=f\"step_{index+1}_{step_type}\",\n",
    "            type=step_type,\n",
    "            operations=operations,\n",
    "            estimated_time=estimated_time,\n",
    "            estimated_rows=estimated_rows,\n",
    "            details=details\n",
    "        )\n",
    "    \n",
    "    def estimate_step_time(self, step_type: str, config: Any) -> float:\n",
    "        \"\"\"Estimate step execution time.\"\"\"\n",
    "        base_time = self.STEP_TIME_ESTIMATES.get(step_type, 1.0)\n",
    "        \n",
    "        # Adjust based on complexity\n",
    "        if isinstance(config, dict):\n",
    "            if 'sql' in config:\n",
    "                # Longer SQL = more time\n",
    "                base_time *= (1 + len(config['sql']) / 1000)\n",
    "            if 'code' in config:\n",
    "                # Count operations\n",
    "                lines = config['code'].count('\\n') + 1\n",
    "                base_time *= (1 + lines / 10)\n",
    "        \n",
    "        return round(base_time, 1)\n",
    "    \n",
    "    def estimate_row_count(self, step_type: str, config: Any) -> int:\n",
    "        \"\"\"Estimate number of rows.\"\"\"\n",
    "        # In reality, would query metadata or use historical stats\n",
    "        if step_type == 'extract':\n",
    "            return 10000\n",
    "        elif step_type == 'aggregate':\n",
    "            return 100  # Aggregation reduces rows\n",
    "        else:\n",
    "            return 10000  # Assume same as input\n",
    "    \n",
    "    def analyze_operations(self, step_type: str, config: Any) -> List[str]:\n",
    "        \"\"\"Analyze what operations will be performed.\"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        if isinstance(config, dict):\n",
    "            if 'code' in config:\n",
    "                # Parse code for column operations\n",
    "                code = config['code']\n",
    "                if 'df[' in code:\n",
    "                    import re\n",
    "                    cols = re.findall(r\"df\\['([^']+)'\\]\", code)\n",
    "                    operations.append(f\"{len(cols)} column(s) modified\")\n",
    "            \n",
    "            if 'groupby' in config:\n",
    "                operations.append(f\"Group by {len(config['groupby'])} column(s)\")\n",
    "            \n",
    "            if 'checks' in config:\n",
    "                operations.append(f\"{len(config['checks'])} validation check(s)\")\n",
    "        \n",
    "        if not operations:\n",
    "            operations.append(f\"Standard {step_type} operation\")\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def get_data_flow(self, pipeline_name: str, steps: List[Dict]) -> str:\n",
    "        \"\"\"Generate data flow diagram.\"\"\"\n",
    "        flow = []\n",
    "        \n",
    "        for step in steps:\n",
    "            step_type = list(step.keys())[0]\n",
    "            flow.append(step_type)\n",
    "        \n",
    "        return \" â†’ \".join(flow)\n",
    "\n",
    "@click.command()\n",
    "@click.argument('config', type=click.Path(exists=True))\n",
    "@click.option('--pipeline', '-p', help='Pipeline name to run')\n",
    "@click.option('--dry-run', is_flag=True, help='Show execution plan without running')\n",
    "def run(config: str, pipeline: str, dry_run: bool) -> None:\n",
    "    \"\"\"Execute pipeline (or show plan with --dry-run).\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    with open(config) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    \n",
    "    # Get pipeline\n",
    "    pipelines = cfg.get('pipelines', {})\n",
    "    if not pipelines:\n",
    "        click.secho(\"No pipelines found\", fg='red', err=True)\n",
    "        raise click.Abort()\n",
    "    \n",
    "    # Select pipeline\n",
    "    if pipeline:\n",
    "        if pipeline not in pipelines:\n",
    "            click.secho(f\"Pipeline '{pipeline}' not found\", fg='red', err=True)\n",
    "            raise click.Abort()\n",
    "        pipeline_name = pipeline\n",
    "    else:\n",
    "        pipeline_name = list(pipelines.keys())[0]\n",
    "    \n",
    "    if dry_run:\n",
    "        # Show execution plan\n",
    "        planner = ExecutionPlanner(cfg)\n",
    "        plan = planner.plan(pipeline_name)\n",
    "        \n",
    "        click.echo()\n",
    "        click.secho(\"Execution Plan\", fg='cyan', bold=True)\n",
    "        click.secho(\"=\" * 60, fg='cyan')\n",
    "        click.echo()\n",
    "        \n",
    "        click.echo(f\"Pipeline: {plan['pipeline_name']}\")\n",
    "        click.echo(f\"Connection: {plan['connection']}\")\n",
    "        click.echo()\n",
    "        \n",
    "        click.secho(\"Steps:\", fg='cyan')\n",
    "        for i, step in enumerate(plan['steps'], 1):\n",
    "            click.echo(f\"  {i}. {step.type}\")\n",
    "            \n",
    "            for op in step.operations:\n",
    "                click.echo(f\"     â€¢ {op}\")\n",
    "            \n",
    "            if step.details:\n",
    "                for key, value in step.details.items():\n",
    "                    if key == 'sql':\n",
    "                        click.echo(f\"     SQL: {value}\")\n",
    "                    elif key == 'target':\n",
    "                        click.echo(f\"     Target: {value}\")\n",
    "                    elif key == 'checks':\n",
    "                        click.echo(f\"     Checks: {', '.join(value) if isinstance(value, list) else value}\")\n",
    "            \n",
    "            click.echo(f\"     Estimated rows: ~{step.estimated_rows:,}\")\n",
    "            click.echo(f\"     Estimated time: {step.estimated_time}s\")\n",
    "            click.echo()\n",
    "        \n",
    "        click.secho(f\"Total estimated time: {plan['total_time']:.1f}s\", fg='yellow', bold=True)\n",
    "        click.echo()\n",
    "        \n",
    "        click.secho(\"Data Flow:\", fg='cyan')\n",
    "        click.echo(f\"  {plan['data_flow']}\")\n",
    "        click.echo()\n",
    "        \n",
    "        click.secho(\"âš ï¸  This is a dry run. No data will be modified.\", fg='yellow', bold=True)\n",
    "        click.echo()\n",
    "    else:\n",
    "        # Actually run pipeline\n",
    "        click.secho(f\"Running pipeline: {pipeline_name}\", fg='green')\n",
    "        # ... execution code ...\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    from click.testing import CliRunner\n",
    "    \n",
    "    test_config = \"\"\"\n",
    "connections:\n",
    "  prod_db:\n",
    "    type: databricks\n",
    "    catalog: main\n",
    "\n",
    "pipelines:\n",
    "  sales_etl:\n",
    "    connection: prod_db\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales WHERE date >= CURRENT_DATE - 30\"\n",
    "      - transform:\n",
    "          code: |\n",
    "            df['revenue'] = df['price'] * df['quantity']\n",
    "            df['margin'] = df['revenue'] - df['cost']\n",
    "            df['margin_pct'] = (df['margin'] / df['revenue']) * 100\n",
    "      - validate:\n",
    "          checks:\n",
    "            - no_nulls: ['revenue', 'margin']\n",
    "            - range_check: {margin_pct: [0, 100]}\n",
    "      - load:\n",
    "          table: \"sales_clean\"\n",
    "\"\"\"\n",
    "    \n",
    "    runner = CliRunner()\n",
    "    \n",
    "    with runner.isolated_filesystem():\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write(test_config)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Test: Dry run\")\n",
    "        print(\"=\" * 60)\n",
    "        result = runner.invoke(run, ['config.yaml', '--dry-run'])\n",
    "        print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Click Makes CLIs Easy\n",
    "- Decorators reduce boilerplate\n",
    "- Automatic validation and help\n",
    "- Built-in testing support\n",
    "\n",
    "### 2. Rich Error Messages Matter\n",
    "- Context helps debugging\n",
    "- Suggestions guide users\n",
    "- Color improves readability\n",
    "\n",
    "### 3. Validation Prevents Issues\n",
    "- Catch errors before execution\n",
    "- Enforce quality standards\n",
    "- Provide actionable feedback\n",
    "\n",
    "### 4. Testing Is Critical\n",
    "- CliRunner enables isolated tests\n",
    "- Test success and failure cases\n",
    "- Verify exit codes and output\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
