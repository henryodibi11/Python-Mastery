{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: CLI Validation\n",
    "\n",
    "**Practice building CLIs with Click**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build `odibi list` Command\n",
    "\n",
    "Create a command that lists available resources.\n",
    "\n",
    "**Requirements:**\n",
    "- `odibi list connections` - List all connections\n",
    "- `odibi list pipelines` - List all pipelines\n",
    "- `odibi list schemas` - List available schemas\n",
    "- Add `--format` option (table, json, yaml)\n",
    "- Add `--filter` option to filter by name pattern\n",
    "\n",
    "**Example:**\n",
    "```bash\n",
    "$ odibi list connections\n",
    "Connections:\n",
    "  • prod_db (databricks)\n",
    "  • staging_db (databricks)\n",
    "  • warehouse (snowflake)\n",
    "\n",
    "$ odibi list pipelines --filter \"sales*\"\n",
    "Pipelines:\n",
    "  • sales_etl (3 steps)\n",
    "  • sales_report (5 steps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import fnmatch\n",
    "\n",
    "# Sample config for testing\n",
    "SAMPLE_CONFIG = \"\"\"\n",
    "connections:\n",
    "  prod_db:\n",
    "    type: databricks\n",
    "    catalog: main\n",
    "  staging_db:\n",
    "    type: databricks\n",
    "    catalog: staging\n",
    "  warehouse:\n",
    "    type: snowflake\n",
    "    account: abc123\n",
    "\n",
    "pipelines:\n",
    "  sales_etl:\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales\"\n",
    "      - transform:\n",
    "          code: \"df['revenue'] = df['price'] * df['quantity']\"\n",
    "      - load:\n",
    "          table: \"sales_clean\"\n",
    "  \n",
    "  sales_report:\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales_clean\"\n",
    "      - aggregate:\n",
    "          groupby: [\"region\"]\n",
    "      - transform:\n",
    "          code: \"df['margin'] = df['revenue'] - df['cost']\"\n",
    "      - validate:\n",
    "          checks: [\"no_nulls\"]\n",
    "      - load:\n",
    "          table: \"sales_summary\"\n",
    "  \n",
    "  customer_analysis:\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM customers\"\n",
    "      - transform:\n",
    "          code: \"df['lifetime_value'] = df['total_purchases'] * df['avg_order']\"\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Create the 'list' command group with subcommands\n",
    "# Implement connections, pipelines, and schemas subcommands\n",
    "# Add --format and --filter options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "if __name__ == \"__main__\":\n",
    "    from click.testing import CliRunner\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    runner = CliRunner()\n",
    "    \n",
    "    with runner.isolated_filesystem():\n",
    "        # Create test config\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write(SAMPLE_CONFIG)\n",
    "        \n",
    "        # Test commands\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Test 1: List connections\")\n",
    "        print(\"=\" * 50)\n",
    "        # result = runner.invoke(odibi, ['list', 'connections', 'config.yaml'])\n",
    "        # print(result.output)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Test 2: List pipelines with filter\")\n",
    "        print(\"=\" * 50)\n",
    "        # result = runner.invoke(odibi, ['list', 'pipelines', 'config.yaml', '--filter', 'sales*'])\n",
    "        # print(result.output)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Test 3: List connections as JSON\")\n",
    "        print(\"=\" * 50)\n",
    "        # result = runner.invoke(odibi, ['list', 'connections', 'config.yaml', '--format', 'json'])\n",
    "        # print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Add Progress Bars\n",
    "\n",
    "Enhance the `run` command with rich progress indication.\n",
    "\n",
    "**Requirements:**\n",
    "- Show progress bar for pipeline execution\n",
    "- Display current step name\n",
    "- Show timing for each step\n",
    "- Add `--quiet` mode (no progress)\n",
    "- Add `--summary` flag to show stats at end\n",
    "\n",
    "**Example:**\n",
    "```bash\n",
    "$ odibi run config.yaml\n",
    "Running pipeline: sales_etl\n",
    "Progress [################................] 50% extract (1.2s)\n",
    "\n",
    "Summary:\n",
    "  Total time: 4.8s\n",
    "  Steps completed: 3\n",
    "  Rows processed: 10,234\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Mock pipeline execution\n",
    "class Pipeline:\n",
    "    def __init__(self, name: str, steps: List[str]):\n",
    "        self.name = name\n",
    "        self.steps = steps\n",
    "    \n",
    "    def run_step(self, step_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate step execution.\"\"\"\n",
    "        import random\n",
    "        time.sleep(random.uniform(0.5, 2.0))\n",
    "        return {\n",
    "            'rows_processed': random.randint(1000, 50000),\n",
    "            'duration': random.uniform(0.5, 2.0)\n",
    "        }\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Create 'run' command with progress bars\n",
    "# Add --quiet and --summary options\n",
    "# Track timing and statistics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "if __name__ == \"__main__\":\n",
    "    runner = CliRunner()\n",
    "    \n",
    "    with runner.isolated_filesystem():\n",
    "        # Create minimal config\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write('pipelines:\\n  test:\\n    steps: []\\n')\n",
    "        \n",
    "        # Test\n",
    "        # result = runner.invoke(run, ['config.yaml', '--summary'])\n",
    "        # print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Config Linter\n",
    "\n",
    "Create a `lint` command that checks config quality.\n",
    "\n",
    "**Requirements:**\n",
    "- Check for common anti-patterns\n",
    "- Detect missing best practices\n",
    "- Report style issues\n",
    "- Add `--fix` option to auto-fix issues\n",
    "- Generate lint report\n",
    "\n",
    "**Checks:**\n",
    "- Unused connections\n",
    "- Pipelines without descriptions\n",
    "- Steps without explanations\n",
    "- Missing validation steps\n",
    "- Long pipelines (>10 steps)\n",
    "- Hardcoded values in SQL\n",
    "\n",
    "**Example:**\n",
    "```bash\n",
    "$ odibi lint config.yaml\n",
    "\n",
    "❌ Errors (2):\n",
    "  • sales_etl.step2: Missing explanation\n",
    "  • customer_pipeline: No validation step\n",
    "\n",
    "⚠️  Warnings (3):\n",
    "  • Connection 'old_db' is unused\n",
    "  • sales_etl: Long pipeline (12 steps)\n",
    "  • Hardcoded date in SQL: '2024-01-01'\n",
    "\n",
    "ℹ️  Suggestions:\n",
    "  • Add pipeline descriptions\n",
    "  • Consider splitting sales_etl into smaller pipelines\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LintResult:\n",
    "    severity: str  # 'error', 'warning', 'info'\n",
    "    location: str\n",
    "    message: str\n",
    "    rule: str\n",
    "    fixable: bool = False\n",
    "\n",
    "class ConfigLinter:\n",
    "    \"\"\"Lints Odibi configuration files.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.results: List[LintResult] = []\n",
    "    \n",
    "    def lint(self) -> List[LintResult]:\n",
    "        \"\"\"Run all lint checks.\"\"\"\n",
    "        self.check_unused_connections()\n",
    "        self.check_missing_descriptions()\n",
    "        self.check_missing_explanations()\n",
    "        self.check_missing_validation()\n",
    "        self.check_long_pipelines()\n",
    "        self.check_hardcoded_values()\n",
    "        return self.results\n",
    "    \n",
    "    def check_unused_connections(self):\n",
    "        \"\"\"Check for connections that are never used.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def check_missing_descriptions(self):\n",
    "        \"\"\"Check for pipelines without descriptions.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def check_missing_explanations(self):\n",
    "        \"\"\"Check for steps without explanations.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def check_missing_validation(self):\n",
    "        \"\"\"Check for pipelines without validation steps.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def check_long_pipelines(self, max_steps: int = 10):\n",
    "        \"\"\"Check for overly long pipelines.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def check_hardcoded_values(self):\n",
    "        \"\"\"Check for hardcoded dates, numbers in SQL.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Create 'lint' command\n",
    "# Use ConfigLinter to check config\n",
    "# Format and display results\n",
    "# Add --fix option\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "if __name__ == \"__main__\":\n",
    "    test_config = {\n",
    "        'connections': {\n",
    "            'prod_db': {'type': 'databricks'},\n",
    "            'old_db': {'type': 'postgres'},  # Unused\n",
    "        },\n",
    "        'pipelines': {\n",
    "            'sales_etl': {\n",
    "                'steps': [\n",
    "                    {'extract': {'sql': \"SELECT * FROM sales WHERE date = '2024-01-01'\"}},\n",
    "                    {'transform': {'code': 'df[\"x\"] = 1'}},  # No explanation\n",
    "                    {'load': {'table': 'output'}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    linter = ConfigLinter(test_config)\n",
    "    results = linter.lint()\n",
    "    \n",
    "    print(f\"Found {len(results)} issues:\")\n",
    "    for result in results:\n",
    "        print(f\"  {result.severity.upper()}: {result.location} - {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Dry-Run Mode\n",
    "\n",
    "Add `--dry-run` flag to `run` command.\n",
    "\n",
    "**Requirements:**\n",
    "- Show what would be executed without running\n",
    "- Display execution plan\n",
    "- Show which connections would be used\n",
    "- Estimate runtime based on historical data\n",
    "- Show data flow diagram\n",
    "\n",
    "**Example:**\n",
    "```bash\n",
    "$ odibi run config.yaml --dry-run\n",
    "\n",
    "Execution Plan:\n",
    "================\n",
    "\n",
    "Pipeline: sales_etl\n",
    "Connection: prod_db (databricks)\n",
    "\n",
    "Steps:\n",
    "  1. extract\n",
    "     • SQL: SELECT * FROM sales\n",
    "     • Estimated rows: ~10,000\n",
    "     • Estimated time: 2.3s\n",
    "  \n",
    "  2. transform\n",
    "     • Operations: 3 columns added\n",
    "     • Estimated time: 0.8s\n",
    "  \n",
    "  3. validate\n",
    "     • Checks: no_nulls, range_check\n",
    "     • Estimated time: 0.5s\n",
    "  \n",
    "  4. load\n",
    "     • Target: sales_clean\n",
    "     • Estimated time: 1.2s\n",
    "\n",
    "Total estimated time: 4.8s\n",
    "\n",
    "Data Flow:\n",
    "  sales → extract → transform → validate → sales_clean\n",
    "\n",
    "⚠️  This is a dry run. No data will be modified.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class ExecutionPlanner:\n",
    "    \"\"\"Plans pipeline execution without running.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "    \n",
    "    def plan(self, pipeline_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate execution plan.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def estimate_step_time(self, step: Dict[str, Any]) -> float:\n",
    "        \"\"\"Estimate step execution time.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Use historical data, step type, complexity\n",
    "        pass\n",
    "    \n",
    "    def estimate_row_count(self, step: Dict[str, Any]) -> int:\n",
    "        \"\"\"Estimate number of rows.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def analyze_operations(self, step: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Analyze what operations will be performed.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def get_data_flow(self, pipeline_name: str) -> str:\n",
    "        \"\"\"Generate data flow diagram.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Enhance 'run' command with --dry-run flag\n",
    "# Use ExecutionPlanner to generate plan\n",
    "# Display formatted execution plan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "if __name__ == \"__main__\":\n",
    "    test_config = \"\"\"\n",
    "connections:\n",
    "  prod_db:\n",
    "    type: databricks\n",
    "    catalog: main\n",
    "\n",
    "pipelines:\n",
    "  sales_etl:\n",
    "    connection: prod_db\n",
    "    steps:\n",
    "      - extract:\n",
    "          sql: \"SELECT * FROM sales\"\n",
    "      - transform:\n",
    "          code: |\n",
    "            df['revenue'] = df['price'] * df['quantity']\n",
    "            df['margin'] = df['revenue'] - df['cost']\n",
    "      - validate:\n",
    "          checks:\n",
    "            - no_nulls: ['revenue', 'margin']\n",
    "      - load:\n",
    "          table: \"sales_clean\"\n",
    "\"\"\"\n",
    "    \n",
    "    runner = CliRunner()\n",
    "    with runner.isolated_filesystem():\n",
    "        with open('config.yaml', 'w') as f:\n",
    "            f.write(test_config)\n",
    "        \n",
    "        # Test dry run\n",
    "        # result = runner.invoke(run, ['config.yaml', '--dry-run'])\n",
    "        # print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Challenge: Interactive Config Builder\n",
    "\n",
    "Create `odibi init` command that interactively builds config.\n",
    "\n",
    "**Requirements:**\n",
    "- Ask user for project name\n",
    "- Prompt for connections (with validation)\n",
    "- Guide pipeline creation\n",
    "- Suggest best practices\n",
    "- Generate complete config file\n",
    "- Add example pipelines\n",
    "\n",
    "**Example:**\n",
    "```bash\n",
    "$ odibi init\n",
    "\n",
    "Welcome to Odibi! Let's set up your project.\n",
    "\n",
    "Project name: My Analytics Project\n",
    "\n",
    "Add a connection? [y/n]: y\n",
    "Connection name: prod_db\n",
    "Connection type: \n",
    "  1. Databricks\n",
    "  2. Snowflake\n",
    "  3. Postgres\n",
    "Choice [1]: 1\n",
    "\n",
    "Databricks catalog: main\n",
    "Databricks schema: analytics\n",
    "\n",
    "✓ Connection 'prod_db' configured\n",
    "\n",
    "Add another connection? [y/n]: n\n",
    "\n",
    "Create a pipeline? [y/n]: y\n",
    "Pipeline name: sales_etl\n",
    "...\n",
    "\n",
    "✅ Configuration saved to odibi_config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Create 'init' command with interactive prompts\n",
    "# Use click.prompt() and click.confirm()\n",
    "# Build config incrementally\n",
    "# Generate YAML output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "# Note: Can't fully test interactive mode in notebook\n",
    "# But you can test with CliRunner and input parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
