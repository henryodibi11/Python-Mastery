{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Execution Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Custom Node Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, List\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import time\n",
    "import pandasql as ps\n",
    "\n",
    "@dataclass\n",
    "class SimpleNodeConfig:\n",
    "    name: str\n",
    "    read_path: Optional[str] = None\n",
    "    transform_sql: Optional[str] = None\n",
    "    write_path: Optional[str] = None\n",
    "\n",
    "class SimpleNode:\n",
    "    \"\"\"Simplified Node implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SimpleNodeConfig):\n",
    "        self.config = config\n",
    "        self._execution_steps = []\n",
    "    \n",
    "    def execute(self) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the node.\"\"\"\n",
    "        start_time = time.time()\n",
    "        result_df = None\n",
    "        \n",
    "        try:\n",
    "            # Read phase\n",
    "            if self.config.read_path:\n",
    "                result_df = pd.read_csv(self.config.read_path)\n",
    "                self._execution_steps.append(f\"Read from {self.config.read_path}\")\n",
    "            \n",
    "            # Transform phase\n",
    "            if self.config.transform_sql and result_df is not None:\n",
    "                # Use pandasql to execute SQL on DataFrame\n",
    "                result_df = ps.sqldf(self.config.transform_sql, locals())\n",
    "                self._execution_steps.append(\"Applied SQL transform\")\n",
    "            \n",
    "            # Write phase\n",
    "            if self.config.write_path and result_df is not None:\n",
    "                result_df.to_csv(self.config.write_path, index=False)\n",
    "                self._execution_steps.append(f\"Written to {self.config.write_path}\")\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"duration\": duration,\n",
    "                \"rows\": len(result_df) if result_df is not None else 0,\n",
    "                \"steps\": self._execution_steps\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            duration = time.time() - start_time\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"duration\": duration,\n",
    "                \"error\": str(e),\n",
    "                \"steps\": self._execution_steps\n",
    "            }\n",
    "\n",
    "# Test\n",
    "test_data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"value\": [10, 20, 30, 40, 50]\n",
    "})\n",
    "test_data.to_csv(\"test_input.csv\", index=False)\n",
    "\n",
    "config = SimpleNodeConfig(\n",
    "    name=\"test_node\",\n",
    "    read_path=\"test_input.csv\",\n",
    "    transform_sql=\"SELECT * FROM result_df WHERE value > 20\",\n",
    "    write_path=\"test_output.csv\"\n",
    ")\n",
    "\n",
    "node = SimpleNode(config)\n",
    "result = node.execute()\n",
    "print(\"Execution result:\", result)\n",
    "\n",
    "# Verify output\n",
    "output = pd.read_csv(\"test_output.csv\")\n",
    "print(\"\\nOutput DataFrame:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Transform Step Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Callable\n",
    "import pandasql as ps\n",
    "\n",
    "class TransformRouter:\n",
    "    \"\"\"Routes transform steps to appropriate executors.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.functions = {}\n",
    "    \n",
    "    def register_function(self, name: str, func: Callable):\n",
    "        \"\"\"Register a transform function.\"\"\"\n",
    "        self.functions[name] = func\n",
    "    \n",
    "    def execute_step(self, step: Union[str, Dict], current_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Execute a transform step.\"\"\"\n",
    "        \n",
    "        # SQL step (string)\n",
    "        if isinstance(step, str):\n",
    "            # Execute SQL using pandasql\n",
    "            # Make current_df available as 'df' in SQL\n",
    "            df = current_df\n",
    "            return ps.sqldf(step, locals())\n",
    "        \n",
    "        # Function or operation step (dict)\n",
    "        elif isinstance(step, dict):\n",
    "            if \"function\" in step:\n",
    "                # Function step\n",
    "                func_name = step[\"function\"]\n",
    "                params = step.get(\"params\", {})\n",
    "                \n",
    "                if func_name not in self.functions:\n",
    "                    raise ValueError(f\"Function '{func_name}' not registered\")\n",
    "                \n",
    "                func = self.functions[func_name]\n",
    "                return func(current_df.copy(), **params)\n",
    "            \n",
    "            elif \"operation\" in step:\n",
    "                # Built-in operation\n",
    "                operation = step[\"operation\"]\n",
    "                params = step.get(\"params\", {})\n",
    "                \n",
    "                if operation == \"pivot\":\n",
    "                    return current_df.pivot(**params)\n",
    "                elif operation == \"melt\":\n",
    "                    return current_df.melt(**params)\n",
    "                elif operation == \"sort\":\n",
    "                    return current_df.sort_values(**params)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown operation: {operation}\")\n",
    "        \n",
    "        raise ValueError(f\"Invalid step format: {step}\")\n",
    "\n",
    "# Test\n",
    "router = TransformRouter()\n",
    "\n",
    "# Register custom functions\n",
    "def double_values(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df[column] = df[column] * 2\n",
    "    return df\n",
    "\n",
    "def add_column(df: pd.DataFrame, name: str, value: Any) -> pd.DataFrame:\n",
    "    df[name] = value\n",
    "    return df\n",
    "\n",
    "router.register_function(\"double_values\", double_values)\n",
    "router.register_function(\"add_column\", add_column)\n",
    "\n",
    "# Test data\n",
    "df = pd.DataFrame({\"id\": [1, 2, 3], \"value\": [10, 20, 30]})\n",
    "\n",
    "# Test 1: SQL step\n",
    "print(\"Test 1: SQL step\")\n",
    "result1 = router.execute_step(\"SELECT * FROM df WHERE value > 15\", df)\n",
    "print(result1)\n",
    "\n",
    "# Test 2: Function step\n",
    "print(\"\\nTest 2: Function step\")\n",
    "result2 = router.execute_step(\n",
    "    {\"function\": \"double_values\", \"params\": {\"column\": \"value\"}},\n",
    "    df\n",
    ")\n",
    "print(result2)\n",
    "\n",
    "# Test 3: Operation step\n",
    "print(\"\\nTest 3: Operation step\")\n",
    "result3 = router.execute_step(\n",
    "    {\"operation\": \"sort\", \"params\": {\"by\": \"value\", \"ascending\": False}},\n",
    "    df\n",
    ")\n",
    "print(result3)\n",
    "\n",
    "# Test 4: Chained steps\n",
    "print(\"\\nTest 4: Chained steps\")\n",
    "steps = [\n",
    "    {\"function\": \"add_column\", \"params\": {\"name\": \"category\", \"value\": \"A\"}},\n",
    "    \"SELECT id, value * 2 as value, category FROM df\",\n",
    "    {\"operation\": \"sort\", \"params\": {\"by\": \"value\"}}\n",
    "]\n",
    "result = df\n",
    "for step in steps:\n",
    "    result = router.execute_step(step, result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Validation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    not_empty: bool = False\n",
    "    no_nulls: Optional[List[str]] = None\n",
    "    min_rows: Optional[int] = None\n",
    "    max_rows: Optional[int] = None\n",
    "    required_columns: Optional[List[str]] = None\n",
    "\n",
    "class ValidationEngine:\n",
    "    \"\"\"Validates DataFrames against rules.\"\"\"\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame, config: ValidationConfig) -> List[str]:\n",
    "        \"\"\"Validate DataFrame against rules.\"\"\"\n",
    "        failures = []\n",
    "        \n",
    "        # Check 1: Not empty\n",
    "        if config.not_empty:\n",
    "            if len(df) == 0:\n",
    "                failures.append(\"DataFrame is empty\")\n",
    "        \n",
    "        # Check 2: No nulls\n",
    "        if config.no_nulls:\n",
    "            for col in config.no_nulls:\n",
    "                if col not in df.columns:\n",
    "                    failures.append(f\"Column '{col}' does not exist\")\n",
    "                    continue\n",
    "                \n",
    "                null_count = df[col].isnull().sum()\n",
    "                if null_count > 0:\n",
    "                    failures.append(f\"Column '{col}' has {null_count} null values\")\n",
    "        \n",
    "        # Check 3: Min rows\n",
    "        if config.min_rows is not None:\n",
    "            if len(df) < config.min_rows:\n",
    "                failures.append(\n",
    "                    f\"DataFrame has {len(df)} rows, minimum required is {config.min_rows}\"\n",
    "                )\n",
    "        \n",
    "        # Check 4: Max rows\n",
    "        if config.max_rows is not None:\n",
    "            if len(df) > config.max_rows:\n",
    "                failures.append(\n",
    "                    f\"DataFrame has {len(df)} rows, maximum allowed is {config.max_rows}\"\n",
    "                )\n",
    "        \n",
    "        # Check 5: Required columns\n",
    "        if config.required_columns:\n",
    "            missing_cols = set(config.required_columns) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                failures.append(\n",
    "                    f\"Missing required columns: {', '.join(missing_cols)}\"\n",
    "                )\n",
    "        \n",
    "        return failures\n",
    "\n",
    "# Test suite\n",
    "validator = ValidationEngine()\n",
    "\n",
    "print(\"Running validation tests...\\n\")\n",
    "\n",
    "# Test 1: Valid DataFrame\n",
    "df1 = pd.DataFrame({\"id\": [1, 2, 3], \"name\": [\"A\", \"B\", \"C\"]})\n",
    "config1 = ValidationConfig(\n",
    "    not_empty=True,\n",
    "    no_nulls=[\"id\", \"name\"],\n",
    "    min_rows=2,\n",
    "    required_columns=[\"id\", \"name\"]\n",
    ")\n",
    "failures1 = validator.validate(df1, config1)\n",
    "assert len(failures1) == 0, f\"Expected no failures, got: {failures1}\"\n",
    "print(\"‚úì Test 1 passed: Valid DataFrame\")\n",
    "\n",
    "# Test 2: Empty DataFrame\n",
    "df2 = pd.DataFrame()\n",
    "config2 = ValidationConfig(not_empty=True)\n",
    "failures2 = validator.validate(df2, config2)\n",
    "assert len(failures2) > 0\n",
    "print(f\"‚úì Test 2 passed: Empty DataFrame detected - {failures2[0]}\")\n",
    "\n",
    "# Test 3: Null values\n",
    "df3 = pd.DataFrame({\"id\": [1, None, 3], \"name\": [\"A\", \"B\", \"C\"]})\n",
    "config3 = ValidationConfig(no_nulls=[\"id\"])\n",
    "failures3 = validator.validate(df3, config3)\n",
    "assert len(failures3) > 0\n",
    "print(f\"‚úì Test 3 passed: Null values detected - {failures3[0]}\")\n",
    "\n",
    "# Test 4: Row count violations\n",
    "df4 = pd.DataFrame({\"id\": [1, 2]})\n",
    "config4 = ValidationConfig(min_rows=5, max_rows=1)\n",
    "failures4 = validator.validate(df4, config4)\n",
    "assert len(failures4) == 2  # Both min and max violated\n",
    "print(f\"‚úì Test 4 passed: Row count violations - {len(failures4)} failures\")\n",
    "\n",
    "# Test 5: Missing columns\n",
    "df5 = pd.DataFrame({\"id\": [1, 2, 3]})\n",
    "config5 = ValidationConfig(required_columns=[\"id\", \"name\", \"value\"])\n",
    "failures5 = validator.validate(df5, config5)\n",
    "assert len(failures5) > 0\n",
    "print(f\"‚úì Test 5 passed: Missing columns detected - {failures5[0]}\")\n",
    "\n",
    "# Test 6: Multiple failures\n",
    "df6 = pd.DataFrame({\"id\": [None, None]})\n",
    "config6 = ValidationConfig(\n",
    "    not_empty=True,\n",
    "    no_nulls=[\"id\"],\n",
    "    min_rows=5,\n",
    "    required_columns=[\"id\", \"name\"]\n",
    ")\n",
    "failures6 = validator.validate(df6, config6)\n",
    "print(f\"\\n‚úì Test 6 passed: Multiple failures ({len(failures6)}):\")\n",
    "for failure in failures6:\n",
    "    print(f\"  - {failure}\")\n",
    "\n",
    "print(\"\\nAll validation tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Error Context Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class ExecutionContext:\n",
    "    node_name: str\n",
    "    config_file: Optional[str] = None\n",
    "    step_index: Optional[int] = None\n",
    "    total_steps: Optional[int] = None\n",
    "    previous_steps: List[str] = field(default_factory=list)\n",
    "    input_schema: List[str] = field(default_factory=list)\n",
    "\n",
    "class ErrorContextBuilder:\n",
    "    \"\"\"Builds rich error messages with context.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_message(error: Exception, context: ExecutionContext) -> str:\n",
    "        \"\"\"Build a comprehensive error message.\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"=\" * 70)\n",
    "        lines.append(\"NODE EXECUTION ERROR\")\n",
    "        lines.append(\"=\" * 70)\n",
    "        \n",
    "        # Node and file info\n",
    "        lines.append(f\"\\nNode: {context.node_name}\")\n",
    "        if context.config_file:\n",
    "            lines.append(f\"Config: {context.config_file}\")\n",
    "        \n",
    "        # Step info\n",
    "        if context.step_index is not None and context.total_steps is not None:\n",
    "            lines.append(\n",
    "                f\"\\nFailed at: Step {context.step_index + 1} of {context.total_steps}\"\n",
    "            )\n",
    "        \n",
    "        # Previous steps (what succeeded)\n",
    "        if context.previous_steps:\n",
    "            lines.append(\"\\nPrevious successful steps:\")\n",
    "            for i, step in enumerate(context.previous_steps, 1):\n",
    "                lines.append(f\"  {i}. ‚úì {step}\")\n",
    "        \n",
    "        # Input schema\n",
    "        if context.input_schema:\n",
    "            lines.append(f\"\\nInput schema: {', '.join(context.input_schema)}\")\n",
    "        \n",
    "        # Original error\n",
    "        lines.append(f\"\\nError type: {error.__class__.__name__}\")\n",
    "        lines.append(f\"Error message: {str(error)}\")\n",
    "        \n",
    "        # Suggestions\n",
    "        suggestions = ErrorContextBuilder.generate_suggestions(error)\n",
    "        if suggestions:\n",
    "            lines.append(\"\\nSuggestions:\")\n",
    "            for suggestion in suggestions:\n",
    "                lines.append(f\"  üí° {suggestion}\")\n",
    "        \n",
    "        lines.append(\"\\n\" + \"=\" * 70)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_suggestions(error: Exception) -> List[str]:\n",
    "        \"\"\"Generate helpful suggestions based on error.\"\"\"\n",
    "        suggestions = []\n",
    "        error_str = str(error).lower()\n",
    "        error_type = error.__class__.__name__.lower()\n",
    "        \n",
    "        # KeyError suggestions\n",
    "        if \"keyerror\" in error_type:\n",
    "            suggestions.append(\n",
    "                \"Check that all referenced DataFrames are registered in context\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Verify node dependencies in 'depends_on' list\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Ensure previous nodes executed successfully\"\n",
    "            )\n",
    "        \n",
    "        # Column errors\n",
    "        if \"column\" in error_str or \"attributeerror\" in error_type:\n",
    "            suggestions.append(\n",
    "                \"Check that previous nodes output the expected columns\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Use df.columns to inspect available columns\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Verify column names match exactly (case-sensitive)\"\n",
    "            )\n",
    "        \n",
    "        # Type errors\n",
    "        if \"valueerror\" in error_type or \"typeerror\" in error_type:\n",
    "            suggestions.append(\n",
    "                \"Check data types match expected types\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Use df.dtypes to inspect column types\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Consider adding type conversions in transform step\"\n",
    "            )\n",
    "        \n",
    "        # File errors\n",
    "        if \"filenotfounderror\" in error_type or \"not found\" in error_str:\n",
    "            suggestions.append(\n",
    "                \"Verify file paths are correct and files exist\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Check file permissions\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Use absolute paths instead of relative paths\"\n",
    "            )\n",
    "        \n",
    "        # Function errors\n",
    "        if \"function\" in error_str and \"not\" in error_str:\n",
    "            suggestions.append(\n",
    "                \"Ensure the transform function is decorated with @transform\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Import the module containing the transform function\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                \"Check function name spelling in config\"\n",
    "            )\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# Test cases\n",
    "builder = ErrorContextBuilder()\n",
    "\n",
    "print(\"Test 1: Transform step error\\n\")\n",
    "context1 = ExecutionContext(\n",
    "    node_name=\"transform_data\",\n",
    "    config_file=\"pipeline.yaml\",\n",
    "    step_index=2,\n",
    "    total_steps=5,\n",
    "    previous_steps=[\"Read from CSV\", \"Applied filter\"],\n",
    "    input_schema=[\"id\", \"name\", \"value\"]\n",
    ")\n",
    "error1 = KeyError(\"customer_id\")\n",
    "message1 = builder.build_message(error1, context1)\n",
    "print(message1)\n",
    "\n",
    "print(\"\\n\\nTest 2: Column not found error\\n\")\n",
    "context2 = ExecutionContext(\n",
    "    node_name=\"aggregate_data\",\n",
    "    previous_steps=[\"Read from source\"],\n",
    "    input_schema=[\"id\", \"name\"]\n",
    ")\n",
    "error2 = AttributeError(\"DataFrame has no column 'total'\")\n",
    "message2 = builder.build_message(error2, context2)\n",
    "print(message2)\n",
    "\n",
    "print(\"\\n\\nTest 3: File not found error\\n\")\n",
    "context3 = ExecutionContext(\n",
    "    node_name=\"load_data\",\n",
    "    config_file=\"etl.yaml\"\n",
    ")\n",
    "error3 = FileNotFoundError(\"data/input.csv not found\")\n",
    "message3 = builder.build_message(error3, context3)\n",
    "print(message3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Metadata Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, Dict, Optional\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class MetadataCollector:\n",
    "    \"\"\"Collects execution metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.execution_steps = []\n",
    "    \n",
    "    def track_step(self, step: str):\n",
    "        \"\"\"Track an execution step.\"\"\"\n",
    "        self.execution_steps.append(step)\n",
    "    \n",
    "    def collect(self, df: Optional[pd.DataFrame], duration: float) -> Dict[str, Any]:\n",
    "        \"\"\"Collect comprehensive metadata.\"\"\"\n",
    "        metadata = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration\": round(duration, 3),\n",
    "            \"steps\": self.execution_steps.copy()\n",
    "        }\n",
    "        \n",
    "        if df is not None:\n",
    "            # Basic stats\n",
    "            metadata[\"rows\"] = len(df)\n",
    "            metadata[\"columns\"] = len(df.columns)\n",
    "            \n",
    "            # Schema with types\n",
    "            metadata[\"schema\"] = {\n",
    "                col: str(dtype) for col, dtype in df.dtypes.items()\n",
    "            }\n",
    "            \n",
    "            # Memory usage (in MB)\n",
    "            memory_bytes = df.memory_usage(deep=True).sum()\n",
    "            metadata[\"memory_usage_mb\"] = round(memory_bytes / (1024 ** 2), 2)\n",
    "            \n",
    "            # Null counts per column\n",
    "            null_counts = df.isnull().sum()\n",
    "            metadata[\"null_counts\"] = {\n",
    "                col: int(count) for col, count in null_counts.items() if count > 0\n",
    "            }\n",
    "            \n",
    "            # Data quality score (% of non-null values)\n",
    "            total_cells = len(df) * len(df.columns)\n",
    "            null_cells = df.isnull().sum().sum()\n",
    "            metadata[\"data_quality_score\"] = round(\n",
    "                ((total_cells - null_cells) / total_cells * 100) if total_cells > 0 else 0,\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def to_json(self, df: Optional[pd.DataFrame], duration: float) -> str:\n",
    "        \"\"\"Export metadata as JSON.\"\"\"\n",
    "        metadata = self.collect(df, duration)\n",
    "        return json.dumps(metadata, indent=2)\n",
    "    \n",
    "    def display(self, df: Optional[pd.DataFrame], duration: float):\n",
    "        \"\"\"Display formatted metadata.\"\"\"\n",
    "        metadata = self.collect(df, duration)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"EXECUTION METADATA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nTimestamp: {metadata['timestamp']}\")\n",
    "        print(f\"Duration: {metadata['duration']}s\")\n",
    "        \n",
    "        print(\"\\nExecution Steps:\")\n",
    "        for i, step in enumerate(metadata['steps'], 1):\n",
    "            print(f\"  {i}. {step}\")\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"\\nDataFrame Stats:\")\n",
    "            print(f\"  Rows: {metadata['rows']:,}\")\n",
    "            print(f\"  Columns: {metadata['columns']}\")\n",
    "            print(f\"  Memory: {metadata['memory_usage_mb']} MB\")\n",
    "            print(f\"  Quality Score: {metadata['data_quality_score']}%\")\n",
    "            \n",
    "            print(f\"\\nSchema:\")\n",
    "            for col, dtype in metadata['schema'].items():\n",
    "                null_info = \"\"\n",
    "                if col in metadata['null_counts']:\n",
    "                    null_info = f\" ({metadata['null_counts'][col]} nulls)\"\n",
    "                print(f\"  {col}: {dtype}{null_info}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Test\n",
    "collector = MetadataCollector()\n",
    "collector.track_step(\"Read from CSV\")\n",
    "collector.track_step(\"Applied filter: value > 10\")\n",
    "collector.track_step(\"Grouped by category\")\n",
    "collector.track_step(\"Calculated aggregates\")\n",
    "\n",
    "# Create test DataFrame with some nulls\n",
    "test_df = pd.DataFrame({\n",
    "    \"id\": range(1, 1001),\n",
    "    \"value\": [i * 10 if i % 5 != 0 else None for i in range(1, 1001)],\n",
    "    \"category\": [\"A\", \"B\", \"C\", \"D\", \"E\"] * 200,\n",
    "    \"name\": [f\"Item_{i}\" for i in range(1, 1001)],\n",
    "    \"score\": [i / 10 if i % 7 != 0 else None for i in range(1, 1001)]\n",
    "})\n",
    "\n",
    "# Display metadata\n",
    "collector.display(test_df, duration=0.245)\n",
    "\n",
    "# Export as JSON\n",
    "print(\"\\n\\nMetadata as JSON:\")\n",
    "print(collector.to_json(test_df, duration=0.245))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Mini Orchestrator (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Set\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "\n",
    "class MiniOrchestrator:\n",
    "    \"\"\"Simple node orchestrator with topological sort.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodes = {}  # node_name -> {node, depends_on}\n",
    "        self.results = {}  # node_name -> result\n",
    "    \n",
    "    def add_node(self, node, depends_on: List[str] = None):\n",
    "        \"\"\"Add a node with dependencies.\"\"\"\n",
    "        self.nodes[node.config.name] = {\n",
    "            \"node\": node,\n",
    "            \"depends_on\": depends_on or []\n",
    "        }\n",
    "    \n",
    "    def _topological_sort(self) -> List[str]:\n",
    "        \"\"\"Sort nodes in dependency order using Kahn's algorithm.\"\"\"\n",
    "        # Build in-degree map\n",
    "        in_degree = {name: 0 for name in self.nodes}\n",
    "        \n",
    "        for name, info in self.nodes.items():\n",
    "            for dep in info[\"depends_on\"]:\n",
    "                if dep in in_degree:\n",
    "                    in_degree[name] += 1\n",
    "        \n",
    "        # Queue nodes with no dependencies\n",
    "        queue = deque([name for name, degree in in_degree.items() if degree == 0])\n",
    "        sorted_nodes = []\n",
    "        \n",
    "        while queue:\n",
    "            # Process node with no remaining dependencies\n",
    "            current = queue.popleft()\n",
    "            sorted_nodes.append(current)\n",
    "            \n",
    "            # Reduce in-degree for dependent nodes\n",
    "            for name, info in self.nodes.items():\n",
    "                if current in info[\"depends_on\"]:\n",
    "                    in_degree[name] -= 1\n",
    "                    if in_degree[name] == 0:\n",
    "                        queue.append(name)\n",
    "        \n",
    "        # Check for cycles\n",
    "        if len(sorted_nodes) != len(self.nodes):\n",
    "            raise ValueError(\"Circular dependency detected in node graph\")\n",
    "        \n",
    "        return sorted_nodes\n",
    "    \n",
    "    def execute_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"Execute all nodes in dependency order.\"\"\"\n",
    "        start_time = time.time()\n",
    "        execution_order = self._topological_sort()\n",
    "        \n",
    "        summary = {\n",
    "            \"total_nodes\": len(self.nodes),\n",
    "            \"execution_order\": execution_order,\n",
    "            \"node_results\": {},\n",
    "            \"failures\": [],\n",
    "            \"total_duration\": 0\n",
    "        }\n",
    "        \n",
    "        print(f\"Executing {len(execution_order)} nodes in order: {' ‚Üí '.join(execution_order)}\\n\")\n",
    "        \n",
    "        for node_name in execution_order:\n",
    "            node_info = self.nodes[node_name]\n",
    "            node = node_info[\"node\"]\n",
    "            \n",
    "            # Check dependencies succeeded\n",
    "            deps_failed = False\n",
    "            for dep in node_info[\"depends_on\"]:\n",
    "                if dep in summary[\"failures\"]:\n",
    "                    print(f\"‚è≠Ô∏è  Skipping {node_name} (dependency {dep} failed)\")\n",
    "                    summary[\"failures\"].append(node_name)\n",
    "                    deps_failed = True\n",
    "                    break\n",
    "            \n",
    "            if deps_failed:\n",
    "                continue\n",
    "            \n",
    "            # Execute node\n",
    "            print(f\"‚ñ∂Ô∏è  Executing {node_name}...\")\n",
    "            result = node.execute()\n",
    "            \n",
    "            self.results[node_name] = result\n",
    "            summary[\"node_results\"][node_name] = result\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"   ‚úì Success ({result['duration']:.3f}s, {result.get('rows', 0)} rows)\")\n",
    "            else:\n",
    "                print(f\"   ‚úó Failed: {result.get('error', 'Unknown error')}\")\n",
    "                summary[\"failures\"].append(node_name)\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        summary[\"total_duration\"] = time.time() - start_time\n",
    "        summary[\"success_count\"] = len(execution_order) - len(summary[\"failures\"])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def display_summary(self, summary: Dict[str, Any]):\n",
    "        \"\"\"Display execution summary.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ORCHESTRATION SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nTotal nodes: {summary['total_nodes']}\")\n",
    "        print(f\"Successful: {summary['success_count']}\")\n",
    "        print(f\"Failed: {len(summary['failures'])}\")\n",
    "        print(f\"Total duration: {summary['total_duration']:.3f}s\")\n",
    "        \n",
    "        if summary['failures']:\n",
    "            print(f\"\\nFailed nodes: {', '.join(summary['failures'])}\")\n",
    "        \n",
    "        print(\"\\nNode Details:\")\n",
    "        for node_name in summary['execution_order']:\n",
    "            if node_name in summary['node_results']:\n",
    "                result = summary['node_results'][node_name]\n",
    "                status = \"‚úì\" if result['success'] else \"‚úó\"\n",
    "                print(f\"  {status} {node_name}: {result['duration']:.3f}s\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Test with a 3-node pipeline\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MockNodeConfig:\n",
    "    name: str\n",
    "\n",
    "class MockNode:\n",
    "    def __init__(self, name: str, should_fail: bool = False):\n",
    "        self.config = MockNodeConfig(name=name)\n",
    "        self.should_fail = should_fail\n",
    "    \n",
    "    def execute(self):\n",
    "        import time\n",
    "        import random\n",
    "        \n",
    "        duration = random.uniform(0.1, 0.3)\n",
    "        time.sleep(duration)\n",
    "        \n",
    "        if self.should_fail:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"duration\": duration,\n",
    "                \"error\": \"Simulated failure\",\n",
    "                \"steps\": []\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"duration\": duration,\n",
    "            \"rows\": random.randint(100, 1000),\n",
    "            \"steps\": [f\"Executed {self.config.name}\"]\n",
    "        }\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = MiniOrchestrator()\n",
    "\n",
    "# Add nodes\n",
    "node_a = MockNode(\"load_data\")\n",
    "node_b = MockNode(\"transform_data\")\n",
    "node_c = MockNode(\"aggregate_data\")\n",
    "node_d = MockNode(\"write_results\")\n",
    "\n",
    "orchestrator.add_node(node_a)  # No dependencies\n",
    "orchestrator.add_node(node_b, depends_on=[\"load_data\"])  # Depends on A\n",
    "orchestrator.add_node(node_c, depends_on=[\"transform_data\"])  # Depends on B\n",
    "orchestrator.add_node(node_d, depends_on=[\"aggregate_data\"])  # Depends on C\n",
    "\n",
    "# Execute pipeline\n",
    "summary = orchestrator.execute_all()\n",
    "\n",
    "# Display summary\n",
    "orchestrator.display_summary(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
