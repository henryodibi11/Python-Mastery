{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Execution Deep Dive\n",
    "\n",
    "## Introduction\n",
    "This lesson explores ODIBI's Node execution engine - the orchestrator that transforms declarative configs into actual data operations.\n",
    "\n",
    "### What You'll Learn\n",
    "1. Node class architecture and initialization\n",
    "2. The 4-phase execution lifecycle\n",
    "3. Engine and connection integration\n",
    "4. Transform execution patterns\n",
    "5. Validation and error handling\n",
    "6. Metadata collection and caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Node Architecture\n",
    "\n",
    "### Node Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.node import Node, NodeResult\n",
    "from odibi.config import NodeConfig\n",
    "from odibi.context import Context\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Node requires 5 key components\n",
    "class NodeComponents:\n",
    "    \"\"\"Understanding Node initialization.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def show_structure():\n",
    "        print(\"\"\"\n",
    "Node Initialization Requirements:\n",
    "\n",
    "1. config: NodeConfig\n",
    "   - Declarative configuration (read/transform/validate/write)\n",
    "   - Parsed and validated by Pydantic\n",
    "   \n",
    "2. context: Context\n",
    "   - In-memory data registry for passing DataFrames\n",
    "   - Enables node-to-node communication\n",
    "   \n",
    "3. engine: Any (Spark/Pandas adapter)\n",
    "   - Abstraction over execution engine\n",
    "   - Implements read/write/transform operations\n",
    "   \n",
    "4. connections: Dict[str, Any]\n",
    "   - Registry of available data connections\n",
    "   - Resolved from ConnectionConfig\n",
    "   \n",
    "5. config_file: Optional[str]\n",
    "   - Path to YAML config (for error reporting)\n",
    "   - Enriches error messages with file context\n",
    "        \"\"\")\n",
    "\n",
    "NodeComponents.show_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a Node instance\n",
    "import pandas as pd\n",
    "from odibi.engines.pandas_engine import PandasEngine\n",
    "\n",
    "# Setup components\n",
    "config = NodeConfig(\n",
    "    name=\"example_node\",\n",
    "    read={\"connection\": \"local\", \"format\": \"csv\", \"path\": \"data.csv\"}\n",
    ")\n",
    "\n",
    "context = Context()\n",
    "engine = PandasEngine()\n",
    "connections = {\"local\": {\"type\": \"local\"}}\n",
    "\n",
    "# Initialize Node\n",
    "node = Node(\n",
    "    config=config,\n",
    "    context=context,\n",
    "    engine=engine,\n",
    "    connections=connections,\n",
    "    config_file=\"pipeline.yaml\"\n",
    ")\n",
    "\n",
    "print(f\"Node created: {node.config.name}\")\n",
    "print(f\"Execution steps tracked: {node._execution_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Execution Lifecycle\n",
    "\n",
    "### The Main Execute Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution flow pseudocode\n",
    "execution_flow = \"\"\"\n",
    "def execute() -> NodeResult:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result_df = None\n",
    "        \n",
    "        # Phase 1: READ\n",
    "        if config.read:\n",
    "            result_df = _execute_read()\n",
    "            track_step(\"Read from connection\")\n",
    "        \n",
    "        # Phase 2: TRANSFORM\n",
    "        if config.transform:\n",
    "            result_df = _execute_transform(result_df)\n",
    "            track_step(\"Applied N transform steps\")\n",
    "        \n",
    "        # Phase 3: VALIDATE\n",
    "        if config.validation and result_df:\n",
    "            _execute_validation(result_df)\n",
    "            track_step(\"Validation passed\")\n",
    "        \n",
    "        # Phase 4: WRITE\n",
    "        if config.write:\n",
    "            if not result_df and config.depends_on:\n",
    "                # Write-only node: get data from dependencies\n",
    "                result_df = context.get(config.depends_on[0])\n",
    "            \n",
    "            _execute_write(result_df)\n",
    "            track_step(\"Written to connection\")\n",
    "        \n",
    "        # Register in context for downstream nodes\n",
    "        if result_df:\n",
    "            context.register(config.name, result_df)\n",
    "            \n",
    "            # Optional caching\n",
    "            if config.cache:\n",
    "                _cached_result = result_df\n",
    "        \n",
    "        # Collect metadata\n",
    "        duration = time.time() - start_time\n",
    "        metadata = _collect_metadata(result_df)\n",
    "        \n",
    "        return NodeResult(success=True, duration=duration, ...)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Wrap error with rich context\n",
    "        return NodeResult(success=False, error=enriched_error)\n",
    "\"\"\"\n",
    "\n",
    "print(execution_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Read Phase\n",
    "\n",
    "### Connection Resolution & Engine Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read phase implementation\n",
    "def analyze_read_phase():\n",
    "    \"\"\"\n",
    "    Read Phase Steps:\n",
    "    1. Get connection from registry (with validation)\n",
    "    2. Delegate to engine.read() with parameters\n",
    "    3. Return DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    code = '''\n",
    "def _execute_read(self) -> Any:\n",
    "    read_config = self.config.read\n",
    "    \n",
    "    # Step 1: Resolve connection\n",
    "    connection = self.connections.get(read_config.connection)\n",
    "    \n",
    "    if connection is None:\n",
    "        raise ValueError(\n",
    "            f\"Connection '{read_config.connection}' not found. \"\n",
    "            f\"Available: {', '.join(self.connections.keys())}\"\n",
    "        )\n",
    "    \n",
    "    # Step 2: Delegate to engine (abstraction layer)\n",
    "    df = self.engine.read(\n",
    "        connection=connection,\n",
    "        format=read_config.format,      # csv, parquet, delta, sql\n",
    "        table=read_config.table,        # for database sources\n",
    "        path=read_config.path,          # for file sources\n",
    "        options=read_config.options     # engine-specific options\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "    '''\n",
    "    \n",
    "    print(code)\n",
    "    print(\"\"\"\n",
    "Key Points:\n",
    "- Connection validation with helpful error messages\n",
    "- Engine abstraction allows Spark/Pandas/Polars flexibility\n",
    "- Format-agnostic (engine handles specifics)\n",
    "- Options passed through for advanced configurations\n",
    "    \"\"\")\n",
    "\n",
    "analyze_read_phase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Transform Phase\n",
    "\n",
    "### Three Transform Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform phase architecture\n",
    "print(\"\"\"\n",
    "Transform Phase: 3 Step Types\n",
    "\n",
    "1. SQL Steps (String)\n",
    "   - Raw SQL queries\n",
    "   - Access context DataFrames as tables\n",
    "   - Example: \"SELECT * FROM customers WHERE age > 25\"\n",
    "\n",
    "2. Function Steps (TransformStep)\n",
    "   - Python functions from FunctionRegistry\n",
    "   - Parameters validated via Pydantic schemas\n",
    "   - Can accept 'current' DataFrame or work with context\n",
    "\n",
    "3. Operation Steps (TransformStep)\n",
    "   - Built-in operations (pivot, unpivot, etc.)\n",
    "   - Engine-specific implementations\n",
    "   - Parameter-driven transformations\n",
    "\"\"\")\n",
    "\n",
    "# Transform execution loop\n",
    "transform_code = '''\n",
    "def _execute_transform(self, input_df: Optional[Any]) -> Any:\n",
    "    current_df = input_df\n",
    "    \n",
    "    for step_idx, step in enumerate(self.config.transform.steps):\n",
    "        try:\n",
    "            # Track execution context for errors\n",
    "            exec_context = ExecutionContext(\n",
    "                node_name=self.config.name,\n",
    "                step_index=step_idx,\n",
    "                total_steps=len(self.config.transform.steps)\n",
    "            )\n",
    "            \n",
    "            # Route to appropriate executor\n",
    "            if isinstance(step, str):\n",
    "                # SQL step\n",
    "                current_df = self._execute_sql_step(step)\n",
    "            \n",
    "            elif step.function:\n",
    "                # Function step\n",
    "                current_df = self._execute_function_step(\n",
    "                    step.function, step.params, current_df\n",
    "                )\n",
    "            \n",
    "            elif step.operation:\n",
    "                # Operation step\n",
    "                current_df = self._execute_operation_step(\n",
    "                    step.operation, step.params, current_df\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Enrich error with schema/shape context\n",
    "            exec_context.input_schema = self._get_schema(current_df)\n",
    "            exec_context.input_shape = self._get_shape(current_df)\n",
    "            raise NodeExecutionError(context=exec_context, ...)\n",
    "    \n",
    "    return current_df\n",
    "'''\n",
    "\n",
    "print(transform_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Transform Example\n",
    "print(\"\"\"\n",
    "SQL Step Execution:\n",
    "\n",
    "def _execute_sql_step(self, sql: str) -> Any:\n",
    "    return self.engine.execute_sql(sql, self.context)\n",
    "\n",
    "- Engine translates SQL to native operations (Spark SQL / Pandas SQL)\n",
    "- Context provides named DataFrames as virtual tables\n",
    "- Example: \"SELECT * FROM {{customers}} JOIN {{orders}} ON ...\"\n",
    "\"\"\")\n",
    "\n",
    "# Function Transform Example\n",
    "print(\"\"\"\n",
    "Function Step Execution:\n",
    "\n",
    "def _execute_function_step(self, function_name, params, current_df):\n",
    "    # 1. Validate parameters against schema\n",
    "    FunctionRegistry.validate_params(function_name, params)\n",
    "    \n",
    "    # 2. Get registered function\n",
    "    func = FunctionRegistry.get(function_name)\n",
    "    \n",
    "    # 3. Inspect signature for 'current' parameter\n",
    "    sig = inspect.signature(func)\n",
    "    if \"current\" in sig.parameters:\n",
    "        # Pass current DataFrame\n",
    "        result = func(self.context, current=current_df, **params)\n",
    "    else:\n",
    "        # Context-only (reads from context directly)\n",
    "        result = func(self.context, **params)\n",
    "    \n",
    "    return result\n",
    "\n",
    "Key Innovation: Automatic 'current' injection for pipeline continuity\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Validation Phase\n",
    "\n",
    "### Data Quality Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation implementation\n",
    "validation_code = '''\n",
    "def _execute_validation(self, df: Any) -> None:\n",
    "    validation_config = self.config.validation\n",
    "    failures = []\n",
    "    \n",
    "    # Check 1: Not empty\n",
    "    if validation_config.not_empty:\n",
    "        if self._is_empty(df):\n",
    "            failures.append(\"DataFrame is empty\")\n",
    "    \n",
    "    # Check 2: No nulls in specified columns\n",
    "    if validation_config.no_nulls:\n",
    "        null_counts = self._count_nulls(df, validation_config.no_nulls)\n",
    "        for col, count in null_counts.items():\n",
    "            if count > 0:\n",
    "                failures.append(f\"Column '{col}' has {count} null values\")\n",
    "    \n",
    "    # Check 3: Schema validation\n",
    "    if validation_config.schema_validation:\n",
    "        schema_failures = self._validate_schema(\n",
    "            df, validation_config.schema_validation\n",
    "        )\n",
    "        failures.extend(schema_failures)\n",
    "    \n",
    "    # Fail fast if any validation fails\n",
    "    if failures:\n",
    "        raise ValidationError(self.config.name, failures)\n",
    "'''\n",
    "\n",
    "print(validation_code)\n",
    "print(\"\"\"\n",
    "Validation Types:\n",
    "1. not_empty: Ensures DataFrame has rows\n",
    "2. no_nulls: Checks specified columns for null values\n",
    "3. schema_validation: Type/existence checks (engine-delegated)\n",
    "\n",
    "All failures collected before raising (comprehensive error reporting)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Write Phase\n",
    "\n",
    "### Format Handling & Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write phase implementation\n",
    "write_code = '''\n",
    "def _execute_write(self, df: Any) -> None:\n",
    "    write_config = self.config.write\n",
    "    \n",
    "    # Resolve connection (with validation)\n",
    "    connection = self.connections.get(write_config.connection)\n",
    "    \n",
    "    if connection is None:\n",
    "        raise ValueError(\n",
    "            f\"Connection '{write_config.connection}' not found. \"\n",
    "            f\"Available: {', '.join(self.connections.keys())}\"\n",
    "        )\n",
    "    \n",
    "    # Delegate to engine\n",
    "    self.engine.write(\n",
    "        df=df,\n",
    "        connection=connection,\n",
    "        format=write_config.format,    # csv, parquet, delta, sql\n",
    "        table=write_config.table,      # for database targets\n",
    "        path=write_config.path,        # for file targets\n",
    "        mode=write_config.mode,        # append, overwrite, error, ignore\n",
    "        options=write_config.options   # engine-specific options\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(write_code)\n",
    "print(\"\"\"\n",
    "Write Modes:\n",
    "- append: Add to existing data\n",
    "- overwrite: Replace existing data\n",
    "- error: Fail if target exists (default)\n",
    "- ignore: Skip if target exists\n",
    "\n",
    "Special Case: Write-only nodes\n",
    "- No read/transform phases\n",
    "- Get data from context via depends_on\n",
    "- Example: Export transformed data to multiple formats\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: NodeResult & Metadata\n",
    "\n",
    "### Execution Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NodeResult model\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, List, Any\n",
    "\n",
    "class NodeResultExample(BaseModel):\n",
    "    node_name: str\n",
    "    success: bool\n",
    "    duration: float\n",
    "    rows_processed: Optional[int] = None\n",
    "    result_schema: Optional[List[str]] = None\n",
    "    error: Optional[Exception] = None\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "# Metadata collection\n",
    "metadata_code = '''\n",
    "def _collect_metadata(self, df: Optional[Any]) -> Dict[str, Any]:\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"steps\": self._execution_steps.copy(),  # Audit trail\n",
    "    }\n",
    "    \n",
    "    if df is not None:\n",
    "        metadata[\"rows\"] = self._count_rows(df)\n",
    "        metadata[\"schema\"] = self._get_schema(df)  # Column names\n",
    "    \n",
    "    return metadata\n",
    "'''\n",
    "\n",
    "print(metadata_code)\n",
    "print(\"\"\"\n",
    "NodeResult Benefits:\n",
    "1. Performance tracking (duration)\n",
    "2. Data lineage (rows_processed, schema)\n",
    "3. Execution audit trail (metadata.steps)\n",
    "4. Error context (error with rich details)\n",
    "5. Downstream decision-making (success boolean)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Error Handling & Recovery\n",
    "\n",
    "### Rich Error Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error enrichment\n",
    "error_code = '''\n",
    "except Exception as e:\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Wrap in NodeExecutionError with context\n",
    "    if not isinstance(e, NodeExecutionError):\n",
    "        exec_context = ExecutionContext(\n",
    "            node_name=self.config.name,\n",
    "            config_file=self.config_file,          # YAML file path\n",
    "            previous_steps=self._execution_steps,  # What succeeded\n",
    "        )\n",
    "        \n",
    "        error = NodeExecutionError(\n",
    "            message=str(e),\n",
    "            context=exec_context,\n",
    "            original_error=e,\n",
    "            suggestions=self._generate_suggestions(e)  # AI-like hints\n",
    "        )\n",
    "    else:\n",
    "        error = e\n",
    "    \n",
    "    return NodeResult(\n",
    "        node_name=self.config.name,\n",
    "        success=False,\n",
    "        duration=duration,\n",
    "        error=error,\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(error_code)\n",
    "\n",
    "# Suggestion generation\n",
    "suggestions_code = '''\n",
    "def _generate_suggestions(self, error: Exception) -> List[str]:\n",
    "    suggestions = []\n",
    "    error_str = str(error).lower()\n",
    "    \n",
    "    if \"column\" in error_str and \"not found\" in error_str:\n",
    "        suggestions.append(\n",
    "            \"Check that previous nodes output the expected columns\"\n",
    "        )\n",
    "        suggestions.append(\n",
    "            f\"Use 'odibi run-node {self.config.name} --show-schema' to debug\"\n",
    "        )\n",
    "    \n",
    "    if \"keyerror\" in error.__class__.__name__.lower():\n",
    "        suggestions.append(\n",
    "            \"Verify that all referenced DataFrames are registered in context\"\n",
    "        )\n",
    "        suggestions.append(\n",
    "            \"Check node dependencies in 'depends_on' list\"\n",
    "        )\n",
    "    \n",
    "    if \"function\" in error_str and \"not\" in error_str:\n",
    "        suggestions.append(\n",
    "            \"Ensure the transform function is decorated with @transform\"\n",
    "        )\n",
    "        suggestions.append(\n",
    "            \"Import the module containing the transform function\"\n",
    "        )\n",
    "    \n",
    "    return suggestions\n",
    "'''\n",
    "\n",
    "print(suggestions_code)\n",
    "print(\"\"\"\n",
    "Error Handling Philosophy:\n",
    "- Never swallow exceptions\n",
    "- Always return NodeResult (even on failure)\n",
    "- Enrich errors with context (file, node, step)\n",
    "- Provide actionable suggestions\n",
    "- Track partial success (previous_steps)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Node Execution Mental Model\n",
    "\n",
    "```\n",
    "NodeConfig (declarative)\n",
    "    ↓\n",
    "Node.__init__(config, context, engine, connections)\n",
    "    ↓\n",
    "Node.execute()\n",
    "    ↓\n",
    "┌─────────────────┐\n",
    "│  READ PHASE     │ → Load from source\n",
    "│  result_df      │\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "┌─────────────────┐\n",
    "│ TRANSFORM PHASE │ → SQL/Function/Operation\n",
    "│  result_df      │\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "┌─────────────────┐\n",
    "│ VALIDATE PHASE  │ → Quality checks\n",
    "│  (assertions)   │\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "┌─────────────────┐\n",
    "│  WRITE PHASE    │ → Persist to target\n",
    "│  (side effect)  │\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "context.register(node_name, result_df)\n",
    "    ↓\n",
    "NodeResult(success, duration, metadata, error)\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Node = Config Executor**: Bridges declarative YAML and imperative operations\n",
    "2. **4 Phases**: Read → Transform → Validate → Write (modular, composable)\n",
    "3. **Engine Abstraction**: Same config works with Spark/Pandas/Polars\n",
    "4. **Context Integration**: Nodes communicate via shared registry\n",
    "5. **Error Enrichment**: Failures include context, suggestions, audit trail\n",
    "6. **Metadata Tracking**: Performance, lineage, schema evolution\n",
    "\n",
    "### Next Steps\n",
    "- Complete exercises.ipynb for hands-on practice\n",
    "- Review node_lifecycle.md for visual flow diagrams\n",
    "- Explore Module 07: Orchestrator (how Nodes are coordinated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
