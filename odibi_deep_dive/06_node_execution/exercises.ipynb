{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Execution Exercises\n",
    "\n",
    "Practice implementing and debugging Node execution patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build a Custom Node Executor\n",
    "\n",
    "Create a simplified Node executor that handles read → transform → write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, List\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class SimpleNodeConfig:\n",
    "    name: str\n",
    "    read_path: Optional[str] = None\n",
    "    transform_sql: Optional[str] = None\n",
    "    write_path: Optional[str] = None\n",
    "\n",
    "class SimpleNode:\n",
    "    \"\"\"Simplified Node implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SimpleNodeConfig):\n",
    "        self.config = config\n",
    "        self._execution_steps = []\n",
    "    \n",
    "    def execute(self) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the node.\n",
    "        \n",
    "        TODO: Implement the 3-phase execution:\n",
    "        1. Read phase: Load CSV if read_path is set\n",
    "        2. Transform phase: Apply SQL if transform_sql is set\n",
    "        3. Write phase: Save CSV if write_path is set\n",
    "        \n",
    "        Return: {\"success\": bool, \"duration\": float, \"rows\": int, \"steps\": list}\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        result_df = None\n",
    "        \n",
    "        # TODO: Implement read phase\n",
    "        \n",
    "        # TODO: Implement transform phase\n",
    "        \n",
    "        # TODO: Implement write phase\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"duration\": duration,\n",
    "            \"rows\": len(result_df) if result_df is not None else 0,\n",
    "            \"steps\": self._execution_steps\n",
    "        }\n",
    "\n",
    "# Test your implementation\n",
    "test_data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"value\": [10, 20, 30]\n",
    "})\n",
    "test_data.to_csv(\"test_input.csv\", index=False)\n",
    "\n",
    "config = SimpleNodeConfig(\n",
    "    name=\"test_node\",\n",
    "    read_path=\"test_input.csv\",\n",
    "    write_path=\"test_output.csv\"\n",
    ")\n",
    "\n",
    "node = SimpleNode(config)\n",
    "result = node.execute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Transform Step Router\n",
    "\n",
    "Implement a router that handles different transform types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Callable\n",
    "import pandasql as ps\n",
    "\n",
    "class TransformRouter:\n",
    "    \"\"\"Routes transform steps to appropriate executors.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.functions = {}  # function_name -> callable\n",
    "    \n",
    "    def register_function(self, name: str, func: Callable):\n",
    "        \"\"\"Register a transform function.\"\"\"\n",
    "        self.functions[name] = func\n",
    "    \n",
    "    def execute_step(self, step: Union[str, Dict], current_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute a transform step.\n",
    "        \n",
    "        TODO: Implement routing logic:\n",
    "        - If step is a string: treat as SQL and execute with pandasql\n",
    "        - If step is dict with \"function\" key: call registered function\n",
    "        - If step is dict with \"operation\" key: handle built-in operation\n",
    "        \n",
    "        Args:\n",
    "            step: Transform step (SQL string or dict)\n",
    "            current_df: Current DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            Transformed DataFrame\n",
    "        \"\"\"\n",
    "        # TODO: Implement routing\n",
    "        pass\n",
    "\n",
    "# Test cases\n",
    "router = TransformRouter()\n",
    "\n",
    "# Register a custom function\n",
    "def double_values(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df[column] = df[column] * 2\n",
    "    return df\n",
    "\n",
    "router.register_function(\"double_values\", double_values)\n",
    "\n",
    "# Test data\n",
    "df = pd.DataFrame({\"id\": [1, 2, 3], \"value\": [10, 20, 30]})\n",
    "\n",
    "# Test SQL step\n",
    "result1 = router.execute_step(\"SELECT * FROM df WHERE value > 15\", df)\n",
    "print(\"SQL result:\", result1)\n",
    "\n",
    "# Test function step\n",
    "result2 = router.execute_step(\n",
    "    {\"function\": \"double_values\", \"params\": {\"column\": \"value\"}},\n",
    "    df\n",
    ")\n",
    "print(\"Function result:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Validation Engine\n",
    "\n",
    "Build a validation engine with multiple check types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    not_empty: bool = False\n",
    "    no_nulls: Optional[List[str]] = None\n",
    "    min_rows: Optional[int] = None\n",
    "    max_rows: Optional[int] = None\n",
    "    required_columns: Optional[List[str]] = None\n",
    "\n",
    "class ValidationEngine:\n",
    "    \"\"\"Validates DataFrames against rules.\"\"\"\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame, config: ValidationConfig) -> List[str]:\n",
    "        \"\"\"\n",
    "        Validate DataFrame against rules.\n",
    "        \n",
    "        TODO: Implement all validation checks:\n",
    "        1. not_empty: Check DataFrame has rows\n",
    "        2. no_nulls: Check specified columns have no nulls\n",
    "        3. min_rows: Check minimum row count\n",
    "        4. max_rows: Check maximum row count\n",
    "        5. required_columns: Check all required columns exist\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            config: Validation configuration\n",
    "        \n",
    "        Returns:\n",
    "            List of failure messages (empty if all pass)\n",
    "        \"\"\"\n",
    "        failures = []\n",
    "        \n",
    "        # TODO: Implement validation logic\n",
    "        \n",
    "        return failures\n",
    "\n",
    "# Test cases\n",
    "validator = ValidationEngine()\n",
    "\n",
    "# Test 1: Valid DataFrame\n",
    "df1 = pd.DataFrame({\"id\": [1, 2, 3], \"name\": [\"A\", \"B\", \"C\"]})\n",
    "config1 = ValidationConfig(\n",
    "    not_empty=True,\n",
    "    no_nulls=[\"id\", \"name\"],\n",
    "    min_rows=2,\n",
    "    required_columns=[\"id\", \"name\"]\n",
    ")\n",
    "failures1 = validator.validate(df1, config1)\n",
    "assert len(failures1) == 0, f\"Expected no failures, got: {failures1}\"\n",
    "print(\"✓ Test 1 passed\")\n",
    "\n",
    "# Test 2: Empty DataFrame\n",
    "df2 = pd.DataFrame()\n",
    "config2 = ValidationConfig(not_empty=True)\n",
    "failures2 = validator.validate(df2, config2)\n",
    "assert len(failures2) > 0, \"Expected failure for empty DataFrame\"\n",
    "print(\"✓ Test 2 passed\")\n",
    "\n",
    "# Test 3: Null values\n",
    "df3 = pd.DataFrame({\"id\": [1, None, 3], \"name\": [\"A\", \"B\", \"C\"]})\n",
    "config3 = ValidationConfig(no_nulls=[\"id\"])\n",
    "failures3 = validator.validate(df3, config3)\n",
    "assert len(failures3) > 0, \"Expected failure for null values\"\n",
    "print(\"✓ Test 3 passed\")\n",
    "\n",
    "# Test 4: Row count\n",
    "df4 = pd.DataFrame({\"id\": [1, 2]})\n",
    "config4 = ValidationConfig(min_rows=5)\n",
    "failures4 = validator.validate(df4, config4)\n",
    "assert len(failures4) > 0, \"Expected failure for min_rows\"\n",
    "print(\"✓ Test 4 passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Error Context Builder\n",
    "\n",
    "Create rich error messages with execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class ExecutionContext:\n",
    "    node_name: str\n",
    "    config_file: Optional[str] = None\n",
    "    step_index: Optional[int] = None\n",
    "    total_steps: Optional[int] = None\n",
    "    previous_steps: List[str] = field(default_factory=list)\n",
    "    input_schema: List[str] = field(default_factory=list)\n",
    "\n",
    "class ErrorContextBuilder:\n",
    "    \"\"\"Builds rich error messages with context.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_message(error: Exception, context: ExecutionContext) -> str:\n",
    "        \"\"\"\n",
    "        Build a comprehensive error message.\n",
    "        \n",
    "        TODO: Create a formatted error message that includes:\n",
    "        1. Node name and config file\n",
    "        2. Current step (if in transform phase)\n",
    "        3. Previous successful steps\n",
    "        4. Input schema (if available)\n",
    "        5. Original error message\n",
    "        6. Suggestions based on error type\n",
    "        \n",
    "        Args:\n",
    "            error: Original exception\n",
    "            context: Execution context\n",
    "        \n",
    "        Returns:\n",
    "            Formatted error message\n",
    "        \"\"\"\n",
    "        # TODO: Build formatted message\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_suggestions(error: Exception) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate helpful suggestions based on error.\n",
    "        \n",
    "        TODO: Analyze error and provide suggestions for:\n",
    "        - KeyError: Check context registration\n",
    "        - AttributeError: Check column names\n",
    "        - ValueError: Check data types\n",
    "        - FileNotFoundError: Check paths\n",
    "        \n",
    "        Args:\n",
    "            error: Exception\n",
    "        \n",
    "        Returns:\n",
    "            List of suggestions\n",
    "        \"\"\"\n",
    "        # TODO: Generate suggestions\n",
    "        pass\n",
    "\n",
    "# Test cases\n",
    "builder = ErrorContextBuilder()\n",
    "\n",
    "# Test 1: KeyError\n",
    "context1 = ExecutionContext(\n",
    "    node_name=\"transform_data\",\n",
    "    config_file=\"pipeline.yaml\",\n",
    "    step_index=2,\n",
    "    total_steps=5,\n",
    "    previous_steps=[\"Read from CSV\", \"Applied filter\"],\n",
    "    input_schema=[\"id\", \"name\", \"value\"]\n",
    ")\n",
    "error1 = KeyError(\"missing_column\")\n",
    "message1 = builder.build_message(error1, context1)\n",
    "print(\"Error message 1:\")\n",
    "print(message1)\n",
    "print()\n",
    "\n",
    "# Test 2: Column not found\n",
    "context2 = ExecutionContext(\n",
    "    node_name=\"aggregate_data\",\n",
    "    previous_steps=[\"Read from source\"]\n",
    ")\n",
    "error2 = ValueError(\"Column 'total' not found\")\n",
    "suggestions2 = builder.generate_suggestions(error2)\n",
    "print(\"Suggestions for column error:\")\n",
    "for s in suggestions2:\n",
    "    print(f\"  - {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Metadata Collector\n",
    "\n",
    "Build a system to collect and track execution metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "import pandas as pd\n",
    "\n",
    "class MetadataCollector:\n",
    "    \"\"\"Collects execution metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.execution_steps = []\n",
    "    \n",
    "    def track_step(self, step: str):\n",
    "        \"\"\"Track an execution step.\"\"\"\n",
    "        self.execution_steps.append(step)\n",
    "    \n",
    "    def collect(self, df: Optional[pd.DataFrame], duration: float) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Collect comprehensive metadata.\n",
    "        \n",
    "        TODO: Return metadata dictionary with:\n",
    "        1. timestamp: ISO format\n",
    "        2. duration: Execution time\n",
    "        3. steps: List of execution steps\n",
    "        4. rows: Row count (if df is not None)\n",
    "        5. columns: Column count\n",
    "        6. schema: Column names and types\n",
    "        7. memory_usage: DataFrame memory usage in MB\n",
    "        8. null_counts: Null count per column\n",
    "        \n",
    "        Args:\n",
    "            df: Result DataFrame (can be None)\n",
    "            duration: Execution duration in seconds\n",
    "        \n",
    "        Returns:\n",
    "            Metadata dictionary\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration\": duration,\n",
    "            \"steps\": self.execution_steps.copy()\n",
    "        }\n",
    "        \n",
    "        # TODO: Add DataFrame metadata if available\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "# Test\n",
    "collector = MetadataCollector()\n",
    "collector.track_step(\"Read from CSV\")\n",
    "collector.track_step(\"Applied filter: value > 10\")\n",
    "collector.track_step(\"Grouped by category\")\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"value\": [10, None, 30, 40, 50],\n",
    "    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"]\n",
    "})\n",
    "\n",
    "metadata = collector.collect(test_df, duration=0.125)\n",
    "print(\"Collected metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Node Execution Debugger\n",
    "\n",
    "Create a debugging tool that inspects node execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeDebugger:\n",
    "    \"\"\"Debug tool for node execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "    \n",
    "    def show_config(self):\n",
    "        \"\"\"Display node configuration.\"\"\"\n",
    "        print(f\"Node: {self.node.config.name}\")\n",
    "        print(f\"Read: {self.node.config.read}\")\n",
    "        print(f\"Transform: {self.node.config.transform}\")\n",
    "        print(f\"Validate: {self.node.config.validation}\")\n",
    "        print(f\"Write: {self.node.config.write}\")\n",
    "    \n",
    "    def execute_with_trace(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute node with detailed tracing.\n",
    "        \n",
    "        TODO: Implement execution with phase-by-phase output:\n",
    "        1. Print \"Starting READ phase\" and show result shape\n",
    "        2. Print \"Starting TRANSFORM phase\" and show each step\n",
    "        3. Print \"Starting VALIDATE phase\" and show checks\n",
    "        4. Print \"Starting WRITE phase\" and confirm success\n",
    "        5. Return detailed trace with timing for each phase\n",
    "        \n",
    "        Returns:\n",
    "            Trace dictionary with phase timings and results\n",
    "        \"\"\"\n",
    "        trace = {}\n",
    "        \n",
    "        # TODO: Implement traced execution\n",
    "        \n",
    "        return trace\n",
    "    \n",
    "    def inspect_dataframe(self, df: pd.DataFrame, label: str = \"DataFrame\"):\n",
    "        \"\"\"Display DataFrame inspection.\"\"\"\n",
    "        print(f\"\\n{label} Inspection:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print(f\"  Dtypes:\\n{df.dtypes}\")\n",
    "        print(f\"  Null counts:\\n{df.isnull().sum()}\")\n",
    "        print(f\"  Sample (first 3 rows):\\n{df.head(3)}\")\n",
    "\n",
    "# Test with a mock node\n",
    "# (Create test implementation based on Exercise 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Build a Mini Node Orchestrator\n",
    "\n",
    "Create a simple orchestrator that executes multiple nodes in dependency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class MiniOrchestrator:\n",
    "    \"\"\"Simple node orchestrator.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_node(self, node, depends_on: List[str] = None):\n",
    "        \"\"\"Add a node with dependencies.\"\"\"\n",
    "        self.nodes[node.config.name] = {\n",
    "            \"node\": node,\n",
    "            \"depends_on\": depends_on or []\n",
    "        }\n",
    "    \n",
    "    def execute_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute all nodes in dependency order.\n",
    "        \n",
    "        TODO: Implement topological sort execution:\n",
    "        1. Build dependency graph\n",
    "        2. Sort nodes topologically\n",
    "        3. Execute each node in order\n",
    "        4. Track results and handle failures\n",
    "        5. Return execution summary\n",
    "        \n",
    "        Returns:\n",
    "            Summary with node results and total duration\n",
    "        \"\"\"\n",
    "        # TODO: Implement orchestration\n",
    "        pass\n",
    "\n",
    "# Test: Create a 3-node pipeline\n",
    "# node_a: Read data\n",
    "# node_b: Transform (depends on node_a)\n",
    "# node_c: Write (depends on node_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
