{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection Layer Deep Dive\n",
    "\n",
    "## üéØ The Problem\n",
    "\n",
    "Your data pipeline needs to run:\n",
    "- Locally during development (filesystem)\n",
    "- In Azure for production (Data Lake)\n",
    "- On Databricks (DBFS)\n",
    "- Maybe S3 or GCS in the future\n",
    "\n",
    "**Without connection abstraction**:\n",
    "```python\n",
    "# Nightmare code - don't do this!\n",
    "if env == \"local\":\n",
    "    path = f\"./data/{table}.parquet\"\n",
    "    df = pd.read_parquet(path)\n",
    "elif env == \"azure\":\n",
    "    path = f\"abfss://{container}@{account}.dfs.core.windows.net/{table}.parquet\"\n",
    "    storage_options = {\"account_key\": get_key_from_vault()}\n",
    "    df = pd.read_parquet(path, storage_options=storage_options)\n",
    "elif env == \"databricks\":\n",
    "    path = f\"dbfs:/FileStore/{table}.parquet\"\n",
    "    df = spark.read.parquet(path).toPandas()\n",
    "```\n",
    "\n",
    "**With connection abstraction**:\n",
    "```python\n",
    "path = connection.get_path(f\"{table}.parquet\")\n",
    "df = pd.read_parquet(path, storage_options=connection.storage_options())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶â First Principles: The Contract\n",
    "\n",
    "All connections must implement:\n",
    "1. **`get_path(relative_path)`** - Convert relative to absolute/URI\n",
    "2. **`validate()`** - Check configuration early\n",
    "\n",
    "Optional but useful:\n",
    "3. **`storage_options()`** - Credentials for pandas/fsspec\n",
    "4. **`configure_spark(spark)`** - Set up Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Read Odibi: BaseConnection\n",
    "\n",
    "The foundation - an Abstract Base Class (ABC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseConnection(ABC):\n",
    "    \"\"\"Abstract base class for connections.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Get full path for a relative path.\n",
    "\n",
    "        Args:\n",
    "            relative_path: Relative path or table name\n",
    "\n",
    "        Returns:\n",
    "            Full path to resource\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate connection configuration.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If validation fails\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ABC?\n",
    "\n",
    "- **Enforces interface**: You can't create a connection without implementing these methods\n",
    "- **Type safety**: All connections can be typed as `BaseConnection`\n",
    "- **Extensibility**: Easy to add S3, GCS, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Read Odibi: LocalConnection\n",
    "\n",
    "Simplest implementation - just prefix paths with a base directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class LocalConnection(BaseConnection):\n",
    "    \"\"\"Connection to local filesystem.\"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str = \"./data\"):\n",
    "        self.base_path = Path(base_path)\n",
    "\n",
    "    def get_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Get full path for a relative path.\"\"\"\n",
    "        full_path = self.base_path / relative_path\n",
    "        return str(full_path.absolute())\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate that base path exists or can be created.\"\"\"\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LocalConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = LocalConnection(base_path=\"./my_data\")\n",
    "conn.validate()\n",
    "\n",
    "print(conn.get_path(\"raw/sales.parquet\"))\n",
    "print(conn.get_path(\"processed/sales_clean.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Read Odibi: AzureADLS\n",
    "\n",
    "Production-ready Azure Data Lake connection with multiple features:\n",
    "\n",
    "### Key Features\n",
    "1. **Two auth modes**: Key Vault (production) or direct key (development)\n",
    "2. **Path prefix**: Namespace within container\n",
    "3. **Storage options**: For pandas/fsspec\n",
    "4. **Spark configuration**: Auto-configure Spark sessions\n",
    "5. **Validation**: Fail fast with clear errors\n",
    "6. **Key caching**: Avoid repeated Key Vault calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import posixpath\n",
    "import warnings\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class AzureADLS(BaseConnection):\n",
    "    \"\"\"Azure Data Lake Storage Gen2 connection.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        account: str,\n",
    "        container: str,\n",
    "        path_prefix: str = \"\",\n",
    "        auth_mode: str = \"key_vault\",\n",
    "        key_vault_name: Optional[str] = None,\n",
    "        secret_name: Optional[str] = None,\n",
    "        account_key: Optional[str] = None,\n",
    "        validate: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.account = account\n",
    "        self.container = container\n",
    "        self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n",
    "        self.auth_mode = auth_mode\n",
    "        self.key_vault_name = key_vault_name\n",
    "        self.secret_name = secret_name\n",
    "        self.account_key = account_key\n",
    "        self._cached_key: Optional[str] = None\n",
    "\n",
    "        if validate:\n",
    "            self.validate()\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate ADLS connection configuration.\"\"\"\n",
    "        if not self.account:\n",
    "            raise ValueError(\"ADLS connection requires 'account'\")\n",
    "        if not self.container:\n",
    "            raise ValueError(\"ADLS connection requires 'container'\")\n",
    "\n",
    "        if self.auth_mode == \"key_vault\":\n",
    "            if not self.key_vault_name or not self.secret_name:\n",
    "                raise ValueError(\n",
    "                    f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n",
    "                    f\"for connection to {self.account}/{self.container}\"\n",
    "                )\n",
    "        elif self.auth_mode == \"direct_key\":\n",
    "            if not self.account_key:\n",
    "                raise ValueError(\n",
    "                    f\"direct_key mode requires 'account_key' \"\n",
    "                    f\"for connection to {self.account}/{self.container}\"\n",
    "                )\n",
    "            if os.getenv(\"ODIBI_ENV\") == \"production\":\n",
    "                warnings.warn(\n",
    "                    f\"‚ö†Ô∏è  Using direct_key in production is not recommended. \"\n",
    "                    f\"Use auth_mode: key_vault.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "\n",
    "    def get_storage_key(self, timeout: float = 30.0) -> str:\n",
    "        \"\"\"Get storage account key (cached).\"\"\"\n",
    "        if self._cached_key:\n",
    "            return self._cached_key\n",
    "\n",
    "        if self.auth_mode == \"key_vault\":\n",
    "            from azure.identity import DefaultAzureCredential\n",
    "            from azure.keyvault.secrets import SecretClient\n",
    "            import concurrent.futures\n",
    "\n",
    "            credential = DefaultAzureCredential()\n",
    "            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n",
    "            client = SecretClient(vault_url=kv_uri, credential=credential)\n",
    "\n",
    "            def _fetch():\n",
    "                secret = client.get_secret(self.secret_name)\n",
    "                return secret.value\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                future = executor.submit(_fetch)\n",
    "                try:\n",
    "                    self._cached_key = future.result(timeout=timeout)\n",
    "                    return self._cached_key\n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    raise TimeoutError(\n",
    "                        f\"Key Vault fetch timed out after {timeout}s\"\n",
    "                    )\n",
    "\n",
    "        elif self.auth_mode == \"direct_key\":\n",
    "            return self.account_key\n",
    "\n",
    "    def pandas_storage_options(self) -> dict:\n",
    "        \"\"\"Get storage options for pandas/fsspec.\"\"\"\n",
    "        return {\"account_name\": self.account, \"account_key\": self.get_storage_key()}\n",
    "\n",
    "    def configure_spark(self, spark) -> None:\n",
    "        \"\"\"Configure Spark session with storage account key.\"\"\"\n",
    "        config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n",
    "        spark.conf.set(config_key, self.get_storage_key())\n",
    "\n",
    "    def uri(self, path: str) -> str:\n",
    "        \"\"\"Build abfss:// URI for given path.\"\"\"\n",
    "        if self.path_prefix:\n",
    "            full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n",
    "        else:\n",
    "            full_path = path.lstrip(\"/\")\n",
    "\n",
    "        return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n",
    "\n",
    "    def get_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Get full abfss:// URI for relative path.\"\"\"\n",
    "        return self.uri(relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AzureADLS (Development Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzureADLS(\n",
    "    account=\"mystorageaccount\",\n",
    "    container=\"data\",\n",
    "    path_prefix=\"project_x/v2\",\n",
    "    auth_mode=\"direct_key\",\n",
    "    account_key=\"fake_key_for_demo\"\n",
    ")\n",
    "\n",
    "print(conn.get_path(\"raw/sales.parquet\"))\n",
    "print(conn.get_path(\"processed/sales_clean.parquet\"))\n",
    "print()\n",
    "print(\"Storage options:\", conn.pandas_storage_options())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = conn.get_path(\"raw/sales.parquet\")\n",
    "storage_options = conn.pandas_storage_options()\n",
    "\n",
    "# df = pd.read_parquet(path, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Read Odibi: LocalDBFS\n",
    "\n",
    "Mock Databricks filesystem for local development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "class LocalDBFS(BaseConnection):\n",
    "    \"\"\"Mock DBFS connection for local development.\n",
    "    \n",
    "    Maps dbfs:/ paths to local filesystem for testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: Union[str, Path] = \".dbfs\"):\n",
    "        self.root = Path(root).resolve()\n",
    "\n",
    "    def resolve(self, path: str) -> str:\n",
    "        \"\"\"Resolve dbfs:/ path to local filesystem path.\"\"\"\n",
    "        clean_path = path.replace(\"dbfs:/\", \"\").lstrip(\"/\")\n",
    "        local_path = self.root / clean_path\n",
    "        return str(local_path)\n",
    "\n",
    "    def ensure_dir(self, path: str) -> None:\n",
    "        \"\"\"Create parent directories for given path.\"\"\"\n",
    "        local_path = Path(self.resolve(path))\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Get local filesystem path for DBFS path.\"\"\"\n",
    "        return self.resolve(relative_path)\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate local DBFS configuration.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LocalDBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = LocalDBFS(root=\"./mock_dbfs\")\n",
    "\n",
    "print(conn.get_path(\"dbfs:/FileStore/raw/sales.parquet\"))\n",
    "print(conn.get_path(\"dbfs:/mnt/data/processed/sales.parquet\"))\n",
    "print(conn.get_path(\"FileStore/tables/customers.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Path Resolution Strategies\n",
    "\n",
    "### LocalConnection: Base Path Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = LocalConnection(base_path=\"/data/project\")\n",
    "print(local.get_path(\"raw/sales.parquet\"))\n",
    "# /data/project/raw/sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureADLS: URI Construction with Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure = AzureADLS(\n",
    "    account=\"myaccount\",\n",
    "    container=\"datalake\",\n",
    "    path_prefix=\"team/project\",\n",
    "    auth_mode=\"direct_key\",\n",
    "    account_key=\"key\"\n",
    ")\n",
    "print(azure.get_path(\"raw/sales.parquet\"))\n",
    "# abfss://datalake@myaccount.dfs.core.windows.net/team/project/raw/sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalDBFS: Protocol Stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfs = LocalDBFS(root=\"/tmp/dbfs\")\n",
    "print(dbfs.get_path(\"dbfs:/FileStore/raw/sales.parquet\"))\n",
    "# /tmp/dbfs/FileStore/raw/sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Polymorphism in Action\n",
    "\n",
    "The power: write code once, swap connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sales_data(connection: BaseConnection) -> pd.DataFrame:\n",
    "    \"\"\"Read sales data using any connection.\"\"\"\n",
    "    path = connection.get_path(\"raw/sales.parquet\")\n",
    "    \n",
    "    # Get storage options if available\n",
    "    storage_options = {}\n",
    "    if hasattr(connection, 'pandas_storage_options'):\n",
    "        storage_options = connection.pandas_storage_options()\n",
    "    \n",
    "    return pd.read_parquet(path, storage_options=storage_options)\n",
    "\n",
    "# Works with any connection!\n",
    "# df = read_sales_data(LocalConnection())\n",
    "# df = read_sales_data(AzureADLS(...))\n",
    "# df = read_sales_data(LocalDBFS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Connection Validation\n",
    "\n",
    "Validation catches configuration errors early:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bad_conn = AzureADLS(\n",
    "        account=\"myaccount\",\n",
    "        container=\"data\",\n",
    "        auth_mode=\"key_vault\"\n",
    "        # Missing: key_vault_name and secret_name!\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Validation caught error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bad_conn = AzureADLS(\n",
    "        account=\"\",  # Empty!\n",
    "        container=\"data\",\n",
    "        auth_mode=\"direct_key\",\n",
    "        account_key=\"key\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Validation caught error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "1. **BaseConnection** defines the contract all connections must follow\n",
    "2. **LocalConnection** is simple: base_path + relative_path\n",
    "3. **AzureADLS** is complex: authentication, URI building, Spark config\n",
    "4. **LocalDBFS** enables local testing of Databricks code\n",
    "5. **Validation** catches errors at configuration time, not runtime\n",
    "6. **Polymorphism** lets you write storage-agnostic code\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Try the exercises to build S3 and GCS connections!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
