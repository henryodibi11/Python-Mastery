{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Orchestration: The Complete Symphony\n",
    "\n",
    "The Pipeline is where **everything comes together** - Config, Graph, Context, Engine, and Node working in harmony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Pipeline Architecture\n",
    "\n",
    "### The Pipeline's Responsibilities\n",
    "\n",
    "1. **Initialization**: Set up engine, context, graph, and story generator\n",
    "2. **Orchestration**: Execute nodes in dependency order\n",
    "3. **Tracking**: Monitor success, failures, and skipped nodes\n",
    "4. **Recovery**: Handle failures gracefully\n",
    "5. **Documentation**: Generate execution stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline initialization breakdown\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Key components initialized by Pipeline:\n",
    "components = {\n",
    "    'config': 'PipelineConfig - defines what to run',\n",
    "    'engine': 'PandasEngine/SparkEngine - how to process data',\n",
    "    'context': 'ExecutionContext - shares data between nodes',\n",
    "    'graph': 'DependencyGraph - determines execution order',\n",
    "    'story_generator': 'StoryGenerator - creates documentation',\n",
    "    'connections': 'Dict of connection objects - I/O handlers'\n",
    "}\n",
    "\n",
    "for name, purpose in components.items():\n",
    "    print(f\"{name:20} -> {purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Pipeline initialization (from source)\n",
    "'''\n",
    "class Pipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline_config: PipelineConfig,\n",
    "        engine: str = \"pandas\",\n",
    "        connections: Optional[Dict[str, Any]] = None,\n",
    "        generate_story: bool = True,\n",
    "        story_config: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        self.config = pipeline_config\n",
    "        self.engine_type = engine\n",
    "        self.connections = connections or {}\n",
    "        self.generate_story = generate_story\n",
    "        \n",
    "        # Initialize story generator\n",
    "        self.story_generator = StoryGenerator(...)\n",
    "        \n",
    "        # Initialize engine\n",
    "        if engine == \"pandas\":\n",
    "            self.engine = PandasEngine()\n",
    "        \n",
    "        # Initialize context\n",
    "        self.context = create_context(engine)\n",
    "        \n",
    "        # Build dependency graph\n",
    "        self.graph = DependencyGraph(pipeline_config.nodes)\n",
    "'''\n",
    "print(\"Pipeline initialization creates a complete execution environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: PipelineResults - Tracking Execution\n",
    "\n",
    "The `PipelineResults` dataclass captures **everything** about a pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PipelineResults structure\n",
    "@dataclass\n",
    "class PipelineResults:\n",
    "    pipeline_name: str\n",
    "    completed: list = field(default_factory=list)      # Successfully executed nodes\n",
    "    failed: list = field(default_factory=list)         # Nodes that failed\n",
    "    skipped: list = field(default_factory=list)        # Nodes skipped due to failures\n",
    "    node_results: Dict = field(default_factory=dict)   # Detailed results per node\n",
    "    duration: float = 0.0                              # Total execution time\n",
    "    start_time: Optional[str] = None                   # ISO timestamp\n",
    "    end_time: Optional[str] = None                     # ISO timestamp\n",
    "    story_path: Optional[str] = None                   # Path to generated story\n",
    "\n",
    "# Example results\n",
    "example_results = PipelineResults(\n",
    "    pipeline_name=\"bronze_to_silver\",\n",
    "    completed=[\"raw_customers\", \"clean_customers\"],\n",
    "    failed=[\"raw_orders\"],\n",
    "    skipped=[\"clean_orders\", \"customer_orders\"],\n",
    "    duration=12.5,\n",
    "    start_time=\"2025-01-15T10:30:00\",\n",
    "    end_time=\"2025-01-15T10:30:12\"\n",
    ")\n",
    "\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"  ‚úÖ Completed: {len(example_results.completed)}\")\n",
    "print(f\"  ‚ùå Failed: {len(example_results.failed)}\")\n",
    "print(f\"  ‚è≠Ô∏è  Skipped: {len(example_results.skipped)}\")\n",
    "print(f\"  ‚è±Ô∏è  Duration: {example_results.duration}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Pipeline Execution Flow\n",
    "\n",
    "The `run()` method orchestrates the entire execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution flow breakdown\n",
    "execution_steps = [\n",
    "    \"1. Start timer and create PipelineResults\",\n",
    "    \"2. Get execution order from graph.topological_sort()\",\n",
    "    \"3. For each node in order:\",\n",
    "    \"   a. Check if dependencies failed -> skip if yes\",\n",
    "    \"   b. Create Node instance with config, context, engine\",\n",
    "    \"   c. Execute node\",\n",
    "    \"   d. Store result in PipelineResults\",\n",
    "    \"   e. Mark as completed or failed\",\n",
    "    \"4. Calculate total duration\",\n",
    "    \"5. Generate story (if enabled)\",\n",
    "    \"6. Return PipelineResults\"\n",
    "]\n",
    "\n",
    "for step in execution_steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified run() logic\n",
    "'''\n",
    "def run(self, parallel: bool = False) -> PipelineResults:\n",
    "    start_time = time.time()\n",
    "    results = PipelineResults(pipeline_name=self.config.pipeline)\n",
    "    \n",
    "    # Get execution order from dependency graph\n",
    "    execution_order = self.graph.topological_sort()\n",
    "    \n",
    "    # Execute nodes in order\n",
    "    for node_name in execution_order:\n",
    "        node_config = self.graph.nodes[node_name]\n",
    "        \n",
    "        # Skip if dependencies failed\n",
    "        deps_failed = any(dep in results.failed for dep in node_config.depends_on)\n",
    "        if deps_failed:\n",
    "            results.skipped.append(node_name)\n",
    "            continue\n",
    "        \n",
    "        # Execute node\n",
    "        node = Node(\n",
    "            config=node_config,\n",
    "            context=self.context,\n",
    "            engine=self.engine,\n",
    "            connections=self.connections\n",
    "        )\n",
    "        \n",
    "        node_result = node.execute()\n",
    "        results.node_results[node_name] = node_result\n",
    "        \n",
    "        if node_result.success:\n",
    "            results.completed.append(node_name)\n",
    "        else:\n",
    "            results.failed.append(node_name)\n",
    "    \n",
    "    results.duration = time.time() - start_time\n",
    "    \n",
    "    # Generate story\n",
    "    if self.generate_story:\n",
    "        story_path = self.story_generator.generate(...)\n",
    "        results.story_path = story_path\n",
    "    \n",
    "    return results\n",
    "'''\n",
    "print(\"Pipeline.run() coordinates all components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Failure Propagation\n",
    "\n",
    "When a node fails, downstream dependencies are automatically skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Failure propagation\n",
    "class FailurePropagationDemo:\n",
    "    def __init__(self):\n",
    "        self.failed = []\n",
    "        self.skipped = []\n",
    "        self.completed = []\n",
    "    \n",
    "    def execute_node(self, node_name, depends_on, will_fail=False):\n",
    "        # Check if dependencies failed\n",
    "        deps_failed = any(dep in self.failed for dep in depends_on)\n",
    "        \n",
    "        if deps_failed:\n",
    "            self.skipped.append(node_name)\n",
    "            print(f\"‚è≠Ô∏è  SKIPPED: {node_name} (dependency failed)\")\n",
    "            return\n",
    "        \n",
    "        if will_fail:\n",
    "            self.failed.append(node_name)\n",
    "            print(f\"‚ùå FAILED: {node_name}\")\n",
    "        else:\n",
    "            self.completed.append(node_name)\n",
    "            print(f\"‚úÖ COMPLETED: {node_name}\")\n",
    "\n",
    "# Simulate pipeline execution\n",
    "demo = FailurePropagationDemo()\n",
    "\n",
    "print(\"\\nSimulating Pipeline Execution:\\n\")\n",
    "demo.execute_node(\"raw_customers\", [])\n",
    "demo.execute_node(\"raw_orders\", [], will_fail=True)  # This fails!\n",
    "demo.execute_node(\"clean_customers\", [\"raw_customers\"])\n",
    "demo.execute_node(\"clean_orders\", [\"raw_orders\"])  # Skipped due to failure\n",
    "demo.execute_node(\"customer_orders\", [\"clean_customers\", \"clean_orders\"])  # Skipped\n",
    "\n",
    "print(f\"\\nFinal Status:\")\n",
    "print(f\"  Completed: {demo.completed}\")\n",
    "print(f\"  Failed: {demo.failed}\")\n",
    "print(f\"  Skipped: {demo.skipped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Layer-Based Execution\n",
    "\n",
    "The graph groups nodes into **layers** for potential parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution layers example\n",
    "layers = [\n",
    "    [\"raw_customers\", \"raw_orders\", \"raw_products\"],  # Layer 0: No dependencies\n",
    "    [\"clean_customers\", \"clean_orders\"],               # Layer 1: Depend on Layer 0\n",
    "    [\"customer_orders\"],                               # Layer 2: Depends on Layer 1\n",
    "    [\"customer_analytics\"]                             # Layer 3: Depends on Layer 2\n",
    "]\n",
    "\n",
    "print(\"Execution Layers:\\n\")\n",
    "for i, layer in enumerate(layers):\n",
    "    print(f\"Layer {i}: {layer}\")\n",
    "    print(f\"  ‚Üí Can execute in parallel: {len(layer) > 1}\\n\")\n",
    "\n",
    "print(\"Note: Current implementation is sequential,\")\n",
    "print(\"but layers enable future parallel execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: PipelineManager - Multi-Pipeline Orchestration\n",
    "\n",
    "The `PipelineManager` manages multiple pipelines from a single YAML configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PipelineManager responsibilities\n",
    "manager_features = {\n",
    "    'Load from YAML': 'Parse entire project configuration',\n",
    "    'Build Connections': 'Instantiate all connection objects',\n",
    "    'Create Pipelines': 'Initialize all pipeline instances',\n",
    "    'Run Selector': 'Run all, one, or multiple pipelines',\n",
    "    'Story Config': 'Configure story generation globally',\n",
    "    'Pipeline Access': 'Get specific pipeline instances'\n",
    "}\n",
    "\n",
    "print(\"PipelineManager Features:\\n\")\n",
    "for feature, description in manager_features.items():\n",
    "    print(f\"{feature:20} -> {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PipelineManager usage patterns\n",
    "'''\n",
    "# Pattern 1: Run all pipelines\n",
    "manager = PipelineManager.from_yaml(\"config.yaml\")\n",
    "results = manager.run()  # Dict[str, PipelineResults]\n",
    "\n",
    "# Pattern 2: Run single pipeline\n",
    "result = manager.run('bronze_to_silver')  # Returns single PipelineResults\n",
    "\n",
    "# Pattern 3: Run multiple specific pipelines\n",
    "results = manager.run(['bronze_to_silver', 'silver_to_gold'])\n",
    "\n",
    "# Pattern 4: Get pipeline instance for inspection\n",
    "pipeline = manager.get_pipeline('bronze_to_silver')\n",
    "validation = pipeline.validate()\n",
    "layers = pipeline.get_execution_layers()\n",
    "'''\n",
    "print(\"PipelineManager supports flexible execution patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Connection Management\n",
    "\n",
    "PipelineManager builds all connections from the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection building logic\n",
    "'''\n",
    "@staticmethod\n",
    "def _build_connections(conn_configs: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    connections = {}\n",
    "    \n",
    "    for conn_name, conn_config in conn_configs.items():\n",
    "        conn_type = conn_config.get(\"type\", \"local\")\n",
    "        \n",
    "        if conn_type == \"local\":\n",
    "            connections[conn_name] = LocalConnection(\n",
    "                base_path=conn_config.get(\"base_path\", \"./data\")\n",
    "            )\n",
    "        elif conn_type == \"azure_adls\":\n",
    "            connections[conn_name] = AzureADLS(\n",
    "                account=conn_config[\"account\"],\n",
    "                container=conn_config[\"container\"],\n",
    "                ...\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported connection type: {conn_type}\")\n",
    "    \n",
    "    return connections\n",
    "'''\n",
    "\n",
    "# Example connection config\n",
    "example_connections = {\n",
    "    'local_data': {'type': 'local', 'base_path': './data'},\n",
    "    'azure_storage': {\n",
    "        'type': 'azure_adls',\n",
    "        'account': 'mystorageaccount',\n",
    "        'container': 'data',\n",
    "        'auth_mode': 'key_vault'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Connection types supported:\")\n",
    "for name, config in example_connections.items():\n",
    "    print(f\"  {name}: {config['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Story Generation Integration\n",
    "\n",
    "After execution, the Pipeline automatically generates documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story generation in Pipeline\n",
    "'''\n",
    "# After all nodes execute:\n",
    "if self.generate_story:\n",
    "    story_path = self.story_generator.generate(\n",
    "        node_results=results.node_results,\n",
    "        completed=results.completed,\n",
    "        failed=results.failed,\n",
    "        skipped=results.skipped,\n",
    "        duration=results.duration,\n",
    "        start_time=results.start_time,\n",
    "        end_time=results.end_time,\n",
    "        context=self.context,\n",
    "    )\n",
    "    results.story_path = story_path\n",
    "'''\n",
    "\n",
    "print(\"Story generation happens automatically after pipeline execution\")\n",
    "print(\"\\nStory includes:\")\n",
    "story_contents = [\n",
    "    \"- Pipeline summary\",\n",
    "    \"- Execution timeline\",\n",
    "    \"- Node-by-node details\",\n",
    "    \"- Data samples\",\n",
    "    \"- Transformation SQL\",\n",
    "    \"- Success/failure status\"\n",
    "]\n",
    "for item in story_contents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Pipeline Validation\n",
    "\n",
    "Validate a pipeline **before** execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation logic\n",
    "'''\n",
    "def validate(self) -> Dict[str, Any]:\n",
    "    validation = {\n",
    "        \"valid\": True,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"node_count\": len(self.graph.nodes),\n",
    "        \"execution_order\": [],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Test topological sort (checks for cycles)\n",
    "        execution_order = self.graph.topological_sort()\n",
    "        validation[\"execution_order\"] = execution_order\n",
    "    except DependencyError as e:\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(str(e))\n",
    "    \n",
    "    # Check for missing connections\n",
    "    for node in self.config.nodes:\n",
    "        if node.read and node.read.connection not in self.connections:\n",
    "            validation[\"warnings\"].append(\n",
    "                f\"Node '{node.name}': connection '{node.read.connection}' not configured\"\n",
    "            )\n",
    "    \n",
    "    return validation\n",
    "'''\n",
    "\n",
    "# Example validation result\n",
    "validation_result = {\n",
    "    \"valid\": True,\n",
    "    \"errors\": [],\n",
    "    \"warnings\": [\"Node 'raw_orders': connection 'azure_storage' not configured\"],\n",
    "    \"node_count\": 5,\n",
    "    \"execution_order\": [\"raw_customers\", \"raw_orders\", \"clean_customers\", \"clean_orders\", \"customer_orders\"]\n",
    "}\n",
    "\n",
    "print(\"Validation Result:\")\n",
    "print(f\"  Valid: {validation_result['valid']}\")\n",
    "print(f\"  Nodes: {validation_result['node_count']}\")\n",
    "print(f\"  Warnings: {len(validation_result['warnings'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Complete Execution Example\n",
    "\n",
    "Putting it all together: from YAML to results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow\n",
    "'''\n",
    "# Step 1: Load configuration\n",
    "manager = PipelineManager.from_yaml(\"project_config.yaml\")\n",
    "\n",
    "# Step 2: List available pipelines\n",
    "pipelines = manager.list_pipelines()\n",
    "print(f\"Available pipelines: {pipelines}\")\n",
    "\n",
    "# Step 3: Validate before running\n",
    "pipeline = manager.get_pipeline('bronze_to_silver')\n",
    "validation = pipeline.validate()\n",
    "if not validation['valid']:\n",
    "    print(f\"Validation errors: {validation['errors']}\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 4: Inspect execution plan\n",
    "layers = pipeline.get_execution_layers()\n",
    "print(f\"Execution will happen in {len(layers)} layers\")\n",
    "\n",
    "# Step 5: Execute pipeline\n",
    "results = manager.run('bronze_to_silver')\n",
    "\n",
    "# Step 6: Check results\n",
    "if results.failed:\n",
    "    print(f\"‚ùå Pipeline failed. Failed nodes: {results.failed}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Pipeline succeeded in {results.duration:.2f}s\")\n",
    "    print(f\"üìñ Story: {results.story_path}\")\n",
    "\n",
    "# Step 7: Inspect individual node results\n",
    "for node_name in results.completed:\n",
    "    node_result = results.get_node_result(node_name)\n",
    "    print(f\"  {node_name}: {node_result.rows_affected} rows\")\n",
    "'''\n",
    "print(\"Complete pipeline execution workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Complete Picture\n",
    "\n",
    "### Pipeline Responsibilities\n",
    "1. ‚úÖ Initialize all components (engine, context, graph, story)\n",
    "2. ‚úÖ Execute nodes in dependency order\n",
    "3. ‚úÖ Handle failures gracefully\n",
    "4. ‚úÖ Track detailed results\n",
    "5. ‚úÖ Generate documentation\n",
    "\n",
    "### PipelineManager Responsibilities\n",
    "1. ‚úÖ Load YAML configuration\n",
    "2. ‚úÖ Build connections\n",
    "3. ‚úÖ Create multiple pipeline instances\n",
    "4. ‚úÖ Orchestrate multi-pipeline execution\n",
    "5. ‚úÖ Provide pipeline access and inspection\n",
    "\n",
    "### Key Patterns\n",
    "- **Dependency Injection**: All components passed to constructors\n",
    "- **Single Responsibility**: Each class has one clear purpose\n",
    "- **Fail Fast**: Validation before execution\n",
    "- **Graceful Degradation**: Partial completion on failure\n",
    "- **Comprehensive Tracking**: Detailed results and timing\n",
    "\n",
    "### Integration Flow\n",
    "```\n",
    "YAML Config\n",
    "    ‚Üì\n",
    "PipelineManager.from_yaml()\n",
    "    ‚Üì\n",
    "Parse ProjectConfig\n",
    "    ‚Üì\n",
    "Build Connections\n",
    "    ‚Üì\n",
    "Create Pipeline instances\n",
    "    ‚Üì\n",
    "Pipeline.run()\n",
    "    ‚Üì\n",
    "Graph.topological_sort() ‚Üí execution order\n",
    "    ‚Üì\n",
    "For each node:\n",
    "  - Check dependencies\n",
    "  - Create Node instance\n",
    "  - Execute with Engine\n",
    "  - Store in Context\n",
    "  - Track in Results\n",
    "    ‚Üì\n",
    "Generate Story\n",
    "    ‚Üì\n",
    "Return PipelineResults\n",
    "```\n",
    "\n",
    "**You now understand the complete Odibi execution lifecycle!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
