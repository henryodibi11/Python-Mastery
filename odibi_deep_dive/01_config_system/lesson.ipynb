{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive 01: Odibi Configuration System\n",
    "\n",
    "## üéØ The Problem\n",
    "\n",
    "Data pipelines need configuration:\n",
    "- Where to read data from?\n",
    "- What transformations to apply?\n",
    "- Where to write results?\n",
    "- How nodes depend on each other?\n",
    "\n",
    "**Without type-safe config:**\n",
    "```python\n",
    "# ‚ùå Errors caught at runtime (too late!)\n",
    "config = yaml.safe_load(open('pipeline.yaml'))\n",
    "connection = config['conections']  # Typo - fails later\n",
    "mode = config.get('mode', 'overrite')  # Wrong default - silent corruption\n",
    "```\n",
    "\n",
    "**With Pydantic:**\n",
    "```python\n",
    "# ‚úÖ Errors caught immediately on load\n",
    "config = ProjectConfig.from_yaml('project.yaml')\n",
    "# Typos, missing fields, wrong types = instant, clear errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶â First Principles\n",
    "\n",
    "### 1. Validate Early, Fail Fast\n",
    "Catch config errors **before** executing any pipeline logic.\n",
    "\n",
    "### 2. Type Safety\n",
    "Use Python's type system + Pydantic to enforce correctness.\n",
    "\n",
    "### 3. Clear Error Messages\n",
    "Users should know *exactly* what's wrong and where.\n",
    "\n",
    "### 4. Composability\n",
    "Build complex configs from simple, reusable pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Read Actual Odibi Code\n",
    "\n",
    "Let's examine the production config system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "odibi_config_path = Path(r\"c:/Users/hodibi/OneDrive - Ingredion/Desktop/Repos/Odibi/odibi/config.py\")\n",
    "\n",
    "with open(odibi_config_path) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(f\"\\nFirst 50 lines:\\n\")\n",
    "print(''.join(lines[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analysis: Configuration Architecture\n",
    "\n",
    "### Hierarchy Overview\n",
    "\n",
    "```\n",
    "ProjectConfig (top-level)\n",
    "‚îú‚îÄ‚îÄ project: str\n",
    "‚îú‚îÄ‚îÄ engine: EngineType\n",
    "‚îú‚îÄ‚îÄ connections: Dict[str, ConnectionConfig]\n",
    "‚îú‚îÄ‚îÄ pipelines: List[PipelineConfig]\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ PipelineConfig\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ pipeline: str\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ nodes: List[NodeConfig]\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ NodeConfig\n",
    "‚îÇ               ‚îú‚îÄ‚îÄ name: str\n",
    "‚îÇ               ‚îú‚îÄ‚îÄ read: Optional[ReadConfig]\n",
    "‚îÇ               ‚îú‚îÄ‚îÄ transform: Optional[TransformConfig]\n",
    "‚îÇ               ‚îî‚îÄ‚îÄ write: Optional[WriteConfig]\n",
    "‚îú‚îÄ‚îÄ story: StoryConfig\n",
    "‚îú‚îÄ‚îÄ retry: RetryConfig\n",
    "‚îî‚îÄ‚îÄ logging: LoggingConfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Enum-Based Validation\n",
    "\n",
    "### Why Enums?\n",
    "\n",
    "Enums provide:\n",
    "- **Type safety**: Only valid values accepted\n",
    "- **IDE autocomplete**: See all options\n",
    "- **No typos**: `\"sprk\"` rejected, must be `EngineType.SPARK`\n",
    "- **Clear errors**: \"Value must be one of: spark, pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# Odibi's actual enums\n",
    "class EngineType(str, Enum):\n",
    "    \"\"\"Supported execution engines.\"\"\"\n",
    "    SPARK = \"spark\"\n",
    "    PANDAS = \"pandas\"\n",
    "\n",
    "class ConnectionType(str, Enum):\n",
    "    \"\"\"Supported connection types.\"\"\"\n",
    "    LOCAL = \"local\"\n",
    "    AZURE_BLOB = \"azure_blob\"\n",
    "    DELTA = \"delta\"\n",
    "    SQL_SERVER = \"sql_server\"\n",
    "\n",
    "class WriteMode(str, Enum):\n",
    "    \"\"\"Write modes for output operations.\"\"\"\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "\n",
    "class LogLevel(str, Enum):\n",
    "    \"\"\"Logging levels.\"\"\"\n",
    "    DEBUG = \"DEBUG\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "\n",
    "# Test enum validation\n",
    "print(\"‚úÖ Valid:\")\n",
    "print(f\"  Engine: {EngineType.SPARK}\")\n",
    "print(f\"  Connection: {ConnectionType.AZURE_BLOB}\")\n",
    "print(f\"  Write mode: {WriteMode.APPEND}\")\n",
    "\n",
    "# Test with Pydantic\n",
    "class Config(BaseModel):\n",
    "    engine: EngineType\n",
    "    mode: WriteMode\n",
    "\n",
    "valid = Config(engine=\"spark\", mode=\"append\")\n",
    "print(f\"\\n‚úÖ Valid config: {valid}\")\n",
    "\n",
    "try:\n",
    "    invalid = Config(engine=\"dask\", mode=\"append\")\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n‚ùå Invalid engine type:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight: `str, Enum` Pattern\n",
    "\n",
    "```python\n",
    "class EngineType(str, Enum):  # ‚Üê Inherits from str AND Enum\n",
    "```\n",
    "\n",
    "**Why both?**\n",
    "- `Enum`: Provides enumeration behavior\n",
    "- `str`: Makes values JSON/YAML serializable\n",
    "- Pydantic accepts both `\"spark\"` string and `EngineType.SPARK` enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Simple Nested Models\n",
    "\n",
    "Let's build from simple to complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Simplified Odibi configs\n",
    "\n",
    "class RetryConfig(BaseModel):\n",
    "    \"\"\"Retry configuration.\"\"\"\n",
    "    enabled: bool = True\n",
    "    max_attempts: int = Field(default=3, ge=1, le=10)  # Between 1 and 10\n",
    "    backoff: str = Field(default=\"exponential\", pattern=\"^(exponential|linear|constant)$\")\n",
    "\n",
    "class LoggingConfig(BaseModel):\n",
    "    \"\"\"Logging configuration.\"\"\"\n",
    "    level: LogLevel = LogLevel.INFO\n",
    "    structured: bool = Field(default=False, description=\"Output JSON logs\")\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Extra metadata\")\n",
    "\n",
    "# Test defaults\n",
    "retry = RetryConfig()\n",
    "print(f\"Default retry: {retry}\")\n",
    "\n",
    "logging = LoggingConfig(level=\"DEBUG\", metadata={\"team\": \"data-eng\"})\n",
    "print(f\"Custom logging: {logging}\")\n",
    "\n",
    "# Test constraints\n",
    "try:\n",
    "    bad_retry = RetryConfig(max_attempts=100)  # > 10\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n‚ùå Constraint violation:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Constraints\n",
    "\n",
    "Pydantic provides rich validation:\n",
    "```python\n",
    "Field(default=3, ge=1, le=10)  # Greater/equal 1, less/equal 10\n",
    "Field(pattern=\"^(exponential|linear|constant)$\")  # Regex validation\n",
    "Field(default_factory=dict)  # Mutable defaults (safe!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Read/Write Configs with Model Validators\n",
    "\n",
    "### The Business Rule\n",
    "\n",
    "For reading/writing data, you need **either** a `table` OR a `path`, but not both and not neither.\n",
    "\n",
    "**This is cross-field validation** - can't check with single field validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import model_validator\n",
    "\n",
    "class ReadConfig(BaseModel):\n",
    "    \"\"\"Configuration for reading data.\"\"\"\n",
    "    connection: str = Field(description=\"Connection name\")\n",
    "    format: str = Field(description=\"Data format (csv, parquet, delta)\")\n",
    "    table: Optional[str] = Field(default=None, description=\"Table name for SQL/Delta\")\n",
    "    path: Optional[str] = Field(default=None, description=\"Path for file-based sources\")\n",
    "    options: Dict[str, Any] = Field(default_factory=dict, description=\"Format options\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_table_or_path(self):\n",
    "        \"\"\"Ensure either table or path is provided.\"\"\"\n",
    "        if not self.table and not self.path:\n",
    "            raise ValueError(\"Either 'table' or 'path' must be provided for read config\")\n",
    "        return self\n",
    "\n",
    "class WriteConfig(BaseModel):\n",
    "    \"\"\"Configuration for writing data.\"\"\"\n",
    "    connection: str = Field(description=\"Connection name\")\n",
    "    format: str = Field(description=\"Output format\")\n",
    "    table: Optional[str] = Field(default=None, description=\"Table name\")\n",
    "    path: Optional[str] = Field(default=None, description=\"Output path\")\n",
    "    mode: WriteMode = Field(default=WriteMode.OVERWRITE, description=\"Write mode\")\n",
    "    options: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_table_or_path(self):\n",
    "        \"\"\"Ensure either table or path is provided.\"\"\"\n",
    "        if not self.table and not self.path:\n",
    "            raise ValueError(\"Either 'table' or 'path' must be provided for write config\")\n",
    "        return self\n",
    "\n",
    "# ‚úÖ Valid - has table\n",
    "read1 = ReadConfig(connection=\"delta\", format=\"delta\", table=\"sales\")\n",
    "print(f\"‚úÖ Read with table: {read1}\")\n",
    "\n",
    "# ‚úÖ Valid - has path\n",
    "write1 = WriteConfig(connection=\"local\", format=\"parquet\", path=\"output/data.parquet\")\n",
    "print(f\"‚úÖ Write with path: {write1}\")\n",
    "\n",
    "# ‚ùå Invalid - neither\n",
    "try:\n",
    "    read2 = ReadConfig(connection=\"local\", format=\"csv\")\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n‚ùå Missing table/path:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Model Validator Pattern\n",
    "\n",
    "```python\n",
    "@model_validator(mode=\"after\")\n",
    "def check_something(self):\n",
    "    # self.field1, self.field2, etc. are all populated\n",
    "    if some_condition:\n",
    "        raise ValueError(\"Clear error message\")\n",
    "    return self  # ‚Üê MUST return self!\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Cross-field validation\n",
    "- Business rules involving multiple fields\n",
    "- Complex conditional logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Transform Config - Flexibility\n",
    "\n",
    "Transformations can be:\n",
    "1. Simple SQL strings\n",
    "2. Structured `TransformStep` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class TransformStep(BaseModel):\n",
    "    \"\"\"Single transformation step.\"\"\"\n",
    "    sql: Optional[str] = None\n",
    "    function: Optional[str] = None\n",
    "    operation: Optional[str] = None\n",
    "    params: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_step_type(self):\n",
    "        \"\"\"Ensure exactly one step type is provided.\"\"\"\n",
    "        step_types = [self.sql, self.function, self.operation]\n",
    "        if sum(x is not None for x in step_types) != 1:\n",
    "            raise ValueError(\"Exactly one of 'sql', 'function', or 'operation' must be provided\")\n",
    "        return self\n",
    "\n",
    "class TransformConfig(BaseModel):\n",
    "    \"\"\"Configuration for transforming data.\"\"\"\n",
    "    steps: List[Union[str, TransformStep]] = Field(\n",
    "        description=\"List of transformation steps (SQL strings or TransformStep configs)\"\n",
    "    )\n",
    "\n",
    "# ‚úÖ Simple SQL strings\n",
    "transform1 = TransformConfig(steps=[\n",
    "    \"SELECT * FROM data WHERE amount > 0\",\n",
    "    \"SELECT customer_id, SUM(amount) as total FROM data GROUP BY customer_id\"\n",
    "])\n",
    "print(f\"‚úÖ SQL transforms:\\n{transform1}\\n\")\n",
    "\n",
    "# ‚úÖ Structured steps\n",
    "transform2 = TransformConfig(steps=[\n",
    "    TransformStep(sql=\"SELECT * FROM data\"),\n",
    "    TransformStep(function=\"deduplicate\", params={\"columns\": [\"id\"]}),\n",
    "    TransformStep(operation=\"filter_nulls\", params={\"columns\": [\"name\", \"email\"]})\n",
    "])\n",
    "print(f\"‚úÖ Structured transforms:\\n{transform2}\\n\")\n",
    "\n",
    "# ‚ùå Invalid - multiple step types\n",
    "try:\n",
    "    bad_step = TransformStep(sql=\"SELECT *\", function=\"dedupe\")\n",
    "except ValidationError as e:\n",
    "    print(f\"‚ùå Multiple step types:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union Types for Flexibility\n",
    "\n",
    "```python\n",
    "steps: List[Union[str, TransformStep]]\n",
    "```\n",
    "\n",
    "**Allows:**\n",
    "- Quick prototyping: just pass SQL strings\n",
    "- Advanced usage: structured steps with parameters\n",
    "- Mix both in same pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Node Config - Composition\n",
    "\n",
    "Nodes are the atomic units of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationConfig(BaseModel):\n",
    "    \"\"\"Configuration for data validation.\"\"\"\n",
    "    schema_validation: Optional[Dict[str, Any]] = Field(\n",
    "        default=None, alias=\"schema\", description=\"Schema validation rules\"\n",
    "    )\n",
    "    not_empty: bool = Field(default=False, description=\"Ensure result is not empty\")\n",
    "    no_nulls: List[str] = Field(\n",
    "        default_factory=list, description=\"Columns that must not have nulls\"\n",
    "    )\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    \"\"\"Configuration for a single node.\"\"\"\n",
    "    name: str = Field(description=\"Unique node name\")\n",
    "    description: Optional[str] = Field(default=None, description=\"Human-readable description\")\n",
    "    depends_on: List[str] = Field(default_factory=list, description=\"List of node dependencies\")\n",
    "\n",
    "    # Operations (at least one required)\n",
    "    read: Optional[ReadConfig] = None\n",
    "    transform: Optional[TransformConfig] = None\n",
    "    write: Optional[WriteConfig] = None\n",
    "\n",
    "    # Optional features\n",
    "    cache: bool = Field(default=False, description=\"Cache result for reuse\")\n",
    "    validation: Optional[ValidationConfig] = None\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_at_least_one_operation(self):\n",
    "        \"\"\"Ensure at least one operation is defined.\"\"\"\n",
    "        if not any([self.read, self.transform, self.write]):\n",
    "            raise ValueError(\n",
    "                f\"Node '{self.name}' must have at least one of: read, transform, write\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "# ‚úÖ Read-only node\n",
    "source_node = NodeConfig(\n",
    "    name=\"raw_sales\",\n",
    "    read=ReadConfig(connection=\"delta\", format=\"delta\", table=\"raw.sales\")\n",
    ")\n",
    "print(f\"‚úÖ Source node: {source_node.name}\\n\")\n",
    "\n",
    "# ‚úÖ Transform node with dependencies\n",
    "transform_node = NodeConfig(\n",
    "    name=\"clean_sales\",\n",
    "    depends_on=[\"raw_sales\"],\n",
    "    transform=TransformConfig(steps=[\n",
    "        \"SELECT * FROM raw_sales WHERE amount > 0\"\n",
    "    ]),\n",
    "    validation=ValidationConfig(not_empty=True, no_nulls=[\"customer_id\", \"amount\"]),\n",
    "    cache=True\n",
    ")\n",
    "print(f\"‚úÖ Transform node: {transform_node.name}, depends on {transform_node.depends_on}\\n\")\n",
    "\n",
    "# ‚úÖ Write node\n",
    "sink_node = NodeConfig(\n",
    "    name=\"sales_output\",\n",
    "    depends_on=[\"clean_sales\"],\n",
    "    write=WriteConfig(\n",
    "        connection=\"delta\",\n",
    "        format=\"delta\",\n",
    "        table=\"silver.sales\",\n",
    "        mode=WriteMode.OVERWRITE\n",
    "    )\n",
    ")\n",
    "print(f\"‚úÖ Sink node: {sink_node.name}\\n\")\n",
    "\n",
    "# ‚ùå Invalid - no operations\n",
    "try:\n",
    "    empty_node = NodeConfig(name=\"empty\")\n",
    "except ValidationError as e:\n",
    "    print(f\"‚ùå No operations:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Node Design Principles\n",
    "\n",
    "1. **Flexible**: Can read, transform, write, or any combination\n",
    "2. **Validated**: Must have at least one operation\n",
    "3. **Dependencies**: Explicit via `depends_on`\n",
    "4. **Optional features**: Caching, validation bolt-ons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Pipeline Config - Collections\n",
    "\n",
    "Pipelines are collections of nodes with uniqueness validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_validator\n",
    "\n",
    "class PipelineConfig(BaseModel):\n",
    "    \"\"\"Configuration for a pipeline.\"\"\"\n",
    "    pipeline: str = Field(description=\"Pipeline name\")\n",
    "    description: Optional[str] = Field(default=None, description=\"Pipeline description\")\n",
    "    layer: Optional[str] = Field(default=None, description=\"Logical layer (bronze/silver/gold)\")\n",
    "    nodes: List[NodeConfig] = Field(description=\"List of nodes in this pipeline\")\n",
    "\n",
    "    @field_validator(\"nodes\")\n",
    "    @classmethod\n",
    "    def check_unique_node_names(cls, nodes: List[NodeConfig]) -> List[NodeConfig]:\n",
    "        \"\"\"Ensure all node names are unique within the pipeline.\"\"\"\n",
    "        names = [node.name for node in nodes]\n",
    "        if len(names) != len(set(names)):\n",
    "            duplicates = [name for name in names if names.count(name) > 1]\n",
    "            raise ValueError(f\"Duplicate node names found: {set(duplicates)}\")\n",
    "        return nodes\n",
    "\n",
    "# ‚úÖ Valid pipeline\n",
    "pipeline = PipelineConfig(\n",
    "    pipeline=\"sales_processing\",\n",
    "    description=\"Clean and aggregate sales data\",\n",
    "    layer=\"silver\",\n",
    "    nodes=[source_node, transform_node, sink_node]\n",
    ")\n",
    "print(f\"‚úÖ Pipeline: {pipeline.pipeline}\")\n",
    "print(f\"   Nodes: {[n.name for n in pipeline.nodes]}\\n\")\n",
    "\n",
    "# ‚ùå Invalid - duplicate names\n",
    "try:\n",
    "    bad_pipeline = PipelineConfig(\n",
    "        pipeline=\"bad\",\n",
    "        nodes=[\n",
    "            NodeConfig(name=\"node1\", read=ReadConfig(connection=\"x\", format=\"csv\", path=\"a\")),\n",
    "            NodeConfig(name=\"node1\", read=ReadConfig(connection=\"x\", format=\"csv\", path=\"b\"))\n",
    "        ]\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"‚ùå Duplicate node names:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Validator vs Model Validator\n",
    "\n",
    "**Field Validator:**\n",
    "```python\n",
    "@field_validator(\"nodes\")\n",
    "@classmethod\n",
    "def check_unique_node_names(cls, nodes: List[NodeConfig]):\n",
    "    # Validates ONLY the 'nodes' field\n",
    "    # Must be @classmethod, receives cls + field value\n",
    "```\n",
    "\n",
    "**Model Validator:**\n",
    "```python\n",
    "@model_validator(mode=\"after\")\n",
    "def check_cross_field(self):\n",
    "    # Validates across ALL fields\n",
    "    # Instance method, receives self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Connection Configs - Discriminated Unions\n",
    "\n",
    "Different connection types need different fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class BaseConnectionConfig(BaseModel):\n",
    "    \"\"\"Base configuration for all connections.\"\"\"\n",
    "    type: ConnectionType\n",
    "    validation_mode: str = \"lazy\"  # 'lazy' or 'eager'\n",
    "\n",
    "class LocalConnectionConfig(BaseConnectionConfig):\n",
    "    \"\"\"Local filesystem connection.\"\"\"\n",
    "    type: ConnectionType = ConnectionType.LOCAL\n",
    "    base_path: str = Field(default=\"./data\", description=\"Base directory path\")\n",
    "\n",
    "class AzureBlobConnectionConfig(BaseConnectionConfig):\n",
    "    \"\"\"Azure Blob Storage connection.\"\"\"\n",
    "    type: ConnectionType = ConnectionType.AZURE_BLOB\n",
    "    account_name: str\n",
    "    container: str\n",
    "    auth: Dict[str, str] = Field(default_factory=dict)\n",
    "\n",
    "class DeltaConnectionConfig(BaseConnectionConfig):\n",
    "    \"\"\"Delta Lake connection.\"\"\"\n",
    "    type: ConnectionType = ConnectionType.DELTA\n",
    "    catalog: str\n",
    "    schema_name: str = Field(alias=\"schema\")  # 'schema' is Python keyword\n",
    "\n",
    "class SQLServerConnectionConfig(BaseConnectionConfig):\n",
    "    \"\"\"SQL Server connection.\"\"\"\n",
    "    type: ConnectionType = ConnectionType.SQL_SERVER\n",
    "    host: str\n",
    "    database: str\n",
    "    port: int = 1433\n",
    "    auth: Dict[str, str] = Field(default_factory=dict)\n",
    "\n",
    "# Union of all connection types\n",
    "ConnectionConfig = Union[\n",
    "    LocalConnectionConfig,\n",
    "    AzureBlobConnectionConfig,\n",
    "    DeltaConnectionConfig,\n",
    "    SQLServerConnectionConfig,\n",
    "]\n",
    "\n",
    "# ‚úÖ Local connection\n",
    "local = LocalConnectionConfig(type=\"local\", base_path=\"/data/raw\")\n",
    "print(f\"‚úÖ Local: {local}\\n\")\n",
    "\n",
    "# ‚úÖ Azure connection\n",
    "azure = AzureBlobConnectionConfig(\n",
    "    type=\"azure_blob\",\n",
    "    account_name=\"myaccount\",\n",
    "    container=\"data\",\n",
    "    auth={\"method\": \"sas_token\"}\n",
    ")\n",
    "print(f\"‚úÖ Azure: {azure}\\n\")\n",
    "\n",
    "# ‚úÖ Delta connection\n",
    "delta = DeltaConnectionConfig(\n",
    "    type=\"delta\",\n",
    "    catalog=\"main\",\n",
    "    schema_name=\"bronze\"  # Note: schema_name, not schema (Python keyword)\n",
    ")\n",
    "print(f\"‚úÖ Delta: {delta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Discriminated Union Pattern\n",
    "\n",
    "```python\n",
    "ConnectionConfig = Union[\n",
    "    LocalConnectionConfig,\n",
    "    AzureBlobConnectionConfig,\n",
    "    # ...\n",
    "]\n",
    "```\n",
    "\n",
    "Pydantic uses the `type` field to determine which model to use:\n",
    "- `type: \"local\"` ‚Üí `LocalConnectionConfig`\n",
    "- `type: \"azure_blob\"` ‚Üí `AzureBlobConnectionConfig`\n",
    "- etc.\n",
    "\n",
    "**Benefits:**\n",
    "- Type-safe: Each connection type has its required fields\n",
    "- Extensible: Add new connection types without changing existing code\n",
    "- Clear errors: \"Missing field 'container' for Azure connection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Project Config - The Top Level\n",
    "\n",
    "Everything comes together in `ProjectConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryConfig(BaseModel):\n",
    "    \"\"\"Story generation configuration.\n",
    "    \n",
    "    Stories are ODIBI's core value - execution reports with lineage.\n",
    "    \"\"\"\n",
    "    connection: str = Field(description=\"Connection name for story output\")\n",
    "    path: str = Field(description=\"Path for stories\")\n",
    "    max_sample_rows: int = Field(default=10, ge=0, le=100)\n",
    "    auto_generate: bool = True\n",
    "\n",
    "class ProjectConfig(BaseModel):\n",
    "    \"\"\"Complete project configuration from YAML.\"\"\"\n",
    "    \n",
    "    # === MANDATORY ===\n",
    "    project: str = Field(description=\"Project name\")\n",
    "    engine: EngineType = Field(default=EngineType.PANDAS, description=\"Execution engine\")\n",
    "    connections: Dict[str, Dict[str, Any]] = Field(\n",
    "        description=\"Named connections (at least one required)\"\n",
    "    )\n",
    "    pipelines: List[PipelineConfig] = Field(\n",
    "        description=\"Pipeline definitions (at least one required)\"\n",
    "    )\n",
    "    story: StoryConfig = Field(description=\"Story generation configuration (mandatory)\")\n",
    "\n",
    "    # === OPTIONAL (with sensible defaults) ===\n",
    "    description: Optional[str] = Field(default=None, description=\"Project description\")\n",
    "    version: str = Field(default=\"1.0.0\", description=\"Project version\")\n",
    "    owner: Optional[str] = Field(default=None, description=\"Project owner/contact\")\n",
    "\n",
    "    # Global settings (optional with defaults)\n",
    "    retry: RetryConfig = Field(default_factory=RetryConfig)\n",
    "    logging: LoggingConfig = Field(default_factory=LoggingConfig)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_story_connection_exists(self):\n",
    "        \"\"\"Ensure story.connection is defined in connections.\"\"\"\n",
    "        if self.story.connection not in self.connections:\n",
    "            available = \", \".join(self.connections.keys())\n",
    "            raise ValueError(\n",
    "                f\"Story connection '{self.story.connection}' not found. \"\n",
    "                f\"Available connections: {available}\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "# ‚úÖ Minimal valid project\n",
    "project = ProjectConfig(\n",
    "    project=\"sales_analytics\",\n",
    "    engine=\"pandas\",\n",
    "    connections={\n",
    "        \"local\": {\"type\": \"local\", \"base_path\": \"./data\"}\n",
    "    },\n",
    "    story=StoryConfig(connection=\"local\", path=\"stories/\"),\n",
    "    pipelines=[pipeline]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Project: {project.project}\")\n",
    "print(f\"   Engine: {project.engine}\")\n",
    "print(f\"   Pipelines: {[p.pipeline for p in project.pipelines]}\")\n",
    "print(f\"   Connections: {list(project.connections.keys())}\")\n",
    "print(f\"   Version: {project.version}\")\n",
    "\n",
    "# ‚ùå Invalid - story connection doesn't exist\n",
    "try:\n",
    "    bad_project = ProjectConfig(\n",
    "        project=\"bad\",\n",
    "        connections={\"local\": {\"type\": \"local\"}},\n",
    "        story=StoryConfig(connection=\"azure\", path=\"stories/\"),  # ‚Üê doesn't exist\n",
    "        pipelines=[pipeline]\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n‚ùå Story connection validation:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Cross-Collection Validation\n",
    "\n",
    "The `validate_story_connection_exists` model validator ensures referential integrity:\n",
    "- Story references a connection name\n",
    "- That connection must exist in `connections` dict\n",
    "- Clear error if missing\n",
    "\n",
    "This prevents runtime errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ YAML Loading - The Complete Workflow\n",
    "\n",
    "Now let's load real YAML configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Sample YAML configuration\n",
    "yaml_config = \"\"\"\n",
    "project: sales_analytics\n",
    "description: Daily sales data processing pipeline\n",
    "version: \"2.1.0\"\n",
    "owner: data-team@company.com\n",
    "engine: pandas\n",
    "\n",
    "connections:\n",
    "  local:\n",
    "    type: local\n",
    "    base_path: ./data\n",
    "  \n",
    "  delta:\n",
    "    type: delta\n",
    "    catalog: main\n",
    "    schema: bronze\n",
    "\n",
    "story:\n",
    "  connection: local\n",
    "  path: stories/\n",
    "  max_sample_rows: 5\n",
    "\n",
    "retry:\n",
    "  enabled: true\n",
    "  max_attempts: 3\n",
    "  backoff: exponential\n",
    "\n",
    "logging:\n",
    "  level: INFO\n",
    "  structured: false\n",
    "  metadata:\n",
    "    team: data-engineering\n",
    "    environment: production\n",
    "\n",
    "pipelines:\n",
    "  - pipeline: bronze_ingestion\n",
    "    layer: bronze\n",
    "    description: Ingest raw sales data\n",
    "    nodes:\n",
    "      - name: read_raw_sales\n",
    "        description: Read CSV sales files\n",
    "        read:\n",
    "          connection: local\n",
    "          format: csv\n",
    "          path: input/sales.csv\n",
    "          options:\n",
    "            header: true\n",
    "            inferSchema: true\n",
    "      \n",
    "      - name: write_bronze\n",
    "        depends_on: [read_raw_sales]\n",
    "        write:\n",
    "          connection: delta\n",
    "          format: delta\n",
    "          table: sales\n",
    "          mode: append\n",
    "        validation:\n",
    "          not_empty: true\n",
    "          no_nulls: [transaction_id, amount]\n",
    "\n",
    "  - pipeline: silver_transformation\n",
    "    layer: silver\n",
    "    description: Clean and validate sales\n",
    "    nodes:\n",
    "      - name: clean_sales\n",
    "        transform:\n",
    "          steps:\n",
    "            - SELECT * FROM bronze.sales WHERE amount > 0\n",
    "            - SELECT DISTINCT * FROM data\n",
    "        cache: true\n",
    "      \n",
    "      - name: aggregate_sales\n",
    "        depends_on: [clean_sales]\n",
    "        transform:\n",
    "          steps:\n",
    "            - sql: SELECT customer_id, SUM(amount) as total FROM clean_sales GROUP BY customer_id\n",
    "        write:\n",
    "          connection: delta\n",
    "          format: delta\n",
    "          table: customer_totals\n",
    "          mode: overwrite\n",
    "\"\"\"\n",
    "\n",
    "# Load and validate\n",
    "raw_config = yaml.safe_load(yaml_config)\n",
    "config = ProjectConfig(**raw_config)\n",
    "\n",
    "print(\"‚úÖ Successfully loaded and validated project config!\\n\")\n",
    "print(f\"Project: {config.project}\")\n",
    "print(f\"Version: {config.version}\")\n",
    "print(f\"Owner: {config.owner}\")\n",
    "print(f\"Engine: {config.engine}\")\n",
    "print(f\"\\nConnections: {list(config.connections.keys())}\")\n",
    "print(f\"\\nPipelines:\")\n",
    "for p in config.pipelines:\n",
    "    print(f\"  - {p.pipeline} ({p.layer}): {len(p.nodes)} nodes\")\n",
    "    for n in p.nodes:\n",
    "        ops = []\n",
    "        if n.read: ops.append(\"read\")\n",
    "        if n.transform: ops.append(\"transform\")\n",
    "        if n.write: ops.append(\"write\")\n",
    "        print(f\"    ‚Ä¢ {n.name}: {', '.join(ops)}\")\n",
    "        if n.depends_on:\n",
    "            print(f\"      depends_on: {n.depends_on}\")\n",
    "\n",
    "print(f\"\\nRetry: {config.retry}\")\n",
    "print(f\"Logging: {config.logging}\")\n",
    "print(f\"Story: connection={config.story.connection}, path={config.story.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Error Messages - User Experience\n",
    "\n",
    "Let's see what happens with various config errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 1: Missing required field\n",
    "bad_yaml_1 = \"\"\"\n",
    "project: test\n",
    "# Missing: connections, pipelines, story\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    config = ProjectConfig(**yaml.safe_load(bad_yaml_1))\n",
    "except ValidationError as e:\n",
    "    print(\"‚ùå Error 1: Missing required fields\")\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 2: Invalid enum value\n",
    "bad_yaml_2 = \"\"\"\n",
    "project: test\n",
    "engine: dask  # Not a valid EngineType\n",
    "connections:\n",
    "  local: {type: local}\n",
    "story:\n",
    "  connection: local\n",
    "  path: stories/\n",
    "pipelines:\n",
    "  - pipeline: test\n",
    "    nodes:\n",
    "      - name: node1\n",
    "        read: {connection: local, format: csv, path: data.csv}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    config = ProjectConfig(**yaml.safe_load(bad_yaml_2))\n",
    "except ValidationError as e:\n",
    "    print(\"‚ùå Error 2: Invalid enum value\")\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 3: Model validator failure\n",
    "bad_yaml_3 = \"\"\"\n",
    "project: test\n",
    "engine: pandas\n",
    "connections:\n",
    "  local: {type: local}\n",
    "story:\n",
    "  connection: azure  # Doesn't exist!\n",
    "  path: stories/\n",
    "pipelines:\n",
    "  - pipeline: test\n",
    "    nodes:\n",
    "      - name: node1\n",
    "        read: {connection: local, format: csv, path: data.csv}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    config = ProjectConfig(**yaml.safe_load(bad_yaml_3))\n",
    "except ValidationError as e:\n",
    "    print(\"‚ùå Error 3: Story connection doesn't exist\")\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 4: Duplicate node names\n",
    "bad_yaml_4 = \"\"\"\n",
    "project: test\n",
    "engine: pandas\n",
    "connections:\n",
    "  local: {type: local}\n",
    "story:\n",
    "  connection: local\n",
    "  path: stories/\n",
    "pipelines:\n",
    "  - pipeline: test\n",
    "    nodes:\n",
    "      - name: node1\n",
    "        read: {connection: local, format: csv, path: a.csv}\n",
    "      - name: node1  # Duplicate!\n",
    "        read: {connection: local, format: csv, path: b.csv}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    config = ProjectConfig(**yaml.safe_load(bad_yaml_4))\n",
    "except ValidationError as e:\n",
    "    print(\"‚ùå Error 4: Duplicate node names\")\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 5: Node with no operations\n",
    "bad_yaml_5 = \"\"\"\n",
    "project: test\n",
    "engine: pandas\n",
    "connections:\n",
    "  local: {type: local}\n",
    "story:\n",
    "  connection: local\n",
    "  path: stories/\n",
    "pipelines:\n",
    "  - pipeline: test\n",
    "    nodes:\n",
    "      - name: empty_node\n",
    "        # No read, transform, or write!\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    config = ProjectConfig(**yaml.safe_load(bad_yaml_5))\n",
    "except ValidationError as e:\n",
    "    print(\"‚ùå Error 5: Node with no operations\")\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Build: Simplified Config System\n",
    "\n",
    "Let's create a mini version from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional, List, Dict, Any\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator, ValidationError\n",
    "import yaml\n",
    "\n",
    "# === ENUMS ===\n",
    "class DataFormat(str, Enum):\n",
    "    CSV = \"csv\"\n",
    "    PARQUET = \"parquet\"\n",
    "    JSON = \"json\"\n",
    "\n",
    "class TransformType(str, Enum):\n",
    "    FILTER = \"filter\"\n",
    "    AGGREGATE = \"aggregate\"\n",
    "    JOIN = \"join\"\n",
    "\n",
    "# === CONFIGS ===\n",
    "class SourceConfig(BaseModel):\n",
    "    path: str\n",
    "    format: DataFormat\n",
    "    columns: Optional[List[str]] = None\n",
    "\n",
    "class TransformConfig(BaseModel):\n",
    "    type: TransformType\n",
    "    params: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class SinkConfig(BaseModel):\n",
    "    path: str\n",
    "    format: DataFormat\n",
    "\n",
    "class TaskConfig(BaseModel):\n",
    "    name: str\n",
    "    source: Optional[SourceConfig] = None\n",
    "    transforms: List[TransformConfig] = Field(default_factory=list)\n",
    "    sink: Optional[SinkConfig] = None\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_has_operation(self):\n",
    "        if not any([self.source, self.transforms, self.sink]):\n",
    "            raise ValueError(f\"Task '{self.name}' must have at least one operation\")\n",
    "        return self\n",
    "\n",
    "class WorkflowConfig(BaseModel):\n",
    "    name: str\n",
    "    tasks: List[TaskConfig]\n",
    "    \n",
    "    @field_validator(\"tasks\")\n",
    "    @classmethod\n",
    "    def check_unique_names(cls, tasks):\n",
    "        names = [t.name for t in tasks]\n",
    "        if len(names) != len(set(names)):\n",
    "            raise ValueError(f\"Duplicate task names found\")\n",
    "        return tasks\n",
    "\n",
    "# === TEST ===\n",
    "workflow_yaml = \"\"\"\n",
    "name: data_pipeline\n",
    "tasks:\n",
    "  - name: load_data\n",
    "    source:\n",
    "      path: input/data.csv\n",
    "      format: csv\n",
    "      columns: [id, name, value]\n",
    "  \n",
    "  - name: process_data\n",
    "    transforms:\n",
    "      - type: filter\n",
    "        params:\n",
    "          condition: value > 100\n",
    "      - type: aggregate\n",
    "        params:\n",
    "          group_by: [name]\n",
    "          agg: sum\n",
    "  \n",
    "  - name: save_results\n",
    "    sink:\n",
    "      path: output/results.parquet\n",
    "      format: parquet\n",
    "\"\"\"\n",
    "\n",
    "workflow = WorkflowConfig(**yaml.safe_load(workflow_yaml))\n",
    "print(f\"‚úÖ Workflow: {workflow.name}\")\n",
    "print(f\"   Tasks: {[t.name for t in workflow.tasks]}\")\n",
    "for task in workflow.tasks:\n",
    "    print(f\"\\n   Task: {task.name}\")\n",
    "    if task.source:\n",
    "        print(f\"     Source: {task.source.path} ({task.source.format})\")\n",
    "    if task.transforms:\n",
    "        print(f\"     Transforms: {[t.type for t in task.transforms]}\")\n",
    "    if task.sink:\n",
    "        print(f\"     Sink: {task.sink.path} ({task.sink.format})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Enums**: Type-safe constants prevent errors\n",
    "2. **Nested Models**: Compose complex configs from simple pieces\n",
    "3. **Field Validators**: Validate individual fields with constraints\n",
    "4. **Model Validators**: Cross-field validation and business rules\n",
    "5. **Discriminated Unions**: Different models based on type field\n",
    "6. **YAML ‚Üí Pydantic**: Load and validate in one step\n",
    "7. **Error Messages**: Clear, actionable validation errors\n",
    "\n",
    "### Odibi's Config Architecture\n",
    "\n",
    "```\n",
    "ProjectConfig (313 lines)\n",
    "‚îú‚îÄ‚îÄ Enums (4): EngineType, ConnectionType, WriteMode, LogLevel\n",
    "‚îú‚îÄ‚îÄ Connections (5 types): Local, Azure, Delta, SQL Server + Union\n",
    "‚îú‚îÄ‚îÄ Node Ops (4): ReadConfig, TransformConfig, WriteConfig, ValidationConfig\n",
    "‚îú‚îÄ‚îÄ Hierarchy (3): NodeConfig ‚Üí PipelineConfig ‚Üí ProjectConfig\n",
    "‚îî‚îÄ‚îÄ Global (3): RetryConfig, LoggingConfig, StoryConfig\n",
    "```\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "| Pattern | Use Case | Example |\n",
    "|---------|----------|----------|\n",
    "| `str, Enum` | Type-safe constants | `EngineType.SPARK` |\n",
    "| `Field(default_factory=dict)` | Mutable defaults | `options: Dict = Field(default_factory=dict)` |\n",
    "| `@model_validator(mode=\"after\")` | Cross-field validation | Check table OR path |\n",
    "| `@field_validator(\"field\")` | Single field validation | Unique node names |\n",
    "| `Union[Type1, Type2]` | Discriminated unions | Different connection types |\n",
    "| `Optional[Type]` | Nullable fields | `description: Optional[str] = None` |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Use enums for fixed sets of values\n",
    "- Validate early with Pydantic\n",
    "- Provide clear error messages\n",
    "- Use `default_factory` for mutable defaults\n",
    "- Document fields with `description`\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Use magic strings without validation\n",
    "- Use `= {}` or `= []` for defaults\n",
    "- Write vague error messages\n",
    "- Catch errors at runtime instead of config load\n",
    "- Skip type hints\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete `exercises.ipynb`\n",
    "2. Review `odibi_config_reference.md`\n",
    "3. Move to `02_execution_context/` to see configs in action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
