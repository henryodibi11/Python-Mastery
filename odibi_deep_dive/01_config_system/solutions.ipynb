{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Odibi Configuration System Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: PartitionStrategy Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "class PartitionStrategy(str, Enum):\n",
    "    \"\"\"Partition strategies for data writing.\"\"\"\n",
    "    NONE = \"none\"\n",
    "    HASH = \"hash\"\n",
    "    RANGE = \"range\"\n",
    "    DATE = \"date\"\n",
    "\n",
    "class PartitionConfig(BaseModel):\n",
    "    strategy: PartitionStrategy\n",
    "    columns: List[str] = Field(default_factory=list)\n",
    "\n",
    "# Test valid\n",
    "config1 = PartitionConfig(strategy=\"hash\", columns=[\"user_id\"])\n",
    "print(f\"âœ… Valid: {config1}\")\n",
    "\n",
    "config2 = PartitionConfig(strategy=PartitionStrategy.DATE, columns=[\"created_date\"])\n",
    "print(f\"âœ… Valid: {config2}\")\n",
    "\n",
    "# Test invalid\n",
    "try:\n",
    "    config3 = PartitionConfig(strategy=\"random\", columns=[])\n",
    "except ValidationError as e:\n",
    "    print(f\"\\nâŒ Invalid strategy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Enhanced WriteConfig with Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "\n",
    "class WriteMode(str, Enum):\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "\n",
    "class WriteConfig(BaseModel):\n",
    "    \"\"\"Configuration for writing data.\"\"\"\n",
    "    connection: str\n",
    "    format: str\n",
    "    table: Optional[str] = None\n",
    "    path: Optional[str] = None\n",
    "    mode: WriteMode = WriteMode.OVERWRITE\n",
    "    options: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \n",
    "    # Partitioning\n",
    "    partition_strategy: PartitionStrategy = PartitionStrategy.NONE\n",
    "    partition_columns: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_table_or_path(self):\n",
    "        \"\"\"Ensure either table or path is provided.\"\"\"\n",
    "        if not self.table and not self.path:\n",
    "            raise ValueError(\"Either 'table' or 'path' must be provided\")\n",
    "        return self\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_partition_columns(self):\n",
    "        \"\"\"If partitioning is enabled, columns must be specified.\"\"\"\n",
    "        if self.partition_strategy != PartitionStrategy.NONE:\n",
    "            if not self.partition_columns:\n",
    "                raise ValueError(\n",
    "                    f\"Partition strategy '{self.partition_strategy}' requires partition_columns. \"\n",
    "                    f\"Specify at least one column to partition by.\"\n",
    "                )\n",
    "        return self\n",
    "\n",
    "# Test 1: No partitioning (valid)\n",
    "write1 = WriteConfig(\n",
    "    connection=\"local\",\n",
    "    format=\"parquet\",\n",
    "    path=\"output/data.parquet\"\n",
    ")\n",
    "print(f\"âœ… No partitioning: {write1.partition_strategy}\\n\")\n",
    "\n",
    "# Test 2: With partitioning (valid)\n",
    "write2 = WriteConfig(\n",
    "    connection=\"delta\",\n",
    "    format=\"delta\",\n",
    "    table=\"sales\",\n",
    "    partition_strategy=\"date\",\n",
    "    partition_columns=[\"created_date\", \"region\"]\n",
    ")\n",
    "print(f\"âœ… With partitioning: {write2.partition_strategy}, columns={write2.partition_columns}\\n\")\n",
    "\n",
    "# Test 3: Partitioning without columns (invalid)\n",
    "try:\n",
    "    write3 = WriteConfig(\n",
    "        connection=\"delta\",\n",
    "        format=\"delta\",\n",
    "        table=\"sales\",\n",
    "        partition_strategy=\"hash\"\n",
    "        # Missing partition_columns!\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"âŒ Partitioning without columns:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: ScheduleConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduleType(str, Enum):\n",
    "    \"\"\"Pipeline schedule types.\"\"\"\n",
    "    MANUAL = \"manual\"\n",
    "    CRON = \"cron\"\n",
    "    INTERVAL = \"interval\"\n",
    "\n",
    "class ScheduleConfig(BaseModel):\n",
    "    \"\"\"Configuration for pipeline scheduling.\"\"\"\n",
    "    type: ScheduleType\n",
    "    cron_expression: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Cron expression (required if type is CRON)\"\n",
    "    )\n",
    "    interval_minutes: Optional[int] = Field(\n",
    "        default=None,\n",
    "        ge=1,\n",
    "        description=\"Interval in minutes (required if type is INTERVAL)\"\n",
    "    )\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_schedule_requirements(self):\n",
    "        \"\"\"Validate schedule type-specific requirements.\"\"\"\n",
    "        if self.type == ScheduleType.CRON:\n",
    "            if not self.cron_expression:\n",
    "                raise ValueError(\n",
    "                    \"Schedule type 'cron' requires 'cron_expression'. \"\n",
    "                    \"Example: '0 0 * * *' for daily at midnight.\"\n",
    "                )\n",
    "        \n",
    "        if self.type == ScheduleType.INTERVAL:\n",
    "            if not self.interval_minutes:\n",
    "                raise ValueError(\n",
    "                    \"Schedule type 'interval' requires 'interval_minutes'. \"\n",
    "                    \"Specify how often the pipeline should run (minimum 1 minute).\"\n",
    "                )\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Test 1: Manual schedule\n",
    "schedule1 = ScheduleConfig(type=\"manual\")\n",
    "print(f\"âœ… Manual schedule: {schedule1}\\n\")\n",
    "\n",
    "# Test 2: Cron schedule\n",
    "schedule2 = ScheduleConfig(\n",
    "    type=\"cron\",\n",
    "    cron_expression=\"0 0 * * *\"  # Daily at midnight\n",
    ")\n",
    "print(f\"âœ… Cron schedule: {schedule2.cron_expression}\\n\")\n",
    "\n",
    "# Test 3: Interval schedule\n",
    "schedule3 = ScheduleConfig(\n",
    "    type=\"interval\",\n",
    "    interval_minutes=30\n",
    ")\n",
    "print(f\"âœ… Interval schedule: every {schedule3.interval_minutes} minutes\\n\")\n",
    "\n",
    "# Test 4: Cron without expression (invalid)\n",
    "try:\n",
    "    schedule4 = ScheduleConfig(type=\"cron\")\n",
    "except ValidationError as e:\n",
    "    print(f\"âŒ Cron without expression:\\n{e}\\n\")\n",
    "\n",
    "# Test 5: Interval without minutes (invalid)\n",
    "try:\n",
    "    schedule5 = ScheduleConfig(type=\"interval\")\n",
    "except ValidationError as e:\n",
    "    print(f\"âŒ Interval without minutes:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Storage Configs - Discriminated Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class StorageType(str, Enum):\n",
    "    \"\"\"Supported storage types.\"\"\"\n",
    "    S3 = \"s3\"\n",
    "    GCS = \"gcs\"\n",
    "    AZURE = \"azure\"\n",
    "    LOCAL = \"local\"\n",
    "\n",
    "class BaseStorageConfig(BaseModel):\n",
    "    \"\"\"Base configuration for all storage types.\"\"\"\n",
    "    type: StorageType\n",
    "    encryption: bool = False\n",
    "\n",
    "class S3StorageConfig(BaseStorageConfig):\n",
    "    \"\"\"AWS S3 storage configuration.\"\"\"\n",
    "    type: StorageType = StorageType.S3\n",
    "    bucket: str\n",
    "    region: str\n",
    "    access_key_id: Optional[str] = None  # Can use IAM role instead\n",
    "\n",
    "class GCSStorageConfig(BaseStorageConfig):\n",
    "    \"\"\"Google Cloud Storage configuration.\"\"\"\n",
    "    type: StorageType = StorageType.GCS\n",
    "    bucket: str\n",
    "    project: str\n",
    "\n",
    "class AzureStorageConfig(BaseStorageConfig):\n",
    "    \"\"\"Azure Blob Storage configuration.\"\"\"\n",
    "    type: StorageType = StorageType.AZURE\n",
    "    account_name: str\n",
    "    container: str\n",
    "    sas_token: Optional[str] = None  # Can use managed identity instead\n",
    "\n",
    "class LocalStorageConfig(BaseStorageConfig):\n",
    "    \"\"\"Local filesystem storage configuration.\"\"\"\n",
    "    type: StorageType = StorageType.LOCAL\n",
    "    base_path: str\n",
    "\n",
    "# Union type for polymorphic storage configs\n",
    "StorageConfig = Union[\n",
    "    S3StorageConfig,\n",
    "    GCSStorageConfig,\n",
    "    AzureStorageConfig,\n",
    "    LocalStorageConfig,\n",
    "]\n",
    "\n",
    "# Test S3\n",
    "s3 = S3StorageConfig(\n",
    "    bucket=\"my-data-bucket\",\n",
    "    region=\"us-east-1\",\n",
    "    encryption=True\n",
    ")\n",
    "print(f\"âœ… S3: {s3}\\n\")\n",
    "\n",
    "# Test GCS\n",
    "gcs = GCSStorageConfig(\n",
    "    bucket=\"my-gcs-bucket\",\n",
    "    project=\"my-project-123\"\n",
    ")\n",
    "print(f\"âœ… GCS: {gcs}\\n\")\n",
    "\n",
    "# Test Azure\n",
    "azure = AzureStorageConfig(\n",
    "    account_name=\"myaccount\",\n",
    "    container=\"data\",\n",
    "    sas_token=\"sv=2021...\"\n",
    ")\n",
    "print(f\"âœ… Azure: {azure}\\n\")\n",
    "\n",
    "# Test Local\n",
    "local = LocalStorageConfig(\n",
    "    base_path=\"/data/warehouse\"\n",
    ")\n",
    "print(f\"âœ… Local: {local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Field Validator - Cron Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_validator\n",
    "import re\n",
    "\n",
    "class ScheduleConfig(BaseModel):\n",
    "    \"\"\"Configuration for pipeline scheduling with cron validation.\"\"\"\n",
    "    type: ScheduleType\n",
    "    cron_expression: Optional[str] = None\n",
    "    interval_minutes: Optional[int] = None\n",
    "    \n",
    "    @field_validator(\"cron_expression\")\n",
    "    @classmethod\n",
    "    def validate_cron(cls, v):\n",
    "        \"\"\"Validate cron expression format.\"\"\"\n",
    "        if v is None:\n",
    "            return v\n",
    "        \n",
    "        # Split into parts\n",
    "        parts = v.split()\n",
    "        \n",
    "        # Must have exactly 5 parts\n",
    "        if len(parts) != 5:\n",
    "            raise ValueError(\n",
    "                f\"Cron expression must have exactly 5 parts (minute hour day month weekday). \"\n",
    "                f\"Got {len(parts)} parts: '{v}'. \"\n",
    "                f\"Example: '0 0 * * *' for daily at midnight.\"\n",
    "            )\n",
    "        \n",
    "        # Each part should match cron syntax\n",
    "        # Simple validation: *, number, range (1-5), step (*/5), or list (1,2,3)\n",
    "        cron_pattern = r'^(\\*|\\d+|\\d+-\\d+|\\*/\\d+|\\d+(,\\d+)*)$'\n",
    "        \n",
    "        for i, part in enumerate(parts):\n",
    "            if not re.match(cron_pattern, part):\n",
    "                field_names = [\"minute\", \"hour\", \"day\", \"month\", \"weekday\"]\n",
    "                raise ValueError(\n",
    "                    f\"Invalid cron {field_names[i]} field: '{part}'. \"\n",
    "                    f\"Must be *, a number, a range (1-5), a step (*/5), or a list (1,2,3).\"\n",
    "                )\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_schedule_requirements(self):\n",
    "        \"\"\"Validate schedule type-specific requirements.\"\"\"\n",
    "        if self.type == ScheduleType.CRON and not self.cron_expression:\n",
    "            raise ValueError(\"Schedule type 'cron' requires 'cron_expression'\")\n",
    "        if self.type == ScheduleType.INTERVAL and not self.interval_minutes:\n",
    "            raise ValueError(\"Schedule type 'interval' requires 'interval_minutes'\")\n",
    "        return self\n",
    "\n",
    "# Test valid cron expressions\n",
    "valid_crons = [\n",
    "    \"0 0 * * *\",      # Daily at midnight\n",
    "    \"*/15 * * * *\",   # Every 15 minutes\n",
    "    \"0 9-17 * * 1-5\", # Hourly, 9am-5pm, weekdays\n",
    "    \"30 2 * * 0\",     # Weekly on Sunday at 2:30am\n",
    "    \"0 0 1 * *\",      # Monthly on 1st at midnight\n",
    "]\n",
    "\n",
    "for cron in valid_crons:\n",
    "    schedule = ScheduleConfig(type=\"cron\", cron_expression=cron)\n",
    "    print(f\"âœ… Valid cron: {cron}\")\n",
    "\n",
    "# Test invalid cron expressions\n",
    "invalid_crons = [\n",
    "    \"0 0 * *\",           # Too few parts\n",
    "    \"0 0 * * * *\",       # Too many parts\n",
    "    \"abc 0 * * *\",       # Invalid character\n",
    "    \"0 0-25 * * *\",      # Invalid hour range\n",
    "]\n",
    "\n",
    "print(\"\\nTesting invalid cron expressions:\\n\")\n",
    "for cron in invalid_crons:\n",
    "    try:\n",
    "        schedule = ScheduleConfig(type=\"cron\", cron_expression=cron)\n",
    "    except ValidationError as e:\n",
    "        print(f\"âŒ '{cron}': {e.errors()[0]['msg']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6: Complete Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# First, define all necessary classes (simplified versions)\n",
    "class ReadConfig(BaseModel):\n",
    "    connection: str\n",
    "    format: str\n",
    "    table: Optional[str] = None\n",
    "    path: Optional[str] = None\n",
    "\n",
    "class TransformConfig(BaseModel):\n",
    "    steps: List[str]\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "    depends_on: List[str] = Field(default_factory=list)\n",
    "    read: Optional[ReadConfig] = None\n",
    "    transform: Optional[TransformConfig] = None\n",
    "    write: Optional[WriteConfig] = None\n",
    "\n",
    "class PipelineConfig(BaseModel):\n",
    "    \"\"\"Enhanced pipeline configuration with scheduling.\"\"\"\n",
    "    pipeline: str\n",
    "    description: Optional[str] = None\n",
    "    layer: Optional[str] = None\n",
    "    schedule: Optional[ScheduleConfig] = None  # New!\n",
    "    nodes: List[NodeConfig]\n",
    "\n",
    "# YAML configuration\n",
    "pipeline_yaml = \"\"\"\n",
    "pipeline: daily_sales_processing\n",
    "description: Process daily sales data with partitioning\n",
    "layer: silver\n",
    "\n",
    "schedule:\n",
    "  type: cron\n",
    "  cron_expression: \"0 2 * * *\"  # Daily at 2am\n",
    "\n",
    "nodes:\n",
    "  - name: read_sales\n",
    "    description: Read raw sales data from Delta\n",
    "    read:\n",
    "      connection: delta\n",
    "      format: delta\n",
    "      table: raw.sales\n",
    "  \n",
    "  - name: clean_sales\n",
    "    description: Clean and validate sales data\n",
    "    depends_on: [read_sales]\n",
    "    transform:\n",
    "      steps:\n",
    "        - SELECT * FROM read_sales WHERE amount > 0\n",
    "        - SELECT * FROM data WHERE customer_id IS NOT NULL\n",
    "  \n",
    "  - name: aggregate_by_date\n",
    "    description: Aggregate sales by date and region\n",
    "    depends_on: [clean_sales]\n",
    "    transform:\n",
    "      steps:\n",
    "        - |\n",
    "          SELECT \n",
    "            DATE(created_at) as sale_date,\n",
    "            region,\n",
    "            COUNT(*) as total_orders,\n",
    "            SUM(amount) as total_revenue\n",
    "          FROM clean_sales\n",
    "          GROUP BY DATE(created_at), region\n",
    "    write:\n",
    "      connection: delta\n",
    "      format: delta\n",
    "      table: silver.sales_daily\n",
    "      mode: overwrite\n",
    "      partition_strategy: date\n",
    "      partition_columns: [sale_date, region]\n",
    "\"\"\"\n",
    "\n",
    "# Load and validate\n",
    "config = PipelineConfig(**yaml.safe_load(pipeline_yaml))\n",
    "\n",
    "# Print summary\n",
    "print(\"=\"*60)\n",
    "print(f\"Pipeline: {config.pipeline}\")\n",
    "print(f\"Description: {config.description}\")\n",
    "print(f\"Layer: {config.layer}\")\n",
    "print(f\"\\nSchedule: {config.schedule.type}\")\n",
    "if config.schedule.cron_expression:\n",
    "    print(f\"  Cron: {config.schedule.cron_expression}\")\n",
    "\n",
    "print(f\"\\nNodes ({len(config.nodes)}):\")\n",
    "for node in config.nodes:\n",
    "    print(f\"\\n  ðŸ“¦ {node.name}\")\n",
    "    if node.description:\n",
    "        print(f\"     {node.description}\")\n",
    "    if node.depends_on:\n",
    "        print(f\"     Depends on: {', '.join(node.depends_on)}\")\n",
    "    if node.read:\n",
    "        print(f\"     Read: {node.read.format} from {node.read.table or node.read.path}\")\n",
    "    if node.transform:\n",
    "        print(f\"     Transform: {len(node.transform.steps)} steps\")\n",
    "    if node.write:\n",
    "        print(f\"     Write: {node.write.format} to {node.write.table or node.write.path}\")\n",
    "        if node.write.partition_strategy != PartitionStrategy.NONE:\n",
    "            print(f\"     Partitioned by: {node.write.partition_strategy} on {node.write.partition_columns}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 7: Error Message Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipelineConfig(BaseModel):\n",
    "    \"\"\"Example config with high-quality error messages.\"\"\"\n",
    "    name: str\n",
    "    source_connection: str\n",
    "    target_connection: str\n",
    "    available_connections: List[str] = Field(exclude=True)  # Not in YAML, set programmatically\n",
    "    partitions: int = Field(ge=1, le=1000)\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_connections(self):\n",
    "        \"\"\"Validate connections exist and are different.\"\"\"\n",
    "        # Check source exists\n",
    "        if self.available_connections and self.source_connection not in self.available_connections:\n",
    "            raise ValueError(\n",
    "                f\"Source connection '{self.source_connection}' not found. \"\n",
    "                f\"Available connections: {', '.join(self.available_connections)}. \"\n",
    "                f\"Add it to the 'connections' section of your config or fix the typo.\"\n",
    "            )\n",
    "        \n",
    "        # Check target exists\n",
    "        if self.available_connections and self.target_connection not in self.available_connections:\n",
    "            raise ValueError(\n",
    "                f\"Target connection '{self.target_connection}' not found. \"\n",
    "                f\"Available connections: {', '.join(self.available_connections)}. \"\n",
    "                f\"Add it to the 'connections' section of your config or fix the typo.\"\n",
    "            )\n",
    "        \n",
    "        # Check they're different\n",
    "        if self.source_connection == self.target_connection:\n",
    "            raise ValueError(\n",
    "                f\"Source and target connections must be different (both are '{self.source_connection}'). \"\n",
    "                f\"Reading and writing to the same connection can cause data corruption. \"\n",
    "                f\"Use different connections or add logic to prevent overwrites.\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Test error scenarios\n",
    "available = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "print(\"Testing error messages:\\n\")\n",
    "\n",
    "# Error 1: Source connection not found\n",
    "try:\n",
    "    config = DataPipelineConfig(\n",
    "        name=\"test\",\n",
    "        source_connection=\"raw\",  # Doesn't exist\n",
    "        target_connection=\"silver\",\n",
    "        partitions=10,\n",
    "        available_connections=available\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"âŒ Error 1 - Source not found:\")\n",
    "    print(f\"{e.errors()[0]['msg']}\\n\")\n",
    "\n",
    "# Error 2: Same source and target\n",
    "try:\n",
    "    config = DataPipelineConfig(\n",
    "        name=\"test\",\n",
    "        source_connection=\"silver\",\n",
    "        target_connection=\"silver\",  # Same!\n",
    "        partitions=10,\n",
    "        available_connections=available\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"âŒ Error 2 - Same connection:\")\n",
    "    print(f\"{e.errors()[0]['msg']}\\n\")\n",
    "\n",
    "# Error 3: Partitions out of range\n",
    "try:\n",
    "    config = DataPipelineConfig(\n",
    "        name=\"test\",\n",
    "        source_connection=\"bronze\",\n",
    "        target_connection=\"silver\",\n",
    "        partitions=5000,  # Too many!\n",
    "        available_connections=available\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"âŒ Error 3 - Too many partitions:\")\n",
    "    print(f\"{e.errors()[0]['msg']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 8: Config Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "def merge_configs(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Merge two config dictionaries, override takes precedence.\n",
    "    \n",
    "    Handles nested dictionaries recursively.\n",
    "    \"\"\"\n",
    "    result = deepcopy(base)\n",
    "    \n",
    "    for key, value in override.items():\n",
    "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "            # Recursively merge nested dicts\n",
    "            result[key] = merge_configs(result[key], value)\n",
    "        else:\n",
    "            # Override takes precedence\n",
    "            result[key] = deepcopy(value)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test\n",
    "base_config = {\n",
    "    \"project\": \"data_pipeline\",\n",
    "    \"logging\": {\n",
    "        \"level\": \"INFO\",\n",
    "        \"structured\": False,\n",
    "        \"metadata\": {\n",
    "            \"team\": \"data-eng\",\n",
    "            \"environment\": \"dev\"\n",
    "        }\n",
    "    },\n",
    "    \"retry\": {\n",
    "        \"enabled\": True,\n",
    "        \"max_attempts\": 3\n",
    "    },\n",
    "    \"connections\": {\n",
    "        \"local\": {\n",
    "            \"type\": \"local\",\n",
    "            \"base_path\": \"./data\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Production overrides\n",
    "prod_override = {\n",
    "    \"logging\": {\n",
    "        \"level\": \"ERROR\",\n",
    "        \"structured\": True,\n",
    "        \"metadata\": {\n",
    "            \"environment\": \"production\"  # Only override environment, keep team\n",
    "        }\n",
    "    },\n",
    "    \"retry\": {\n",
    "        \"max_attempts\": 5  # More retries in prod\n",
    "    },\n",
    "    \"connections\": {\n",
    "        \"delta\": {  # Add new connection\n",
    "            \"type\": \"delta\",\n",
    "            \"catalog\": \"prod\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "merged = merge_configs(base_config, prod_override)\n",
    "\n",
    "print(\"Base Config:\")\n",
    "print(yaml.dump(base_config, default_flow_style=False))\n",
    "\n",
    "print(\"\\nProduction Override:\")\n",
    "print(yaml.dump(prod_override, default_flow_style=False))\n",
    "\n",
    "print(\"\\nMerged Config:\")\n",
    "print(yaml.dump(merged, default_flow_style=False))\n",
    "\n",
    "# Verify specific merges\n",
    "assert merged[\"logging\"][\"level\"] == \"ERROR\"  # Overridden\n",
    "assert merged[\"logging\"][\"structured\"] == True  # Overridden\n",
    "assert merged[\"logging\"][\"metadata\"][\"team\"] == \"data-eng\"  # Kept from base\n",
    "assert merged[\"logging\"][\"metadata\"][\"environment\"] == \"production\"  # Overridden\n",
    "assert merged[\"retry\"][\"enabled\"] == True  # Kept from base\n",
    "assert merged[\"retry\"][\"max_attempts\"] == 5  # Overridden\n",
    "assert \"local\" in merged[\"connections\"]  # Kept from base\n",
    "assert \"delta\" in merged[\"connections\"]  # Added from override\n",
    "\n",
    "print(\"\\nâœ… All assertions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 9: Config Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_write_config_valid():\n",
    "    \"\"\"Test valid WriteConfig.\"\"\"\n",
    "    config = WriteConfig(\n",
    "        connection=\"delta\",\n",
    "        format=\"delta\",\n",
    "        table=\"sales\",\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "    assert config.connection == \"delta\"\n",
    "    assert config.table == \"sales\"\n",
    "    assert config.mode == WriteMode.OVERWRITE\n",
    "    print(\"âœ… test_write_config_valid passed\")\n",
    "\n",
    "def test_write_config_missing_table_and_path():\n",
    "    \"\"\"Test WriteConfig with neither table nor path fails.\"\"\"\n",
    "    try:\n",
    "        config = WriteConfig(\n",
    "            connection=\"local\",\n",
    "            format=\"csv\"\n",
    "            # Missing both table and path!\n",
    "        )\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError as e:\n",
    "        assert \"table\" in str(e).lower() or \"path\" in str(e).lower()\n",
    "        print(\"âœ… test_write_config_missing_table_and_path passed\")\n",
    "\n",
    "def test_schedule_config_cron_valid():\n",
    "    \"\"\"Test valid cron schedule.\"\"\"\n",
    "    config = ScheduleConfig(\n",
    "        type=\"cron\",\n",
    "        cron_expression=\"0 0 * * *\"\n",
    "    )\n",
    "    assert config.type == ScheduleType.CRON\n",
    "    assert config.cron_expression == \"0 0 * * *\"\n",
    "    print(\"âœ… test_schedule_config_cron_valid passed\")\n",
    "\n",
    "def test_schedule_config_cron_missing_expression():\n",
    "    \"\"\"Test cron schedule without expression fails.\"\"\"\n",
    "    try:\n",
    "        config = ScheduleConfig(type=\"cron\")\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError as e:\n",
    "        assert \"cron_expression\" in str(e).lower()\n",
    "        print(\"âœ… test_schedule_config_cron_missing_expression passed\")\n",
    "\n",
    "def test_schedule_config_invalid_cron():\n",
    "    \"\"\"Test invalid cron expression fails.\"\"\"\n",
    "    try:\n",
    "        config = ScheduleConfig(\n",
    "            type=\"cron\",\n",
    "            cron_expression=\"invalid\"\n",
    "        )\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError:\n",
    "        print(\"âœ… test_schedule_config_invalid_cron passed\")\n",
    "\n",
    "def test_partition_config_requires_columns():\n",
    "    \"\"\"Test partitioning requires columns.\"\"\"\n",
    "    try:\n",
    "        config = WriteConfig(\n",
    "            connection=\"delta\",\n",
    "            format=\"delta\",\n",
    "            table=\"sales\",\n",
    "            partition_strategy=\"hash\"\n",
    "            # Missing partition_columns!\n",
    "        )\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError as e:\n",
    "        assert \"partition\" in str(e).lower()\n",
    "        print(\"âœ… test_partition_config_requires_columns passed\")\n",
    "\n",
    "# Run all tests\n",
    "print(\"Running config tests...\\n\")\n",
    "test_write_config_valid()\n",
    "test_write_config_missing_table_and_path()\n",
    "test_schedule_config_cron_valid()\n",
    "test_schedule_config_cron_missing_expression()\n",
    "test_schedule_config_invalid_cron()\n",
    "test_partition_config_requires_columns()\n",
    "print(\"\\nðŸŽ‰ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "These solutions demonstrate:\n",
    "\n",
    "1. **Enum Design**: Clear, type-safe constants\n",
    "2. **Field Validators**: Validate individual fields with regex, constraints\n",
    "3. **Model Validators**: Cross-field validation and business rules\n",
    "4. **Discriminated Unions**: Polymorphic configs based on type\n",
    "5. **Error Messages**: Clear, actionable, context-rich\n",
    "6. **Config Merging**: Environment-specific overrides\n",
    "7. **Testing**: Comprehensive validation of both success and failure cases\n",
    "\n",
    "**Production Patterns**:\n",
    "- Always validate early (load-time, not runtime)\n",
    "- Provide helpful defaults\n",
    "- Use `default_factory` for mutable defaults\n",
    "- Write clear error messages with suggestions\n",
    "- Test both valid and invalid configs\n",
    "- Document fields with descriptions\n",
    "\n",
    "These patterns are used throughout Odibi and in production data platforms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
