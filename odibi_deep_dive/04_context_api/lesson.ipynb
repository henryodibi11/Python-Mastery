{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Context API: Data Flow Between Nodes\n",
    "\n",
    "## ðŸŽ¯ The Problem: Passing DataFrames Without Globals\n",
    "\n",
    "**Bad approach:**\n",
    "```python\n",
    "# Global state - hard to test, race conditions\n",
    "global_data = {}\n",
    "\n",
    "def node_a():\n",
    "    global_data['customers'] = pd.read_csv('customers.csv')\n",
    "\n",
    "def node_b():\n",
    "    df = global_data['customers']  # Implicit dependency!\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- Hidden dependencies (what data does node_b need?)\n",
    "- Hard to test in isolation\n",
    "- Race conditions in parallel execution\n",
    "- Memory leaks (when to clear?)\n",
    "\n",
    "**Odibi's solution:** Explicit context passed to each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¦‰ First Principles\n",
    "\n",
    "### Dependency Injection\n",
    "Don't access global state - receive dependencies as parameters.\n",
    "\n",
    "```python\n",
    "# Instead of:\n",
    "def transform(): return global_data['input']\n",
    "\n",
    "# Do:\n",
    "def transform(ctx: Context): return ctx.get('input')\n",
    "```\n",
    "\n",
    "### Interface Segregation\n",
    "Context provides minimal interface: register, get, has, list_names, clear\n",
    "\n",
    "### Polymorphism\n",
    "Same interface works for Pandas (dict storage) and Spark (temp views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Reading Odibi Code: context.py\n",
    "\n",
    "Let's analyze the actual implementation from Odibi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'c:\\Users\\hodibi\\OneDrive - Ingredion\\Desktop\\Repos\\Odibi')\n",
    "\n",
    "from odibi.context import Context, PandasContext, SparkContext, create_context\n",
    "import pandas as pd\n",
    "import inspect\n",
    "\n",
    "# View the Context ABC\n",
    "print(\"Context ABC Methods:\")\n",
    "print(inspect.getsource(Context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Design Points\n",
    "\n",
    "1. **Abstract Base Class**: Forces all implementations to provide the 5 core methods\n",
    "2. **Type Hints**: `Any` allows both Pandas and Spark DataFrames\n",
    "3. **Explicit Errors**: `KeyError` with helpful messages shows available names\n",
    "4. **Lifecycle**: `clear()` enables cleanup between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine PandasContext implementation\n",
    "print(\"PandasContext Implementation:\")\n",
    "print(inspect.getsource(PandasContext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PandasContext Analysis\n",
    "\n",
    "**Storage**: Simple dict `self._data: Dict[str, pd.DataFrame]`\n",
    "\n",
    "**Type Safety**: \n",
    "```python\n",
    "if not isinstance(df, pd.DataFrame):\n",
    "    raise TypeError(f\"Expected pandas.DataFrame, got {type(df)}\")\n",
    "```\n",
    "\n",
    "**Error Messages**: Shows available names when key not found\n",
    "```python\n",
    "available = \", \".join(self._data.keys()) if self._data else \"none\"\n",
    "raise KeyError(f\"DataFrame '{name}' not found. Available: {available}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PandasContext\n",
    "ctx = PandasContext()\n",
    "\n",
    "# Register some data\n",
    "df = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n",
    "ctx.register('test_data', df)\n",
    "\n",
    "print(\"Registered:\", ctx.list_names())\n",
    "print(\"Has 'test_data':\", ctx.has('test_data'))\n",
    "print(\"Retrieved:\\n\", ctx.get('test_data'))\n",
    "\n",
    "# Try to get non-existent data\n",
    "try:\n",
    "    ctx.get('missing')\n",
    "except KeyError as e:\n",
    "    print(f\"\\nExpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine SparkContext\n",
    "print(\"SparkContext Implementation:\")\n",
    "print(inspect.getsource(SparkContext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext Analysis\n",
    "\n",
    "**Storage**: Temp views in Spark catalog (not in-memory dict!)\n",
    "```python\n",
    "df.createOrReplaceTempView(name)\n",
    "self._registered_views.add(name)  # Track names only\n",
    "```\n",
    "\n",
    "**Retrieval**: Query from catalog\n",
    "```python\n",
    "return self.spark.table(name)\n",
    "```\n",
    "\n",
    "**Cleanup**: Drop temp views\n",
    "```python\n",
    "self.spark.catalog.dropTempView(name)\n",
    "```\n",
    "\n",
    "**Why different from Pandas?**\n",
    "- Spark DataFrames are lazy (execution plans)\n",
    "- Temp views integrate with Spark SQL catalog\n",
    "- Avoid serialization overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory pattern\n",
    "print(\"create_context Factory:\")\n",
    "print(inspect.getsource(create_context))\n",
    "\n",
    "# Use the factory\n",
    "pandas_ctx = create_context('pandas')\n",
    "print(f\"\\nCreated: {type(pandas_ctx).__name__}\")\n",
    "\n",
    "# Spark requires session\n",
    "try:\n",
    "    spark_ctx = create_context('spark')\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Build: CachingContext with LRU Eviction\n",
    "\n",
    "Let's extend PandasContext with:\n",
    "- Max size limit\n",
    "- LRU (Least Recently Used) eviction\n",
    "- Access tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "class CachingContext(Context):\n",
    "    \"\"\"Context with LRU eviction policy.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size: Maximum number of DataFrames to cache\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self._data: OrderedDict[str, pd.DataFrame] = OrderedDict()\n",
    "        self._eviction_count = 0\n",
    "    \n",
    "    def register(self, name: str, df: pd.DataFrame) -> None:\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"Expected pandas.DataFrame, got {type(df)}\")\n",
    "        \n",
    "        # Remove if exists (to update position)\n",
    "        if name in self._data:\n",
    "            del self._data[name]\n",
    "        \n",
    "        # Evict LRU if at capacity\n",
    "        if len(self._data) >= self.max_size:\n",
    "            evicted_name, _ = self._data.popitem(last=False)  # FIFO = LRU\n",
    "            self._eviction_count += 1\n",
    "            print(f\"âš ï¸  Evicted '{evicted_name}' (LRU)\")\n",
    "        \n",
    "        # Add to end (most recent)\n",
    "        self._data[name] = df\n",
    "    \n",
    "    def get(self, name: str) -> pd.DataFrame:\n",
    "        if name not in self._data:\n",
    "            available = \", \".join(self._data.keys()) if self._data else \"none\"\n",
    "            raise KeyError(f\"DataFrame '{name}' not found. Available: {available}\")\n",
    "        \n",
    "        # Move to end (mark as recently used)\n",
    "        self._data.move_to_end(name)\n",
    "        return self._data[name]\n",
    "    \n",
    "    def has(self, name: str) -> bool:\n",
    "        return name in self._data\n",
    "    \n",
    "    def list_names(self) -> list[str]:\n",
    "        return list(self._data.keys())\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self._data.clear()\n",
    "        self._eviction_count = 0\n",
    "    \n",
    "    def stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        return {\n",
    "            'size': len(self._data),\n",
    "            'max_size': self.max_size,\n",
    "            'evictions': self._eviction_count\n",
    "        }\n",
    "\n",
    "print(\"âœ… CachingContext implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LRU eviction\n",
    "ctx = CachingContext(max_size=3)\n",
    "\n",
    "# Register 3 DataFrames\n",
    "for i in range(3):\n",
    "    ctx.register(f'df{i}', pd.DataFrame({'x': [i]}))\n",
    "\n",
    "print(\"After 3 registers:\", ctx.list_names())\n",
    "print(\"Stats:\", ctx.stats())\n",
    "\n",
    "# Access df0 (make it recent)\n",
    "_ = ctx.get('df0')\n",
    "print(\"\\nAfter accessing df0:\", ctx.list_names())\n",
    "\n",
    "# Register df3 - should evict df1 (least recent)\n",
    "print(\"\\nRegistering df3...\")\n",
    "ctx.register('df3', pd.DataFrame({'x': [3]}))\n",
    "print(\"After df3:\", ctx.list_names())\n",
    "print(\"Stats:\", ctx.stats())\n",
    "\n",
    "# Verify df1 was evicted\n",
    "print(f\"\\nHas df1: {ctx.has('df1')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Testing: Context Isolation\n",
    "\n",
    "Critical pattern: Each test gets a fresh context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def fresh_context():\n",
    "    \"\"\"Provide isolated context for each test.\"\"\"\n",
    "    ctx = PandasContext()\n",
    "    yield ctx\n",
    "    ctx.clear()  # Cleanup after test\n",
    "\n",
    "# Simulated tests (run with pytest)\n",
    "def test_register_and_get(fresh_context):\n",
    "    df = pd.DataFrame({'a': [1, 2]})\n",
    "    fresh_context.register('test', df)\n",
    "    \n",
    "    retrieved = fresh_context.get('test')\n",
    "    assert retrieved.equals(df)\n",
    "\n",
    "def test_isolation(fresh_context):\n",
    "    # This test gets a NEW context (empty)\n",
    "    assert fresh_context.list_names() == []\n",
    "\n",
    "print(\"âœ… Test patterns defined\")\n",
    "print(\"Run with: pytest lesson.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Memory Management Patterns\n",
    "\n",
    "### Pattern 1: Context as Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def managed_context(engine='pandas'):\n",
    "    \"\"\"Auto-cleanup context.\"\"\"\n",
    "    ctx = create_context(engine)\n",
    "    try:\n",
    "        yield ctx\n",
    "    finally:\n",
    "        ctx.clear()\n",
    "        print(\"ðŸ§¹ Context cleaned\")\n",
    "\n",
    "# Usage\n",
    "with managed_context() as ctx:\n",
    "    ctx.register('temp', pd.DataFrame({'x': [1, 2, 3]}))\n",
    "    print(\"Inside:\", ctx.list_names())\n",
    "\n",
    "# Outside context - data is cleared\n",
    "print(\"Outside: (ctx cleared)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Checkpoint and Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointableContext(PandasContext):\n",
    "    \"\"\"Context that can save/restore state.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._checkpoints = []\n",
    "    \n",
    "    def checkpoint(self) -> None:\n",
    "        \"\"\"Save current state.\"\"\"\n",
    "        # Deep copy the data dict\n",
    "        self._checkpoints.append({k: v.copy() for k, v in self._data.items()})\n",
    "    \n",
    "    def restore(self) -> None:\n",
    "        \"\"\"Restore last checkpoint.\"\"\"\n",
    "        if not self._checkpoints:\n",
    "            raise RuntimeError(\"No checkpoints available\")\n",
    "        self._data = self._checkpoints.pop()\n",
    "\n",
    "# Demo\n",
    "ctx = CheckpointableContext()\n",
    "ctx.register('initial', pd.DataFrame({'x': [1]}))\n",
    "print(\"Initial:\", ctx.list_names())\n",
    "\n",
    "ctx.checkpoint()\n",
    "ctx.register('added', pd.DataFrame({'y': [2]}))\n",
    "print(\"After adding:\", ctx.list_names())\n",
    "\n",
    "ctx.restore()\n",
    "print(\"After restore:\", ctx.list_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Context in Transform Functions\n",
    "\n",
    "How context flows through a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical Odibi node pattern\n",
    "def load_customers(ctx: Context) -> pd.DataFrame:\n",
    "    \"\"\"Load and register customers.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': [1, 2, 3],\n",
    "        'name': ['Alice', 'Bob', 'Carol']\n",
    "    })\n",
    "    ctx.register('customers', df)\n",
    "    return df\n",
    "\n",
    "def load_orders(ctx: Context) -> pd.DataFrame:\n",
    "    \"\"\"Load and register orders.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'order_id': [101, 102],\n",
    "        'customer_id': [1, 2],\n",
    "        'amount': [100, 200]\n",
    "    })\n",
    "    ctx.register('orders', df)\n",
    "    return df\n",
    "\n",
    "def join_data(ctx: Context) -> pd.DataFrame:\n",
    "    \"\"\"Join customers and orders.\"\"\"\n",
    "    customers = ctx.get('customers')  # Explicit dependency!\n",
    "    orders = ctx.get('orders')\n",
    "    \n",
    "    result = orders.merge(customers, on='customer_id')\n",
    "    ctx.register('customer_orders', result)\n",
    "    return result\n",
    "\n",
    "# Execute pipeline\n",
    "ctx = PandasContext()\n",
    "load_customers(ctx)\n",
    "load_orders(ctx)\n",
    "result = join_data(ctx)\n",
    "\n",
    "print(\"Pipeline result:\")\n",
    "print(result)\n",
    "print(\"\\nContext contains:\", ctx.list_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary\n",
    "\n",
    "### Context Design Principles\n",
    "1. **Explicit dependencies**: No globals, pass context to functions\n",
    "2. **Polymorphic**: Same interface for Pandas/Spark\n",
    "3. **Isolated**: Fresh context per pipeline run\n",
    "4. **Lifecycle management**: Clear when done\n",
    "\n",
    "### Implementation Strategies\n",
    "- **PandasContext**: Dict-based, simple and fast\n",
    "- **SparkContext**: Temp views, integrates with catalog\n",
    "- **Extensions**: Caching, checkpoints, LRU eviction\n",
    "\n",
    "### Best Practices\n",
    "- Use `has()` before `get()` for optional data\n",
    "- Always `clear()` in test teardown\n",
    "- Prefer context managers for auto-cleanup\n",
    "- Name DataFrames descriptively\n",
    "\n",
    "### Next Steps\n",
    "â†’ `exercises.ipynb` - Practice implementing context features  \n",
    "â†’ `context_patterns.md` - Best practices reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
