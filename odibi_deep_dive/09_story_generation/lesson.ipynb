{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generation Deep Dive\n",
    "\n",
    "## Introduction\n",
    "This lesson explores ODIBI's Story Generation system - the automatic documentation engine that transforms pipeline execution into rich, shareable documentation.\n",
    "\n",
    "### What You'll Learn\n",
    "1. StoryGenerator architecture and markdown generation\n",
    "2. NodeExecutionMetadata - tracking detailed node metrics\n",
    "3. PipelineStoryMetadata - aggregating pipeline-level data\n",
    "4. Multi-format renderers (Markdown, HTML, JSON)\n",
    "5. Theme system with customization and branding\n",
    "6. Story content patterns (schemas, row counts, durations)\n",
    "7. Testing and validating stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: StoryGenerator Architecture\n",
    "\n",
    "### Basic Story Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.generator import StoryGenerator\n",
    "from odibi.node import NodeResult\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# StoryGenerator creates markdown documentation from pipeline runs\n",
    "class StoryGeneratorBasics:\n",
    "    \"\"\"Understanding StoryGenerator initialization.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def show_structure():\n",
    "        print(\"\"\"\n",
    "StoryGenerator Configuration:\n",
    "\n",
    "1. pipeline_name: str\n",
    "   - Name of the pipeline being documented\n",
    "   - Used in headers and filenames\n",
    "   \n",
    "2. max_sample_rows: int (default: 10)\n",
    "   - Maximum rows to include in data samples\n",
    "   - Balances detail vs. readability\n",
    "   \n",
    "3. output_path: str (default: \"stories/\")\n",
    "   - Directory for generated story files\n",
    "   - Auto-created if doesn't exist\n",
    "        \"\"\")\n",
    "\n",
    "StoryGeneratorBasics.show_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a story from pipeline execution\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create temp directory for stories\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Initialize generator\n",
    "generator = StoryGenerator(\n",
    "    pipeline_name=\"data_pipeline\",\n",
    "    max_sample_rows=5,\n",
    "    output_path=temp_dir\n",
    ")\n",
    "\n",
    "# Mock node results\n",
    "node_results = {\n",
    "    \"extract\": NodeResult(\n",
    "        success=True,\n",
    "        duration=1.23,\n",
    "        result_schema=[\"id\", \"name\", \"value\"],\n",
    "        rows_processed=100,\n",
    "        metadata={\"steps\": [\"Connected to source\", \"Loaded data\"]}\n",
    "    ),\n",
    "    \"transform\": NodeResult(\n",
    "        success=True,\n",
    "        duration=0.45,\n",
    "        result_schema=[\"id\", \"name\", \"value\", \"category\"],\n",
    "        rows_processed=98,\n",
    "        metadata={\"steps\": [\"Filtered nulls\", \"Added category\"]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# Generate story\n",
    "story_path = generator.generate(\n",
    "    node_results=node_results,\n",
    "    completed=[\"extract\", \"transform\"],\n",
    "    failed=[],\n",
    "    skipped=[],\n",
    "    duration=1.68,\n",
    "    start_time=\"2024-01-15T10:30:00\",\n",
    "    end_time=\"2024-01-15T10:30:01\"\n",
    ")\n",
    "\n",
    "print(f\"Story generated: {story_path}\")\n",
    "print(f\"\\nFirst 500 chars:\")\n",
    "with open(story_path, 'r') as f:\n",
    "    print(f.read()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: NodeExecutionMetadata - Tracking Metrics\n",
    "\n",
    "### Node-Level Metadata Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.metadata import NodeExecutionMetadata\n",
    "\n",
    "# NodeExecutionMetadata captures detailed execution information\n",
    "node_meta = NodeExecutionMetadata(\n",
    "    node_name=\"transform_sales\",\n",
    "    operation=\"filter_and_aggregate\",\n",
    "    status=\"success\",\n",
    "    duration=2.45,\n",
    "    rows_in=1000,\n",
    "    rows_out=850,\n",
    "    schema_in=[\"date\", \"product\", \"amount\", \"region\"],\n",
    "    schema_out=[\"date\", \"product\", \"total_amount\", \"region\", \"category\"],\n",
    "    started_at=\"2024-01-15T10:30:00\",\n",
    "    completed_at=\"2024-01-15T10:30:02\"\n",
    ")\n",
    "\n",
    "# Calculate metrics automatically\n",
    "node_meta.calculate_row_change()\n",
    "node_meta.calculate_schema_changes()\n",
    "\n",
    "print(\"Node Execution Metadata:\")\n",
    "print(f\"  Row Change: {node_meta.rows_change} ({node_meta.rows_change_pct:.1f}%)\")\n",
    "print(f\"  Columns Added: {node_meta.columns_added}\")\n",
    "print(f\"  Columns Removed: {node_meta.columns_removed}\")\n",
    "print(f\"\\nAs Dictionary:\")\n",
    "import json\n",
    "print(json.dumps(node_meta.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tracking schema transformations\n",
    "def demonstrate_schema_tracking():\n",
    "    \"\"\"Show how schema changes are automatically detected.\"\"\"\n",
    "    \n",
    "    # Example 1: Column addition\n",
    "    meta1 = NodeExecutionMetadata(\n",
    "        node_name=\"add_features\",\n",
    "        operation=\"feature_engineering\",\n",
    "        status=\"success\",\n",
    "        duration=1.0,\n",
    "        schema_in=[\"id\", \"value\"],\n",
    "        schema_out=[\"id\", \"value\", \"value_squared\", \"value_log\"]\n",
    "    )\n",
    "    meta1.calculate_schema_changes()\n",
    "    \n",
    "    print(\"Example 1: Adding Columns\")\n",
    "    print(f\"  Added: {meta1.columns_added}\")\n",
    "    print(f\"  Removed: {meta1.columns_removed}\")\n",
    "    \n",
    "    # Example 2: Column removal (filtering)\n",
    "    meta2 = NodeExecutionMetadata(\n",
    "        node_name=\"select_columns\",\n",
    "        operation=\"column_selection\",\n",
    "        status=\"success\",\n",
    "        duration=0.5,\n",
    "        schema_in=[\"id\", \"name\", \"temp_col\", \"debug_flag\", \"value\"],\n",
    "        schema_out=[\"id\", \"name\", \"value\"]\n",
    "    )\n",
    "    meta2.calculate_schema_changes()\n",
    "    \n",
    "    print(\"\\nExample 2: Removing Columns\")\n",
    "    print(f\"  Added: {meta2.columns_added}\")\n",
    "    print(f\"  Removed: {meta2.columns_removed}\")\n",
    "    \n",
    "    # Example 3: Both add and remove\n",
    "    meta3 = NodeExecutionMetadata(\n",
    "        node_name=\"reshape_data\",\n",
    "        operation=\"transformation\",\n",
    "        status=\"success\",\n",
    "        duration=1.5,\n",
    "        schema_in=[\"date\", \"product_id\", \"qty\", \"price\"],\n",
    "        schema_out=[\"timestamp\", \"product_id\", \"revenue\", \"category\"]\n",
    "    )\n",
    "    meta3.calculate_schema_changes()\n",
    "    \n",
    "    print(\"\\nExample 3: Complex Schema Change\")\n",
    "    print(f\"  Added: {meta3.columns_added}\")\n",
    "    print(f\"  Removed: {meta3.columns_removed}\")\n",
    "\n",
    "demonstrate_schema_tracking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PipelineStoryMetadata - Aggregation\n",
    "\n",
    "### Pipeline-Level Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.metadata import PipelineStoryMetadata, NodeExecutionMetadata\n",
    "\n",
    "# PipelineStoryMetadata aggregates all node executions\n",
    "pipeline_meta = PipelineStoryMetadata(\n",
    "    pipeline_name=\"sales_etl\",\n",
    "    pipeline_layer=\"bronze_to_silver\",\n",
    "    project=\"analytics\",\n",
    "    plant=\"chicago\",\n",
    "    business_unit=\"operations\",\n",
    "    theme=\"corporate\"\n",
    ")\n",
    "\n",
    "# Add successful node\n",
    "pipeline_meta.add_node(NodeExecutionMetadata(\n",
    "    node_name=\"extract\",\n",
    "    operation=\"read_csv\",\n",
    "    status=\"success\",\n",
    "    duration=1.2,\n",
    "    rows_out=1000\n",
    "))\n",
    "\n",
    "# Add failed node\n",
    "pipeline_meta.add_node(NodeExecutionMetadata(\n",
    "    node_name=\"transform\",\n",
    "    operation=\"complex_join\",\n",
    "    status=\"failed\",\n",
    "    duration=0.8,\n",
    "    error_message=\"Key column 'id' not found\",\n",
    "    error_type=\"KeyError\"\n",
    "))\n",
    "\n",
    "# Add skipped node\n",
    "pipeline_meta.add_node(NodeExecutionMetadata(\n",
    "    node_name=\"load\",\n",
    "    operation=\"write_parquet\",\n",
    "    status=\"skipped\",\n",
    "    duration=0.0\n",
    "))\n",
    "\n",
    "# Complete the pipeline\n",
    "pipeline_meta.completed_at = \"2024-01-15T10:32:00\"\n",
    "pipeline_meta.duration = 2.0\n",
    "\n",
    "# Calculate metrics\n",
    "print(f\"Pipeline: {pipeline_meta.pipeline_name}\")\n",
    "print(f\"Success Rate: {pipeline_meta.get_success_rate():.1f}%\")\n",
    "print(f\"Total Rows: {pipeline_meta.get_total_rows_processed():,}\")\n",
    "print(f\"Status: {pipeline_meta.completed_nodes}C / {pipeline_meta.failed_nodes}F / {pipeline_meta.skipped_nodes}S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Renderers - Multi-Format Output\n",
    "\n",
    "### Markdown Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.renderers import MarkdownStoryRenderer\n",
    "from odibi.story.metadata import PipelineStoryMetadata, NodeExecutionMetadata\n",
    "\n",
    "# Create sample pipeline metadata\n",
    "metadata = PipelineStoryMetadata(\n",
    "    pipeline_name=\"customer_analytics\",\n",
    "    pipeline_layer=\"silver\",\n",
    "    started_at=\"2024-01-15T10:00:00\",\n",
    "    completed_at=\"2024-01-15T10:05:00\",\n",
    "    duration=300.5,\n",
    "    project=\"customer_360\",\n",
    "    plant=\"headquarters\"\n",
    ")\n",
    "\n",
    "# Add nodes with calculated metrics\n",
    "node1 = NodeExecutionMetadata(\n",
    "    node_name=\"load_customers\",\n",
    "    operation=\"read_database\",\n",
    "    status=\"success\",\n",
    "    duration=120.5,\n",
    "    rows_in=0,\n",
    "    rows_out=50000,\n",
    "    schema_out=[\"customer_id\", \"name\", \"email\", \"signup_date\"]\n",
    ")\n",
    "node1.calculate_row_change()\n",
    "metadata.add_node(node1)\n",
    "\n",
    "node2 = NodeExecutionMetadata(\n",
    "    node_name=\"enrich_data\",\n",
    "    operation=\"join_and_transform\",\n",
    "    status=\"success\",\n",
    "    duration=180.0,\n",
    "    rows_in=50000,\n",
    "    rows_out=48500,\n",
    "    schema_in=[\"customer_id\", \"name\", \"email\", \"signup_date\"],\n",
    "    schema_out=[\"customer_id\", \"name\", \"email\", \"signup_date\", \"segment\", \"ltv\"]\n",
    ")\n",
    "node2.calculate_row_change()\n",
    "node2.calculate_schema_changes()\n",
    "metadata.add_node(node2)\n",
    "\n",
    "# Render as markdown\n",
    "renderer = MarkdownStoryRenderer()\n",
    "markdown_story = renderer.render(metadata)\n",
    "\n",
    "print(\"Markdown Story (first 1000 chars):\")\n",
    "print(markdown_story[:1000])\n",
    "print(\"\\n... (truncated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.renderers import JSONStoryRenderer\n",
    "\n",
    "# JSON renderer for machine-readable output\n",
    "json_renderer = JSONStoryRenderer()\n",
    "json_story = json_renderer.render(metadata)\n",
    "\n",
    "print(\"JSON Story:\")\n",
    "print(json_story[:800])\n",
    "print(\"\\n... (truncated)\")\n",
    "\n",
    "# Parse and analyze\n",
    "import json\n",
    "story_dict = json.loads(json_story)\n",
    "print(f\"\\nParsed JSON structure:\")\n",
    "print(f\"  Pipeline: {story_dict['pipeline_name']}\")\n",
    "print(f\"  Nodes: {len(story_dict['nodes'])}\")\n",
    "print(f\"  Success rate: {story_dict['success_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Renderer (if Jinja2 available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML renderer requires jinja2 and template file\n",
    "try:\n",
    "    from odibi.story.renderers import HTMLStoryRenderer\n",
    "    \n",
    "    # Note: HTML renderer requires template file\n",
    "    # In practice, you'd use: renderer = HTMLStoryRenderer()\n",
    "    print(\"HTML renderer available\")\n",
    "    print(\"\\nHTML rendering creates:\")\n",
    "    print(\"  - Professional, responsive design\")\n",
    "    print(\"  - Interactive collapsible sections\")\n",
    "    print(\"  - Color-coded status indicators\")\n",
    "    print(\"  - Summary dashboards\")\n",
    "    print(\"  - Printable reports\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"HTML rendering requires: pip install jinja2\")\n",
    "except Exception as e:\n",
    "    print(f\"HTML renderer note: {e}\")\n",
    "    print(\"(Template file required for actual rendering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Theme System\n",
    "\n",
    "### Built-in Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.themes import (\n",
    "    StoryTheme, \n",
    "    DEFAULT_THEME, \n",
    "    CORPORATE_THEME, \n",
    "    DARK_THEME,\n",
    "    MINIMAL_THEME,\n",
    "    list_themes,\n",
    "    get_theme\n",
    ")\n",
    "\n",
    "# List all built-in themes\n",
    "themes = list_themes()\n",
    "print(\"Built-in Themes:\")\n",
    "for name, theme in themes.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Primary: {theme.primary_color}\")\n",
    "    print(f\"  Success: {theme.success_color}\")\n",
    "    print(f\"  Error: {theme.error_color}\")\n",
    "    print(f\"  Font: {theme.font_family[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom theme\n",
    "custom_theme = StoryTheme(\n",
    "    name=\"ingredion_brand\",\n",
    "    primary_color=\"#006837\",  # Ingredion green\n",
    "    success_color=\"#2e7d32\",\n",
    "    error_color=\"#c62828\",\n",
    "    warning_color=\"#f57c00\",\n",
    "    bg_color=\"#ffffff\",\n",
    "    text_color=\"#212121\",\n",
    "    font_family=\"'Open Sans', Arial, sans-serif\",\n",
    "    heading_font=\"'Roboto', sans-serif\",\n",
    "    company_name=\"Ingredion Incorporated\",\n",
    "    footer_text=\"Confidential - Internal Use Only\",\n",
    "    max_width=\"1400px\"\n",
    ")\n",
    "\n",
    "# Generate CSS\n",
    "css = custom_theme.to_css_string()\n",
    "print(\"Custom Theme CSS:\")\n",
    "print(css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme with custom CSS\n",
    "theme_with_custom_css = StoryTheme(\n",
    "    name=\"custom_styled\",\n",
    "    primary_color=\"#0066cc\",\n",
    "    custom_css=\"\"\"\n",
    "    .node-header {\n",
    "        border-left: 4px solid var(--primary-color);\n",
    "        padding-left: 12px;\n",
    "    }\n",
    "    \n",
    "    .summary {\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        padding: 20px;\n",
    "        border-radius: 8px;\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"Theme with custom CSS:\")\n",
    "print(theme_with_custom_css.to_css_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Story Content Patterns\n",
    "\n",
    "### Tracking Row Counts and Data Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building a complete data lineage story\n",
    "def build_data_lineage_story():\n",
    "    \"\"\"Demonstrate tracking data through pipeline.\"\"\"\n",
    "    \n",
    "    metadata = PipelineStoryMetadata(\n",
    "        pipeline_name=\"data_quality_pipeline\",\n",
    "        pipeline_layer=\"data_quality\",\n",
    "        started_at=\"2024-01-15T09:00:00\"\n",
    "    )\n",
    "    \n",
    "    # Step 1: Extract\n",
    "    extract = NodeExecutionMetadata(\n",
    "        node_name=\"extract_raw_data\",\n",
    "        operation=\"database_read\",\n",
    "        status=\"success\",\n",
    "        duration=5.2,\n",
    "        rows_in=0,\n",
    "        rows_out=100000,\n",
    "        schema_out=[\"id\", \"timestamp\", \"value\", \"sensor_id\", \"quality_flag\"]\n",
    "    )\n",
    "    extract.calculate_row_change()\n",
    "    metadata.add_node(extract)\n",
    "    \n",
    "    # Step 2: Remove duplicates\n",
    "    dedupe = NodeExecutionMetadata(\n",
    "        node_name=\"remove_duplicates\",\n",
    "        operation=\"distinct\",\n",
    "        status=\"success\",\n",
    "        duration=2.1,\n",
    "        rows_in=100000,\n",
    "        rows_out=95000,\n",
    "        schema_in=[\"id\", \"timestamp\", \"value\", \"sensor_id\", \"quality_flag\"],\n",
    "        schema_out=[\"id\", \"timestamp\", \"value\", \"sensor_id\", \"quality_flag\"]\n",
    "    )\n",
    "    dedupe.calculate_row_change()\n",
    "    dedupe.calculate_schema_changes()\n",
    "    metadata.add_node(dedupe)\n",
    "    \n",
    "    # Step 3: Filter bad quality\n",
    "    filter_node = NodeExecutionMetadata(\n",
    "        node_name=\"filter_quality\",\n",
    "        operation=\"filter\",\n",
    "        status=\"success\",\n",
    "        duration=1.5,\n",
    "        rows_in=95000,\n",
    "        rows_out=85000,\n",
    "        schema_in=[\"id\", \"timestamp\", \"value\", \"sensor_id\", \"quality_flag\"],\n",
    "        schema_out=[\"id\", \"timestamp\", \"value\", \"sensor_id\"]\n",
    "    )\n",
    "    filter_node.calculate_row_change()\n",
    "    filter_node.calculate_schema_changes()\n",
    "    metadata.add_node(filter_node)\n",
    "    \n",
    "    # Step 4: Aggregate\n",
    "    aggregate = NodeExecutionMetadata(\n",
    "        node_name=\"hourly_aggregation\",\n",
    "        operation=\"group_by\",\n",
    "        status=\"success\",\n",
    "        duration=3.8,\n",
    "        rows_in=85000,\n",
    "        rows_out=2040,\n",
    "        schema_in=[\"id\", \"timestamp\", \"value\", \"sensor_id\"],\n",
    "        schema_out=[\"hour\", \"sensor_id\", \"avg_value\", \"min_value\", \"max_value\", \"count\"]\n",
    "    )\n",
    "    aggregate.calculate_row_change()\n",
    "    aggregate.calculate_schema_changes()\n",
    "    metadata.add_node(aggregate)\n",
    "    \n",
    "    metadata.completed_at = \"2024-01-15T09:00:13\"\n",
    "    metadata.duration = 12.6\n",
    "    \n",
    "    # Render story\n",
    "    renderer = MarkdownStoryRenderer()\n",
    "    story = renderer.render(metadata)\n",
    "    \n",
    "    return story\n",
    "\n",
    "story = build_data_lineage_story()\n",
    "print(story[:1500])\n",
    "print(\"\\n... (story continues)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration Tracking and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyzing performance from story metadata\n",
    "def analyze_performance(metadata: PipelineStoryMetadata):\n",
    "    \"\"\"Extract performance insights from story.\"\"\"\n",
    "    \n",
    "    print(f\"Pipeline Performance Analysis: {metadata.pipeline_name}\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"\\nOverall:\")\n",
    "    print(f\"  Total Duration: {metadata.duration:.2f}s\")\n",
    "    print(f\"  Total Nodes: {metadata.total_nodes}\")\n",
    "    print(f\"  Avg Time/Node: {metadata.duration/metadata.total_nodes:.2f}s\")\n",
    "    \n",
    "    # Node-level analysis\n",
    "    print(f\"\\nNode Performance:\")\n",
    "    sorted_nodes = sorted(metadata.nodes, key=lambda n: n.duration, reverse=True)\n",
    "    \n",
    "    for i, node in enumerate(sorted_nodes[:5], 1):\n",
    "        pct = (node.duration / metadata.duration * 100) if metadata.duration > 0 else 0\n",
    "        print(f\"  {i}. {node.node_name}: {node.duration:.2f}s ({pct:.1f}%)\")\n",
    "        \n",
    "        if node.rows_in and node.rows_out:\n",
    "            throughput = node.rows_out / node.duration if node.duration > 0 else 0\n",
    "            print(f\"     Throughput: {throughput:,.0f} rows/sec\")\n",
    "    \n",
    "    # Data volume analysis\n",
    "    print(f\"\\nData Volume:\")\n",
    "    total_rows = metadata.get_total_rows_processed()\n",
    "    print(f\"  Total Rows Processed: {total_rows:,}\")\n",
    "    \n",
    "    overall_throughput = total_rows / metadata.duration if metadata.duration > 0 else 0\n",
    "    print(f\"  Overall Throughput: {overall_throughput:,.0f} rows/sec\")\n",
    "\n",
    "# Test with our previous example\n",
    "test_metadata = PipelineStoryMetadata(\n",
    "    pipeline_name=\"test_pipeline\",\n",
    "    duration=10.5\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    node = NodeExecutionMetadata(\n",
    "        node_name=f\"node_{i}\",\n",
    "        operation=\"transform\",\n",
    "        status=\"success\",\n",
    "        duration=3.5 - i,\n",
    "        rows_in=10000,\n",
    "        rows_out=9000 + i*100\n",
    "    )\n",
    "    test_metadata.add_node(node)\n",
    "\n",
    "analyze_performance(test_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Testing Stories\n",
    "\n",
    "### Validating Story Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing story generation\n",
    "def test_story_generation():\n",
    "    \"\"\"Validate story content and structure.\"\"\"\n",
    "    \n",
    "    # Create test metadata\n",
    "    metadata = PipelineStoryMetadata(\n",
    "        pipeline_name=\"test_pipeline\",\n",
    "        started_at=\"2024-01-15T10:00:00\",\n",
    "        completed_at=\"2024-01-15T10:05:00\",\n",
    "        duration=300.0\n",
    "    )\n",
    "    \n",
    "    # Add test nodes\n",
    "    metadata.add_node(NodeExecutionMetadata(\n",
    "        node_name=\"node_1\",\n",
    "        operation=\"read\",\n",
    "        status=\"success\",\n",
    "        duration=100.0,\n",
    "        rows_out=1000\n",
    "    ))\n",
    "    \n",
    "    metadata.add_node(NodeExecutionMetadata(\n",
    "        node_name=\"node_2\",\n",
    "        operation=\"transform\",\n",
    "        status=\"failed\",\n",
    "        duration=50.0,\n",
    "        error_message=\"Column not found\",\n",
    "        error_type=\"KeyError\"\n",
    "    ))\n",
    "    \n",
    "    # Test Markdown rendering\n",
    "    md_renderer = MarkdownStoryRenderer()\n",
    "    md_story = md_renderer.render(metadata)\n",
    "    \n",
    "    # Validate content\n",
    "    tests = [\n",
    "        (\"# üìä Pipeline Run Story\" in md_story, \"Has header\"),\n",
    "        (\"test_pipeline\" in md_story, \"Contains pipeline name\"),\n",
    "        (\"‚úÖ node_1\" in md_story, \"Shows successful node\"),\n",
    "        (\"‚ùå node_2\" in md_story, \"Shows failed node\"),\n",
    "        (\"KeyError\" in md_story, \"Includes error type\"),\n",
    "        (\"Column not found\" in md_story, \"Includes error message\"),\n",
    "        (\"Duration:\" in md_story, \"Shows duration\"),\n",
    "        (\"Success Rate\" in md_story, \"Shows success rate\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Story Content Validation:\")\n",
    "    print(\"=\"*50)\n",
    "    all_passed = True\n",
    "    for passed, description in tests:\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        print(f\"{status}: {description}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Overall: {'‚úÖ ALL TESTS PASSED' if all_passed else '‚ùå SOME TESTS FAILED'}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "test_story_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing JSON serialization\n",
    "def test_json_serialization():\n",
    "    \"\"\"Validate JSON story can be serialized and deserialized.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = PipelineStoryMetadata(\n",
    "        pipeline_name=\"json_test\",\n",
    "        duration=100.5\n",
    "    )\n",
    "    \n",
    "    node = NodeExecutionMetadata(\n",
    "        node_name=\"test_node\",\n",
    "        operation=\"test_op\",\n",
    "        status=\"success\",\n",
    "        duration=50.0,\n",
    "        rows_in=100,\n",
    "        rows_out=90\n",
    "    )\n",
    "    node.calculate_row_change()\n",
    "    metadata.add_node(node)\n",
    "    \n",
    "    # Render as JSON\n",
    "    renderer = JSONStoryRenderer()\n",
    "    json_str = renderer.render(metadata)\n",
    "    \n",
    "    # Parse back\n",
    "    parsed = json.loads(json_str)\n",
    "    \n",
    "    # Validate\n",
    "    tests = [\n",
    "        (parsed[\"pipeline_name\"] == \"json_test\", \"Pipeline name preserved\"),\n",
    "        (parsed[\"duration\"] == 100.5, \"Duration preserved\"),\n",
    "        (len(parsed[\"nodes\"]) == 1, \"Node count correct\"),\n",
    "        (parsed[\"nodes\"][0][\"node_name\"] == \"test_node\", \"Node name preserved\"),\n",
    "        (parsed[\"nodes\"][0][\"rows_change\"] == -10, \"Row change calculated\"),\n",
    "        (parsed[\"success_rate\"] == 100.0, \"Success rate calculated\")\n",
    "    ]\n",
    "    \n",
    "    print(\"JSON Serialization Tests:\")\n",
    "    print(\"=\"*50)\n",
    "    for passed, description in tests:\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        print(f\"{status}: {description}\")\n",
    "    \n",
    "    return all(t[0] for t in tests)\n",
    "\n",
    "test_json_serialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **StoryGenerator**: Creates markdown documentation from pipeline execution\n",
    "   - Configurable sample sizes and output paths\n",
    "   - Automatic DataFrame-to-markdown conversion\n",
    "   - Rich execution context inclusion\n",
    "\n",
    "2. **NodeExecutionMetadata**: Tracks detailed node-level metrics\n",
    "   - Row counts (in/out/change/percentage)\n",
    "   - Schema evolution (added/removed columns)\n",
    "   - Timing and performance data\n",
    "   - Error tracking with full context\n",
    "\n",
    "3. **PipelineStoryMetadata**: Aggregates pipeline-level information\n",
    "   - Overall status and success rates\n",
    "   - Total rows processed across pipeline\n",
    "   - Project/plant/business unit context\n",
    "   - Theme and rendering preferences\n",
    "\n",
    "4. **Multi-Format Renderers**:\n",
    "   - **Markdown**: Human-readable, GitHub-flavored\n",
    "   - **HTML**: Professional, interactive reports (requires Jinja2)\n",
    "   - **JSON**: Machine-readable for APIs and storage\n",
    "\n",
    "5. **Theme System**: Customizable branding and styling\n",
    "   - Built-in themes (default, corporate, dark, minimal)\n",
    "   - Custom color schemes and typography\n",
    "   - Company branding (logo, name, footer)\n",
    "   - CSS customization support\n",
    "\n",
    "### Automatic Documentation Pattern\n",
    "\n",
    "Story generation enables:\n",
    "- **Self-documenting pipelines** - execution creates its own documentation\n",
    "- **Data lineage tracking** - see exactly how data transforms\n",
    "- **Performance monitoring** - identify slow nodes and bottlenecks\n",
    "- **Error debugging** - rich context for troubleshooting\n",
    "- **Compliance reporting** - auditable execution records\n",
    "- **Knowledge sharing** - communicate pipeline behavior to stakeholders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
