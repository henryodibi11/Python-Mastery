{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generation Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: CSV Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.metadata import PipelineStoryMetadata, NodeExecutionMetadata\n",
    "import csv\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "class CSVStoryRenderer:\n",
    "    \"\"\"Renders pipeline stories as CSV.\"\"\"\n",
    "    \n",
    "    def render(self, metadata: PipelineStoryMetadata) -> str:\n",
    "        \"\"\"Render story as CSV string.\"\"\"\n",
    "        output = StringIO()\n",
    "        \n",
    "        fieldnames = [\n",
    "            'node_name', \n",
    "            'operation', \n",
    "            'status', \n",
    "            'duration', \n",
    "            'rows_in', \n",
    "            'rows_out',\n",
    "            'rows_change',\n",
    "            'rows_change_pct'\n",
    "        ]\n",
    "        \n",
    "        writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write node rows\n",
    "        for node in metadata.nodes:\n",
    "            writer.writerow({\n",
    "                'node_name': node.node_name,\n",
    "                'operation': node.operation,\n",
    "                'status': node.status,\n",
    "                'duration': node.duration,\n",
    "                'rows_in': node.rows_in or '',\n",
    "                'rows_out': node.rows_out or '',\n",
    "                'rows_change': node.rows_change or '',\n",
    "                'rows_change_pct': f\"{node.rows_change_pct:.2f}\" if node.rows_change_pct is not None else ''\n",
    "            })\n",
    "        \n",
    "        # Add summary row\n",
    "        writer.writerow({\n",
    "            'node_name': 'PIPELINE_SUMMARY',\n",
    "            'operation': metadata.pipeline_name,\n",
    "            'status': 'success' if metadata.failed_nodes == 0 else 'failed',\n",
    "            'duration': metadata.duration,\n",
    "            'rows_in': '',\n",
    "            'rows_out': metadata.get_total_rows_processed(),\n",
    "            'rows_change': '',\n",
    "            'rows_change_pct': f\"{metadata.get_success_rate():.1f}\"\n",
    "        })\n",
    "        \n",
    "        return output.getvalue()\n",
    "    \n",
    "    def render_to_file(self, metadata: PipelineStoryMetadata, output_path: str) -> str:\n",
    "        \"\"\"Render story and save to CSV file.\"\"\"\n",
    "        csv_content = self.render(metadata)\n",
    "        \n",
    "        output_file = Path(output_path)\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "            f.write(csv_content)\n",
    "        \n",
    "        return str(output_file)\n",
    "\n",
    "# Test implementation\n",
    "test_metadata = PipelineStoryMetadata(\n",
    "    pipeline_name=\"test_pipeline\",\n",
    "    duration=10.5\n",
    ")\n",
    "\n",
    "node1 = NodeExecutionMetadata(\n",
    "    node_name=\"extract\",\n",
    "    operation=\"read_csv\",\n",
    "    status=\"success\",\n",
    "    duration=3.5,\n",
    "    rows_in=0,\n",
    "    rows_out=1000\n",
    ")\n",
    "node1.calculate_row_change()\n",
    "test_metadata.add_node(node1)\n",
    "\n",
    "node2 = NodeExecutionMetadata(\n",
    "    node_name=\"transform\",\n",
    "    operation=\"filter\",\n",
    "    status=\"success\",\n",
    "    duration=2.0,\n",
    "    rows_in=1000,\n",
    "    rows_out=850\n",
    ")\n",
    "node2.calculate_row_change()\n",
    "test_metadata.add_node(node2)\n",
    "\n",
    "renderer = CSVStoryRenderer()\n",
    "csv_output = renderer.render(test_metadata)\n",
    "print(csv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Enhanced Metadata with Memory Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class EnhancedNodeExecutionMetadata:\n",
    "    \"\"\"Extended metadata with memory tracking.\"\"\"\n",
    "    \n",
    "    node_name: str\n",
    "    operation: str\n",
    "    status: str\n",
    "    duration: float\n",
    "    \n",
    "    # Data metrics\n",
    "    rows_in: Optional[int] = None\n",
    "    rows_out: Optional[int] = None\n",
    "    rows_change: Optional[int] = None\n",
    "    rows_change_pct: Optional[float] = None\n",
    "    \n",
    "    # Memory metrics\n",
    "    memory_mb: Optional[float] = None\n",
    "    peak_memory_mb: Optional[float] = None\n",
    "    memory_efficiency: Optional[float] = None  # rows per MB\n",
    "    \n",
    "    # Schema tracking\n",
    "    schema_in: Optional[List[str]] = None\n",
    "    schema_out: Optional[List[str]] = None\n",
    "    \n",
    "    def calculate_row_change(self):\n",
    "        \"\"\"Calculate row count change metrics.\"\"\"\n",
    "        if self.rows_in is not None and self.rows_out is not None:\n",
    "            self.rows_change = self.rows_out - self.rows_in\n",
    "            if self.rows_in > 0:\n",
    "                self.rows_change_pct = (self.rows_change / self.rows_in) * 100\n",
    "            else:\n",
    "                self.rows_change_pct = 0.0 if self.rows_out == 0 else 100.0\n",
    "    \n",
    "    def calculate_memory_efficiency(self) -> Optional[float]:\n",
    "        \"\"\"Calculate rows processed per MB of memory.\"\"\"\n",
    "        if self.rows_out is not None and self.memory_mb and self.memory_mb > 0:\n",
    "            self.memory_efficiency = self.rows_out / self.memory_mb\n",
    "            return self.memory_efficiency\n",
    "        return None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"node_name\": self.node_name,\n",
    "            \"operation\": self.operation,\n",
    "            \"status\": self.status,\n",
    "            \"duration\": self.duration,\n",
    "            \"rows_in\": self.rows_in,\n",
    "            \"rows_out\": self.rows_out,\n",
    "            \"rows_change\": self.rows_change,\n",
    "            \"rows_change_pct\": self.rows_change_pct,\n",
    "            \"memory_mb\": self.memory_mb,\n",
    "            \"peak_memory_mb\": self.peak_memory_mb,\n",
    "            \"memory_efficiency\": self.memory_efficiency,\n",
    "            \"schema_in\": self.schema_in,\n",
    "            \"schema_out\": self.schema_out,\n",
    "        }\n",
    "\n",
    "# Test implementation\n",
    "node = EnhancedNodeExecutionMetadata(\n",
    "    node_name=\"memory_test\",\n",
    "    operation=\"transform\",\n",
    "    status=\"success\",\n",
    "    duration=5.0,\n",
    "    rows_out=100000,\n",
    "    memory_mb=256.5,\n",
    "    peak_memory_mb=312.8\n",
    ")\n",
    "\n",
    "efficiency = node.calculate_memory_efficiency()\n",
    "print(f\"Memory Efficiency: {efficiency:.2f} rows/MB\")\n",
    "print(f\"Peak Memory: {node.peak_memory_mb:.2f} MB\")\n",
    "print(f\"\\nMetadata Dict:\")\n",
    "import json\n",
    "print(json.dumps(node.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Custom Organization Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.story.themes import StoryTheme\n",
    "\n",
    "def create_organization_theme():\n",
    "    \"\"\"Create a branded theme for Ingredion.\"\"\"\n",
    "    \n",
    "    theme = StoryTheme(\n",
    "        name=\"ingredion_corporate\",\n",
    "        \n",
    "        # Color scheme - Ingredion branding\n",
    "        primary_color=\"#006837\",      # Ingredion green\n",
    "        success_color=\"#2e7d32\",      # Green for success\n",
    "        error_color=\"#c62828\",        # Red for errors\n",
    "        warning_color=\"#f57c00\",      # Orange for warnings\n",
    "        bg_color=\"#fafafa\",           # Light background\n",
    "        text_color=\"#212121\",         # Dark text\n",
    "        border_color=\"#e0e0e0\",       # Subtle borders\n",
    "        code_bg=\"#f5f5f5\",            # Code background\n",
    "        \n",
    "        # Typography\n",
    "        font_family=\"'Open Sans', 'Helvetica Neue', Arial, sans-serif\",\n",
    "        heading_font=\"'Roboto', 'Open Sans', Arial, sans-serif\",\n",
    "        code_font=\"'Fira Code', Consolas, Monaco, monospace\",\n",
    "        font_size=\"15px\",\n",
    "        \n",
    "        # Branding\n",
    "        logo_url=\"https://www.ingredion.com/content/dam/ingredion/logos/ingredion-logo.png\",\n",
    "        company_name=\"Ingredion Incorporated\",\n",
    "        footer_text=\"Â© 2024 Ingredion - Confidential & Proprietary\",\n",
    "        \n",
    "        # Layout\n",
    "        max_width=\"1400px\",\n",
    "        sidebar=False,\n",
    "        \n",
    "        # Custom CSS\n",
    "        custom_css=\"\"\"\n",
    "        /* Card styling with rounded corners */\n",
    "        .node-card {\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "            transition: box-shadow 0.3s ease;\n",
    "        }\n",
    "        \n",
    "        .node-card:hover {\n",
    "            box-shadow: 0 4px 8px rgba(0,0,0,0.15);\n",
    "        }\n",
    "        \n",
    "        /* Gradient header */\n",
    "        .story-header {\n",
    "            background: linear-gradient(135deg, #006837 0%, #008c4a 100%);\n",
    "            color: white;\n",
    "            padding: 24px;\n",
    "            border-radius: 8px 8px 0 0;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        \n",
    "        /* Status badges with rounded style */\n",
    "        .status-badge {\n",
    "            display: inline-block;\n",
    "            padding: 4px 12px;\n",
    "            border-radius: 12px;\n",
    "            font-weight: 600;\n",
    "            font-size: 13px;\n",
    "        }\n",
    "        \n",
    "        .status-success {\n",
    "            background-color: #e8f5e9;\n",
    "            color: #2e7d32;\n",
    "        }\n",
    "        \n",
    "        .status-error {\n",
    "            background-color: #ffebee;\n",
    "            color: #c62828;\n",
    "        }\n",
    "        \n",
    "        /* Hover effects on interactive elements */\n",
    "        .expandable-section {\n",
    "            cursor: pointer;\n",
    "            padding: 12px;\n",
    "            border-left: 4px solid var(--primary-color);\n",
    "            transition: background-color 0.2s ease;\n",
    "        }\n",
    "        \n",
    "        .expandable-section:hover {\n",
    "            background-color: rgba(0, 104, 55, 0.05);\n",
    "        }\n",
    "        \n",
    "        /* Data tables */\n",
    "        .data-table {\n",
    "            border-radius: 4px;\n",
    "            overflow: hidden;\n",
    "        }\n",
    "        \n",
    "        .data-table th {\n",
    "            background-color: #006837;\n",
    "            color: white;\n",
    "            padding: 12px;\n",
    "        }\n",
    "        \n",
    "        .data-table td {\n",
    "            padding: 10px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "        }\n",
    "        \n",
    "        .data-table tr:hover {\n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    return theme\n",
    "\n",
    "# Test the theme\n",
    "my_theme = create_organization_theme()\n",
    "print(\"Ingredion Corporate Theme\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCSS Variables:\")\n",
    "for var, value in my_theme.to_css_vars().items():\n",
    "    print(f\"  {var}: {value}\")\n",
    "\n",
    "print(\"\\n\\nComplete CSS:\")\n",
    "print(my_theme.to_css_string()[:500] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Story Diff Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any\n",
    "from odibi.story.metadata import PipelineStoryMetadata, NodeExecutionMetadata\n",
    "\n",
    "class StoryDiff:\n",
    "    \"\"\"Compare two pipeline story executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, before: PipelineStoryMetadata, after: PipelineStoryMetadata):\n",
    "        self.before = before\n",
    "        self.after = after\n",
    "    \n",
    "    def compare_performance(self) -> Dict[str, Any]:\n",
    "        \"\"\"Compare overall performance metrics.\"\"\"\n",
    "        duration_diff = self.after.duration - self.before.duration\n",
    "        duration_pct = (duration_diff / self.before.duration * 100) if self.before.duration > 0 else 0\n",
    "        \n",
    "        success_rate_diff = self.after.get_success_rate() - self.before.get_success_rate()\n",
    "        \n",
    "        return {\n",
    "            'duration_before': self.before.duration,\n",
    "            'duration_after': self.after.duration,\n",
    "            'duration_diff': duration_diff,\n",
    "            'duration_pct_change': duration_pct,\n",
    "            'success_rate_before': self.before.get_success_rate(),\n",
    "            'success_rate_after': self.after.get_success_rate(),\n",
    "            'success_rate_diff': success_rate_diff\n",
    "        }\n",
    "    \n",
    "    def compare_nodes(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Identify node changes.\"\"\"\n",
    "        before_nodes = {node.node_name for node in self.before.nodes}\n",
    "        after_nodes = {node.node_name for node in self.after.nodes}\n",
    "        \n",
    "        added = list(after_nodes - before_nodes)\n",
    "        removed = list(before_nodes - after_nodes)\n",
    "        common = list(before_nodes & after_nodes)\n",
    "        \n",
    "        # Find nodes with status changes\n",
    "        status_changes = []\n",
    "        before_dict = {n.node_name: n for n in self.before.nodes}\n",
    "        after_dict = {n.node_name: n for n in self.after.nodes}\n",
    "        \n",
    "        for node_name in common:\n",
    "            if before_dict[node_name].status != after_dict[node_name].status:\n",
    "                status_changes.append(\n",
    "                    f\"{node_name}: {before_dict[node_name].status} -> {after_dict[node_name].status}\"\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            'added': added,\n",
    "            'removed': removed,\n",
    "            'common': common,\n",
    "            'status_changes': status_changes\n",
    "        }\n",
    "    \n",
    "    def compare_data_volume(self) -> Dict[str, Any]:\n",
    "        \"\"\"Compare data processing volumes.\"\"\"\n",
    "        before_total = self.before.get_total_rows_processed()\n",
    "        after_total = self.after.get_total_rows_processed()\n",
    "        \n",
    "        diff = after_total - before_total\n",
    "        pct = (diff / before_total * 100) if before_total > 0 else 0\n",
    "        \n",
    "        # Compare node-level data volumes\n",
    "        node_changes = []\n",
    "        before_dict = {n.node_name: n for n in self.before.nodes}\n",
    "        after_dict = {n.node_name: n for n in self.after.nodes}\n",
    "        \n",
    "        for node_name in set(before_dict.keys()) & set(after_dict.keys()):\n",
    "            before_rows = before_dict[node_name].rows_out or 0\n",
    "            after_rows = after_dict[node_name].rows_out or 0\n",
    "            \n",
    "            if before_rows != after_rows:\n",
    "                node_diff = after_rows - before_rows\n",
    "                node_pct = (node_diff / before_rows * 100) if before_rows > 0 else 0\n",
    "                node_changes.append({\n",
    "                    'node_name': node_name,\n",
    "                    'before': before_rows,\n",
    "                    'after': after_rows,\n",
    "                    'diff': node_diff,\n",
    "                    'pct': node_pct\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'total_before': before_total,\n",
    "            'total_after': after_total,\n",
    "            'total_diff': diff,\n",
    "            'total_pct_change': pct,\n",
    "            'node_changes': node_changes\n",
    "        }\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate markdown diff report.\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Header\n",
    "        lines.append(f\"# Pipeline Comparison Report\")\n",
    "        lines.append(f\"\")\n",
    "        lines.append(f\"**Pipeline:** {self.before.pipeline_name}\")\n",
    "        lines.append(f\"**Before:** {self.before.started_at}\")\n",
    "        lines.append(f\"**After:** {self.after.started_at}\")\n",
    "        lines.append(f\"\")\n",
    "        lines.append(\"---\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        # Performance comparison\n",
    "        perf = self.compare_performance()\n",
    "        lines.append(\"## Performance Comparison\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        duration_icon = \"ğŸŸ¢\" if perf['duration_diff'] < 0 else \"ğŸ”´\" if perf['duration_diff'] > 0 else \"âšª\"\n",
    "        lines.append(f\"{duration_icon} **Duration:**\")\n",
    "        lines.append(f\"  - Before: {perf['duration_before']:.2f}s\")\n",
    "        lines.append(f\"  - After: {perf['duration_after']:.2f}s\")\n",
    "        lines.append(f\"  - Change: {perf['duration_diff']:+.2f}s ({perf['duration_pct_change']:+.1f}%)\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        success_icon = \"ğŸŸ¢\" if perf['success_rate_diff'] > 0 else \"ğŸ”´\" if perf['success_rate_diff'] < 0 else \"âšª\"\n",
    "        lines.append(f\"{success_icon} **Success Rate:**\")\n",
    "        lines.append(f\"  - Before: {perf['success_rate_before']:.1f}%\")\n",
    "        lines.append(f\"  - After: {perf['success_rate_after']:.1f}%\")\n",
    "        lines.append(f\"  - Change: {perf['success_rate_diff']:+.1f}%\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        # Node changes\n",
    "        nodes = self.compare_nodes()\n",
    "        lines.append(\"## Node Changes\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        if nodes['added']:\n",
    "            lines.append(f\"â• **Added Nodes:** {', '.join(nodes['added'])}\")\n",
    "        if nodes['removed']:\n",
    "            lines.append(f\"â– **Removed Nodes:** {', '.join(nodes['removed'])}\")\n",
    "        if nodes['status_changes']:\n",
    "            lines.append(f\"âš ï¸ **Status Changes:**\")\n",
    "            for change in nodes['status_changes']:\n",
    "                lines.append(f\"  - {change}\")\n",
    "        if not (nodes['added'] or nodes['removed'] or nodes['status_changes']):\n",
    "            lines.append(\"âœ… No node changes\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        # Data volume changes\n",
    "        volume = self.compare_data_volume()\n",
    "        lines.append(\"## Data Volume Changes\")\n",
    "        lines.append(f\"\")\n",
    "        lines.append(f\"**Total Rows Processed:**\")\n",
    "        lines.append(f\"  - Before: {volume['total_before']:,}\")\n",
    "        lines.append(f\"  - After: {volume['total_after']:,}\")\n",
    "        lines.append(f\"  - Change: {volume['total_diff']:+,} ({volume['total_pct_change']:+.1f}%)\")\n",
    "        lines.append(f\"\")\n",
    "        \n",
    "        if volume['node_changes']:\n",
    "            lines.append(\"**Node-Level Changes:**\")\n",
    "            lines.append(f\"\")\n",
    "            lines.append(\"| Node | Before | After | Change | % Change |\")\n",
    "            lines.append(\"|------|--------|-------|--------|----------|\")\n",
    "            for nc in volume['node_changes']:\n",
    "                lines.append(\n",
    "                    f\"| {nc['node_name']} | {nc['before']:,} | {nc['after']:,} | \"\n",
    "                    f\"{nc['diff']:+,} | {nc['pct']:+.1f}% |\"\n",
    "                )\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Test the implementation\n",
    "run1 = PipelineStoryMetadata(\n",
    "    pipeline_name=\"test\",\n",
    "    duration=10.0,\n",
    "    started_at=\"2024-01-15T10:00:00\"\n",
    ")\n",
    "n1 = NodeExecutionMetadata(\n",
    "    node_name=\"extract\",\n",
    "    operation=\"read\",\n",
    "    status=\"success\",\n",
    "    duration=3.0,\n",
    "    rows_out=1000\n",
    ")\n",
    "run1.add_node(n1)\n",
    "\n",
    "n2 = NodeExecutionMetadata(\n",
    "    node_name=\"transform\",\n",
    "    operation=\"filter\",\n",
    "    status=\"success\",\n",
    "    duration=7.0,\n",
    "    rows_in=1000,\n",
    "    rows_out=900\n",
    ")\n",
    "run1.add_node(n2)\n",
    "\n",
    "run2 = PipelineStoryMetadata(\n",
    "    pipeline_name=\"test\",\n",
    "    duration=8.0,\n",
    "    started_at=\"2024-01-15T11:00:00\"\n",
    ")\n",
    "n3 = NodeExecutionMetadata(\n",
    "    node_name=\"extract\",\n",
    "    operation=\"read\",\n",
    "    status=\"success\",\n",
    "    duration=2.5,\n",
    "    rows_out=1000\n",
    ")\n",
    "run2.add_node(n3)\n",
    "\n",
    "n4 = NodeExecutionMetadata(\n",
    "    node_name=\"transform\",\n",
    "    operation=\"filter\",\n",
    "    status=\"success\",\n",
    "    duration=5.5,\n",
    "    rows_in=1000,\n",
    "    rows_out=900\n",
    ")\n",
    "run2.add_node(n4)\n",
    "\n",
    "diff = StoryDiff(run1, run2)\n",
    "report = diff.generate_report()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: Story Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import json\n",
    "\n",
    "class StoryAnalyzer:\n",
    "    \"\"\"Analyze multiple pipeline story executions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stories: List[Dict] = []\n",
    "    \n",
    "    def load_from_directory(self, directory: str):\n",
    "        \"\"\"Load all JSON story files from directory.\"\"\"\n",
    "        directory_path = Path(directory)\n",
    "        for json_file in directory_path.glob('*.json'):\n",
    "            try:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    story = json.load(f)\n",
    "                    self.stories.append(story)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {json_file}: {e}\")\n",
    "    \n",
    "    def add_story(self, story_dict: Dict):\n",
    "        \"\"\"Add a story to the analysis.\"\"\"\n",
    "        self.stories.append(story_dict)\n",
    "    \n",
    "    def get_average_duration(self) -> float:\n",
    "        \"\"\"Calculate average pipeline duration.\"\"\"\n",
    "        if not self.stories:\n",
    "            return 0.0\n",
    "        \n",
    "        total = sum(story.get('duration', 0) for story in self.stories)\n",
    "        return total / len(self.stories)\n",
    "    \n",
    "    def get_failure_hotspots(self) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Find nodes that fail most frequently.\"\"\"\n",
    "        failure_counts = Counter()\n",
    "        \n",
    "        for story in self.stories:\n",
    "            for node in story.get('nodes', []):\n",
    "                if node.get('status') == 'failed':\n",
    "                    failure_counts[node['node_name']] += 1\n",
    "        \n",
    "        return failure_counts.most_common()\n",
    "    \n",
    "    def get_slowest_nodes(self, top_n: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find slowest nodes across all runs.\"\"\"\n",
    "        node_durations = defaultdict(list)\n",
    "        \n",
    "        # Collect all durations for each node\n",
    "        for story in self.stories:\n",
    "            for node in story.get('nodes', []):\n",
    "                node_name = node['node_name']\n",
    "                duration = node.get('duration', 0)\n",
    "                node_durations[node_name].append(duration)\n",
    "        \n",
    "        # Calculate average duration for each node\n",
    "        avg_durations = {\n",
    "            name: sum(durations) / len(durations)\n",
    "            for name, durations in node_durations.items()\n",
    "        }\n",
    "        \n",
    "        # Sort and return top N\n",
    "        sorted_nodes = sorted(avg_durations.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_nodes[:top_n]\n",
    "    \n",
    "    def get_total_data_processed(self) -> int:\n",
    "        \"\"\"Calculate total rows processed across all runs.\"\"\"\n",
    "        total = 0\n",
    "        for story in self.stories:\n",
    "            total += story.get('total_rows_processed', 0)\n",
    "        return total\n",
    "    \n",
    "    def get_success_rate_trend(self) -> List[float]:\n",
    "        \"\"\"Get success rate for each run in chronological order.\"\"\"\n",
    "        # Sort by started_at\n",
    "        sorted_stories = sorted(\n",
    "            self.stories,\n",
    "            key=lambda s: s.get('started_at', '')\n",
    "        )\n",
    "        \n",
    "        return [story.get('success_rate', 0) for story in sorted_stories]\n",
    "    \n",
    "    def generate_dashboard(self) -> str:\n",
    "        \"\"\"Generate markdown analytics dashboard.\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Header\n",
    "        lines.append(\"# Pipeline Analytics Dashboard\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"**Total Runs Analyzed:** {len(self.stories)}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"---\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        lines.append(\"## Overall Metrics\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        avg_duration = self.get_average_duration()\n",
    "        total_data = self.get_total_data_processed()\n",
    "        \n",
    "        lines.append(f\"- â±ï¸ **Average Duration:** {avg_duration:.2f}s\")\n",
    "        lines.append(f\"- ğŸ“Š **Total Data Processed:** {total_data:,} rows\")\n",
    "        \n",
    "        if self.stories:\n",
    "            avg_success = sum(s.get('success_rate', 0) for s in self.stories) / len(self.stories)\n",
    "            lines.append(f\"- âœ… **Average Success Rate:** {avg_success:.1f}%\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Failure hotspots\n",
    "        lines.append(\"## Failure Hotspots\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        failures = self.get_failure_hotspots()\n",
    "        if failures:\n",
    "            lines.append(\"Nodes with most failures:\")\n",
    "            lines.append(\"\")\n",
    "            for node_name, count in failures[:5]:\n",
    "                lines.append(f\"- âŒ **{node_name}**: {count} failure(s)\")\n",
    "        else:\n",
    "            lines.append(\"âœ… No failures detected\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Slowest nodes\n",
    "        lines.append(\"## Performance Bottlenecks\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        slowest = self.get_slowest_nodes(5)\n",
    "        if slowest:\n",
    "            lines.append(\"Slowest nodes (average duration):\")\n",
    "            lines.append(\"\")\n",
    "            for node_name, avg_duration in slowest:\n",
    "                lines.append(f\"- ğŸŒ **{node_name}**: {avg_duration:.2f}s\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Success rate trend\n",
    "        lines.append(\"## Success Rate Trend\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        trend = self.get_success_rate_trend()\n",
    "        if len(trend) >= 2:\n",
    "            first = trend[0]\n",
    "            last = trend[-1]\n",
    "            change = last - first\n",
    "            \n",
    "            trend_icon = \"ğŸ“ˆ\" if change > 0 else \"ğŸ“‰\" if change < 0 else \"â¡ï¸\"\n",
    "            lines.append(f\"{trend_icon} Success rate: {first:.1f}% â†’ {last:.1f}% ({change:+.1f}%)\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"Recent runs:\")\n",
    "            for i, rate in enumerate(trend[-5:], 1):\n",
    "                bars = \"â–ˆ\" * int(rate / 10)\n",
    "                lines.append(f\"  Run {i}: {bars} {rate:.1f}%\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        lines.append(\"## Recommendations\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        if failures:\n",
    "            lines.append(f\"- ğŸ” Investigate failures in **{failures[0][0]}** node\")\n",
    "        \n",
    "        if slowest:\n",
    "            lines.append(f\"- âš¡ Optimize **{slowest[0][0]}** node (slowest on average)\")\n",
    "        \n",
    "        if len(trend) >= 2 and trend[-1] < trend[0]:\n",
    "            lines.append(\"- âš ï¸ Success rate declining - review recent changes\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Test with sample data\n",
    "analyzer = StoryAnalyzer()\n",
    "\n",
    "# Add sample stories\n",
    "for i in range(5):\n",
    "    sample_story = {\n",
    "        \"pipeline_name\": \"test\",\n",
    "        \"started_at\": f\"2024-01-{15+i}T10:00:00\",\n",
    "        \"duration\": 10.0 + i * 0.5,\n",
    "        \"total_rows_processed\": 5000 + i * 100,\n",
    "        \"success_rate\": 100 - i * 5,\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"node_name\": \"extract\",\n",
    "                \"status\": \"success\",\n",
    "                \"duration\": 3.0 + i * 0.2\n",
    "            },\n",
    "            {\n",
    "                \"node_name\": \"transform\",\n",
    "                \"status\": \"failed\" if i % 2 == 1 else \"success\",\n",
    "                \"duration\": 7.0 + i * 0.3\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    analyzer.add_story(sample_story)\n",
    "\n",
    "dashboard = analyzer.generate_dashboard()\n",
    "print(dashboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
