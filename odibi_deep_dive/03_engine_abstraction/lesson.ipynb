{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive 03: Odibi Engine Abstraction\n",
    "\n",
    "## üéØ The Problem\n",
    "\n",
    "Data processing needs different execution engines:\n",
    "- **Small data** ‚Üí Pandas (fast, in-memory)\n",
    "- **Big data** ‚Üí Spark (distributed, scalable)\n",
    "- **Analytics** ‚Üí DuckDB (optimized SQL)\n",
    "\n",
    "**Without abstraction:**\n",
    "```python\n",
    "# ‚ùå Tightly coupled to Pandas\n",
    "df = pd.read_csv('data.csv')\n",
    "result = df.groupby('category').sum()\n",
    "result.to_parquet('output.parquet')\n",
    "\n",
    "# To switch to Spark, rewrite EVERYTHING:\n",
    "df = spark.read.csv('data.csv')\n",
    "result = df.groupBy('category').sum()\n",
    "result.write.parquet('output.parquet')\n",
    "```\n",
    "\n",
    "**With Engine abstraction:**\n",
    "```python\n",
    "# ‚úÖ Engine-agnostic pipeline code\n",
    "df = engine.read(conn, format='csv', path='data.csv')\n",
    "# ... transformations ...\n",
    "engine.write(df, conn, format='parquet', path='output.parquet')\n",
    "\n",
    "# Switch engines with config only - ZERO code changes!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶â First Principles\n",
    "\n",
    "### 1. Abstraction\n",
    "Hide implementation details behind a stable interface.\n",
    "\n",
    "### 2. Polymorphism\n",
    "Treat different engines uniformly through a common base class.\n",
    "\n",
    "### 3. Dependency Inversion\n",
    "Depend on abstractions (Engine ABC), not concrete implementations (PandasEngine).\n",
    "\n",
    "### 4. Open/Closed Principle\n",
    "Open for extension (add new engines), closed for modification (ABC doesn't change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Part 1: The Engine ABC\n",
    "\n",
    "Let's examine the abstract base class that defines what an engine must do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Read the Engine ABC\n",
    "base_engine_path = Path(r\"c:/Users/hodibi/OneDrive - Ingredion/Desktop/Repos/Odibi/odibi/engine/base.py\")\n",
    "\n",
    "with open(base_engine_path) as f:\n",
    "    engine_abc_code = f.read()\n",
    "\n",
    "print(f\"Engine ABC: {len(engine_abc_code.splitlines())} lines\\n\")\n",
    "print(engine_abc_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analysis: The 9 Abstract Methods\n",
    "\n",
    "Every engine MUST implement these methods:\n",
    "\n",
    "### Data I/O (3 methods)\n",
    "1. **`read()`** - Load data from connection\n",
    "2. **`write()`** - Save data to connection  \n",
    "3. **`execute_sql()`** - Run SQL queries\n",
    "\n",
    "### Operations (1 method)\n",
    "4. **`execute_operation()`** - Built-in operations (pivot, etc.)\n",
    "\n",
    "### Introspection (5 methods)\n",
    "5. **`get_schema()`** - Get column names/types\n",
    "6. **`get_shape()`** - Get (rows, columns)\n",
    "7. **`count_rows()`** - Count rows\n",
    "8. **`count_nulls()`** - Count nulls per column\n",
    "9. **`validate_schema()`** - Validate DataFrame structure\n",
    "\n",
    "**Key Insight:** Notice the method signatures use `Any` for DataFrame types. This allows:\n",
    "- PandasEngine to return `pd.DataFrame`\n",
    "- SparkEngine to return `pyspark.sql.DataFrame`\n",
    "- DuckDBEngine to return `duckdb.DuckDBPyRelation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Part 2: PandasEngine Implementation\n",
    "\n",
    "Let's see how Pandas implements the Engine contract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_engine_path = Path(r\"c:/Users/hodibi/OneDrive - Ingredion/Desktop/Repos/Odibi/odibi/engine/pandas_engine.py\")\n",
    "\n",
    "with open(pandas_engine_path) as f:\n",
    "    pandas_code = f.read()\n",
    "\n",
    "print(f\"PandasEngine: {len(pandas_code.splitlines())} lines\\n\")\n",
    "\n",
    "# Show key parts\n",
    "lines = pandas_code.splitlines()\n",
    "print(\"Class definition:\")\n",
    "print('\\n'.join(lines[12:19]))  # Class and __init__\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nread() method signature:\")\n",
    "print('\\n'.join(lines[47:66]))  # read() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Deep Dive: PandasEngine.read()\n",
    "\n",
    "Let's understand the read implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the read() method\n",
    "lines = pandas_code.splitlines()\n",
    "\n",
    "# Find read method (lines 47-134)\n",
    "read_method = '\\n'.join(lines[46:135])\n",
    "print(read_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Analysis: Read Method Pattern\n",
    "\n",
    "Notice the pattern:\n",
    "\n",
    "```python\n",
    "if format == \"csv\":\n",
    "    return pd.read_csv(full_path, **merged_options)\n",
    "elif format == \"parquet\":\n",
    "    return pd.read_parquet(full_path, **merged_options)\n",
    "elif format == \"delta\":\n",
    "    # Special handling for Delta Lake\n",
    "    dt = DeltaTable(full_path, storage_options=storage_opts)\n",
    "    return dt.to_pandas()\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "1. **Format dispatch** - Different code paths per format\n",
    "2. **Storage options** - Merged from connection + user options\n",
    "3. **Cloud support** - Works with ADLS, S3 via `fsspec`\n",
    "4. **Delta Lake** - Uses `deltalake` library, converts to Pandas\n",
    "5. **Error handling** - Clear ImportError messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Deep Dive: PandasEngine.execute_sql()\n",
    "\n",
    "How does Pandas run SQL queries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract execute_sql method (lines 254-299)\n",
    "sql_method = '\\n'.join(lines[253:300])\n",
    "print(sql_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Analysis: SQL Execution Strategy\n",
    "\n",
    "**Clever Two-Tier Fallback:**\n",
    "\n",
    "1. **Prefer DuckDB** (fast, feature-rich SQL engine)\n",
    "   ```python\n",
    "   conn = duckdb.connect(\":memory:\")\n",
    "   for name in context.list_names():\n",
    "       conn.register(name, context.get(name))  # Zero-copy!\n",
    "   return conn.execute(sql).df()\n",
    "   ```\n",
    "\n",
    "2. **Fallback to pandasql** (pure Python, slower)\n",
    "   ```python\n",
    "   from pandasql import sqldf\n",
    "   locals_dict = {name: context.get(name) for name in context.list_names()}\n",
    "   return sqldf(sql, locals_dict)\n",
    "   ```\n",
    "\n",
    "**Why DuckDB?**\n",
    "- 10-100x faster than pandasql\n",
    "- Full SQL support (window functions, CTEs, etc.)\n",
    "- Zero-copy integration with Pandas\n",
    "- Used by many modern tools (Ibis, Hamilton, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Part 3: SparkEngine Implementation\n",
    "\n",
    "Now let's see how Spark implements the same interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_engine_path = Path(r\"c:/Users/hodibi/OneDrive - Ingredion/Desktop/Repos/Odibi/odibi/engine/spark_engine.py\")\n",
    "\n",
    "with open(spark_engine_path) as f:\n",
    "    spark_code = f.read()\n",
    "\n",
    "print(f\"SparkEngine: {len(spark_code.splitlines())} lines\\n\")\n",
    "\n",
    "lines = spark_code.splitlines()\n",
    "print(\"Class definition:\")\n",
    "print('\\n'.join(lines[9:58]))  # Class and __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Deep Dive: SparkEngine.__init__()\n",
    "\n",
    "Notice the differences from PandasEngine:\n",
    "\n",
    "```python\n",
    "def __init__(self, connections=None, spark_session=None, config=None):\n",
    "    # Import guard\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Spark support requires 'pip install odibi[spark]'\")\n",
    "    \n",
    "    # Configure Delta Lake\n",
    "    from delta import configure_spark_with_delta_pip\n",
    "    builder = SparkSession.builder.appName(\"odibi\")\n",
    "    self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # Configure all ADLS connections upfront\n",
    "    self._configure_all_connections()\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "1. **SparkSession** - Heavy object created once, reused\n",
    "2. **Delta integration** - Configured at session level\n",
    "3. **Connection config** - All credentials set upfront (Pandas does per-operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Deep Dive: SparkEngine.read()\n",
    "\n",
    "Compare to PandasEngine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract read method (lines 81-118)\n",
    "spark_read = '\\n'.join(lines[80:119])\n",
    "print(spark_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Analysis: Spark Read Pattern\n",
    "\n",
    "**Much simpler than Pandas!**\n",
    "\n",
    "```python\n",
    "reader = self.spark.read.format(format)\n",
    "for key, value in options.items():\n",
    "    reader = reader.option(key, value)\n",
    "return reader.load(full_path)\n",
    "```\n",
    "\n",
    "**Why simpler?**\n",
    "- Spark has a **unified DataFrameReader API**\n",
    "- All formats (CSV, Parquet, Delta, JSON) use same pattern\n",
    "- No format-specific branches needed\n",
    "- Options are format-agnostic\n",
    "\n",
    "**Trade-off:**\n",
    "- ‚úÖ More generic, extensible\n",
    "- ‚ùå Less control over format-specific features\n",
    "- ‚ùå Requires understanding Spark options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Deep Dive: SparkEngine.execute_sql()\n",
    "\n",
    "SQL in Spark is native:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract execute_sql method (lines 180-194)\n",
    "spark_sql = '\\n'.join(lines[179:195])\n",
    "print(spark_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Analysis: Spark SQL\n",
    "\n",
    "```python\n",
    "# Register DataFrames as temp views\n",
    "for table_name, df in context.items():\n",
    "    df.createOrReplaceTempView(table_name)\n",
    "\n",
    "# Execute SQL\n",
    "return self.spark.sql(sql)\n",
    "```\n",
    "\n",
    "**Much simpler than Pandas!**\n",
    "- No external SQL engine needed\n",
    "- Spark SQL is a first-class feature\n",
    "- Uses Catalyst optimizer\n",
    "- Can leverage distributed joins\n",
    "\n",
    "**Context Difference:**\n",
    "- Pandas: `context.list_names()` + `context.get(name)` (Context API)\n",
    "- Spark: `context.items()` (simple dict)\n",
    "- This shows engines can have different context expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Part 4: The Power of Abstraction\n",
    "\n",
    "Let's visualize how the same pipeline code works with different engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code showing engine-agnostic pipeline\n",
    "\n",
    "def run_pipeline(engine, connection):\n",
    "    \"\"\"\n",
    "    This SAME code works with PandasEngine, SparkEngine, or DuckDBEngine!\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    df = engine.read(\n",
    "        connection=connection,\n",
    "        format='csv',\n",
    "        path='sales.csv'\n",
    "    )\n",
    "    \n",
    "    # Check schema\n",
    "    schema = engine.get_schema(df)\n",
    "    print(f\"Columns: {schema}\")\n",
    "    \n",
    "    # Transform with SQL\n",
    "    result = engine.execute_sql(\n",
    "        \"SELECT category, SUM(amount) as total FROM df GROUP BY category\",\n",
    "        context={'df': df}\n",
    "    )\n",
    "    \n",
    "    # Write output\n",
    "    engine.write(\n",
    "        df=result,\n",
    "        connection=connection,\n",
    "        format='parquet',\n",
    "        path='output.parquet',\n",
    "        mode='overwrite'\n",
    "    )\n",
    "\n",
    "# Same code, different engines:\n",
    "# run_pipeline(PandasEngine(), local_conn)  # In-memory\n",
    "# run_pipeline(SparkEngine(), adls_conn)    # Distributed\n",
    "# run_pipeline(DuckDBEngine(), local_conn)  # Analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 5: Side-by-Side Comparison\n",
    "\n",
    "Let's create a comparison table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Execution Model',\n",
    "        'Memory Model',\n",
    "        'Parallelism',\n",
    "        'SQL Engine',\n",
    "        'Best For',\n",
    "        'Cloud Support',\n",
    "        'Delta Lake',\n",
    "    ],\n",
    "    'PandasEngine': [\n",
    "        'Eager, in-memory',\n",
    "        'All data in RAM',\n",
    "        'Single-threaded',\n",
    "        'DuckDB or pandasql',\n",
    "        'Small-medium (<10GB)',\n",
    "        'Via fsspec',\n",
    "        'Via deltalake lib',\n",
    "    ],\n",
    "    'SparkEngine': [\n",
    "        'Lazy, distributed',\n",
    "        'Spill to disk',\n",
    "        'Multi-node cluster',\n",
    "        'Spark SQL (Catalyst)',\n",
    "        'Large data (>100GB)',\n",
    "        'Native (s3a, abfss)',\n",
    "        'Native (delta-spark)',\n",
    "    ],\n",
    "    'DuckDBEngine (Target)': [\n",
    "        'Eager, in-process',\n",
    "        'Memory-mapped',\n",
    "        'Multi-threaded',\n",
    "        'Native DuckDB SQL',\n",
    "        'Analytics (1-100GB)',\n",
    "        'Via fsspec',\n",
    "        'read_delta() function',\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 6: Design Patterns in Engine Architecture\n",
    "\n",
    "### Pattern 1: Strategy Pattern\n",
    "The Engine ABC is a **Strategy** - different algorithms (engines) with the same interface.\n",
    "\n",
    "### Pattern 2: Template Method (Implicit)\n",
    "The ABC defines the \"what\", implementations define the \"how\".\n",
    "\n",
    "### Pattern 3: Factory (in Odibi core)\n",
    "```python\n",
    "def get_engine(engine_type: str) -> Engine:\n",
    "    if engine_type == \"pandas\":\n",
    "        return PandasEngine()\n",
    "    elif engine_type == \"spark\":\n",
    "        return SparkEngine()\n",
    "```\n",
    "\n",
    "### Pattern 4: Adapter (for Delta Lake)\n",
    "```python\n",
    "# PandasEngine adapts deltalake library to Engine interface\n",
    "dt = DeltaTable(full_path)\n",
    "return dt.to_pandas()  # Adapt to pd.DataFrame\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Part 7: Testing with Mock Engines\n",
    "\n",
    "The abstraction enables easy testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from abc import ABC\n",
    "\n",
    "class MockEngine:\n",
    "    \"\"\"Simple mock engine for testing pipeline logic.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reads = []\n",
    "        self.writes = []\n",
    "        self.sql_queries = []\n",
    "    \n",
    "    def read(self, connection, format, table=None, path=None, options=None):\n",
    "        self.reads.append({\n",
    "            'connection': connection,\n",
    "            'format': format,\n",
    "            'path': path or table\n",
    "        })\n",
    "        # Return mock data\n",
    "        return pd.DataFrame({'id': [1, 2, 3], 'value': [10, 20, 30]})\n",
    "    \n",
    "    def write(self, df, connection, format, table=None, path=None, mode='overwrite', options=None):\n",
    "        self.writes.append({\n",
    "            'connection': connection,\n",
    "            'format': format,\n",
    "            'path': path or table,\n",
    "            'mode': mode,\n",
    "            'rows': len(df)\n",
    "        })\n",
    "    \n",
    "    def execute_sql(self, sql, context):\n",
    "        self.sql_queries.append(sql)\n",
    "        return pd.DataFrame({'result': ['mocked']})\n",
    "    \n",
    "    def get_schema(self, df):\n",
    "        return df.columns.tolist()\n",
    "    \n",
    "    def get_shape(self, df):\n",
    "        return df.shape\n",
    "\n",
    "# Test a pipeline\n",
    "mock = MockEngine()\n",
    "# run_pipeline(mock, connection)\n",
    "\n",
    "# Assert expectations\n",
    "print(f\"Reads: {mock.reads}\")\n",
    "print(f\"Writes: {mock.writes}\")\n",
    "print(f\"SQL: {mock.sql_queries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Insights\n",
    "\n",
    "### 1. ABC Stability is Critical\n",
    "The Engine ABC hasn't changed in 2+ years because:\n",
    "- Covers all essential operations\n",
    "- Methods are small, focused (SRP)\n",
    "- Interface is minimal but sufficient\n",
    "\n",
    "### 2. Different Engines, Different Trade-offs\n",
    "- **Pandas**: Simple, fast for small data, rich ecosystem\n",
    "- **Spark**: Complex, essential for big data, JVM overhead\n",
    "- **DuckDB**: Sweet spot for analytical workloads\n",
    "\n",
    "### 3. Abstraction Enables Evolution\n",
    "Odibi can:\n",
    "- Add new engines without changing pipeline code\n",
    "- Optimize engines independently\n",
    "- Test with mocks\n",
    "- Let users choose based on needs\n",
    "\n",
    "### 4. Storage Options Pattern\n",
    "Two approaches:\n",
    "- **Pandas**: Merge per-operation (flexible)\n",
    "- **Spark**: Configure once (efficient)\n",
    "\n",
    "Both work because they're hidden behind the abstraction!\n",
    "\n",
    "### 5. SQL is Not One-Size-Fits-All\n",
    "- Pandas needs external SQL engine (DuckDB)\n",
    "- Spark has native SQL\n",
    "- The abstraction handles both transparently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "The Engine abstraction is Odibi's architectural foundation:\n",
    "\n",
    "1. **Engine ABC** - 9 methods defining the contract\n",
    "2. **PandasEngine** - In-memory, flexible, feature-rich\n",
    "3. **SparkEngine** - Distributed, scalable, production-grade\n",
    "4. **Abstraction Benefits** - Swappable, testable, extensible\n",
    "\n",
    "**Design Principles Applied:**\n",
    "- ‚úÖ Abstraction (ABC hides details)\n",
    "- ‚úÖ Polymorphism (treat engines uniformly)\n",
    "- ‚úÖ Dependency Inversion (depend on Engine, not PandasEngine)\n",
    "- ‚úÖ Open/Closed (add engines without modifying ABC)\n",
    "\n",
    "**Next Steps:**\n",
    "- Complete `exercises.ipynb` to build DuckDBEngine\n",
    "- Study `engine_comparison.md` for detailed reference\n",
    "- Proceed to `04_context_api/` to see how engines get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Additional Resources\n",
    "\n",
    "### Odibi Source Code\n",
    "- [base.py](file:///c:/Users/hodibi/OneDrive%20-%20Ingredion/Desktop/Repos/Odibi/odibi/engine/base.py)\n",
    "- [pandas_engine.py](file:///c:/Users/hodibi/OneDrive%20-%20Ingredion/Desktop/Repos/Odibi/odibi/engine/pandas_engine.py)\n",
    "- [spark_engine.py](file:///c:/Users/hodibi/OneDrive%20-%20Ingredion/Desktop/Repos/Odibi/odibi/engine/spark_engine.py)\n",
    "\n",
    "### Related Lessons\n",
    "- `foundations/06_abc` - Abstract Base Classes fundamentals\n",
    "- `odibi_deep_dive/01_config_system` - EngineType enum\n",
    "- `odibi_deep_dive/02_connection_layer` - How connections work\n",
    "- `odibi_deep_dive/04_context_api` - How engines receive data\n",
    "\n",
    "### External References\n",
    "- [DuckDB Python API](https://duckdb.org/docs/api/python)\n",
    "- [PySpark SQL Module](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
    "- [Strategy Pattern](https://refactoring.guru/design-patterns/strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
