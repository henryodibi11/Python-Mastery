{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises 03: Build Your Own DuckDBEngine\n",
    "\n",
    "## üéØ Goal\n",
    "\n",
    "Implement a fully functional **DuckDBEngine** that:\n",
    "- Inherits from Odibi's Engine ABC\n",
    "- Uses DuckDB's analytical capabilities\n",
    "- Supports CSV, Parquet, and JSON formats\n",
    "- Leverages DuckDB's zero-copy Pandas integration\n",
    "- Demonstrates a third execution model (analytical, in-process)\n",
    "\n",
    "## üìã Background\n",
    "\n",
    "**Why DuckDB?**\n",
    "- Fast analytical queries (10-100x faster than Pandas on aggregations)\n",
    "- SQL-first design (optimized for SELECT, GROUP BY, JOIN)\n",
    "- Zero-copy integration with Pandas/Arrow\n",
    "- Single-process (no cluster needed)\n",
    "- Perfect for 1GB - 100GB datasets\n",
    "\n",
    "**Use Cases:**\n",
    "- Analytical pipelines (BI, reporting)\n",
    "- SQL-heavy transformations\n",
    "- When Spark is overkill but Pandas is too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 1: Set Up the Class Structure\n",
    "\n",
    "Create the basic DuckDBEngine class that inherits from Engine ABC.\n",
    "\n",
    "**Tasks:**\n",
    "1. Import the Engine ABC from Odibi\n",
    "2. Create DuckDBEngine class\n",
    "3. Implement `__init__()` to create a DuckDB connection\n",
    "4. Add docstring explaining the engine's purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add Odibi to path\n",
    "odibi_path = Path(r\"c:/Users/hodibi/OneDrive - Ingredion/Desktop/Repos/Odibi\")\n",
    "sys.path.insert(0, str(odibi_path))\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Import Engine ABC\n",
    "\n",
    "# TODO: Create DuckDBEngine class\n",
    "class DuckDBEngine:\n",
    "    \"\"\"DuckDB-based execution engine.\n",
    "    \n",
    "    Optimized for analytical workloads on medium-sized datasets (1GB - 100GB).\n",
    "    Uses DuckDB's columnar storage and vectorized execution for fast aggregations,\n",
    "    joins, and window functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \":memory:\"):\n",
    "        \"\"\"Initialize DuckDB engine.\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to DuckDB file, or \":memory:\" for in-memory\n",
    "        \"\"\"\n",
    "        # TODO: Create DuckDB connection\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 2: Implement read()\n",
    "\n",
    "Implement the `read()` method using DuckDB's optimized readers.\n",
    "\n",
    "**DuckDB Reading Patterns:**\n",
    "```python\n",
    "# CSV with auto-detection\n",
    "df = conn.execute(\"SELECT * FROM read_csv_auto('file.csv')\").df()\n",
    "\n",
    "# Parquet\n",
    "df = conn.execute(\"SELECT * FROM read_parquet('file.parquet')\").df()\n",
    "\n",
    "# JSON\n",
    "df = conn.execute(\"SELECT * FROM read_json_auto('file.json')\").df()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "1. Get full path from connection (use `connection.get_path()`)\n",
    "2. Implement format dispatch (csv, parquet, json)\n",
    "3. Use DuckDB's read functions\n",
    "4. Return Pandas DataFrame (`.df()`)\n",
    "5. Raise helpful error for unsupported formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method to DuckDBEngine\n",
    "\n",
    "def read(\n",
    "    self,\n",
    "    connection: Any,\n",
    "    format: str,\n",
    "    table: Optional[str] = None,\n",
    "    path: Optional[str] = None,\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read data using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        connection: Connection object (with get_path method)\n",
    "        format: Data format (csv, parquet, json)\n",
    "        table: Table name\n",
    "        path: File path\n",
    "        options: Format-specific options (ignored for now)\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 3: Implement write()\n",
    "\n",
    "Implement the `write()` method using DuckDB's COPY command.\n",
    "\n",
    "**DuckDB Writing Patterns:**\n",
    "```python\n",
    "# Register DataFrame\n",
    "conn.register('temp_df', df)\n",
    "\n",
    "# Write CSV\n",
    "conn.execute(f\"COPY temp_df TO 'file.csv' (HEADER, DELIMITER ',')\")\n",
    "\n",
    "# Write Parquet\n",
    "conn.execute(f\"COPY temp_df TO 'file.parquet' (FORMAT PARQUET)\")\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "1. Get full path from connection\n",
    "2. Register DataFrame with DuckDB (temporary name)\n",
    "3. Use COPY command for each format\n",
    "4. Handle overwrite mode (delete existing file first)\n",
    "5. Handle append mode (use COPY ... (HEADER false) for CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method to DuckDBEngine\n",
    "\n",
    "def write(\n",
    "    self,\n",
    "    df: pd.DataFrame,\n",
    "    connection: Any,\n",
    "    format: str,\n",
    "    table: Optional[str] = None,\n",
    "    path: Optional[str] = None,\n",
    "    mode: str = \"overwrite\",\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    ") -> None:\n",
    "    \"\"\"Write data using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        connection: Connection object\n",
    "        format: Output format (csv, parquet)\n",
    "        table: Table name\n",
    "        path: File path\n",
    "        mode: Write mode (overwrite/append)\n",
    "        options: Format-specific options\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 4: Implement execute_sql()\n",
    "\n",
    "This is the EASIEST method - DuckDB is SQL-native!\n",
    "\n",
    "**Pattern:**\n",
    "```python\n",
    "# Register all DataFrames from context\n",
    "for name in context.list_names():\n",
    "    df = context.get(name)\n",
    "    conn.register(name, df)\n",
    "\n",
    "# Execute SQL\n",
    "result = conn.execute(sql).df()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "1. Register all DataFrames from context\n",
    "2. Execute the SQL query\n",
    "3. Return result as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method to DuckDBEngine\n",
    "\n",
    "def execute_sql(self, sql: str, context) -> pd.DataFrame:\n",
    "    \"\"\"Execute SQL query using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        sql: SQL query string\n",
    "        context: Execution context with registered DataFrames\n",
    "        \n",
    "    Returns:\n",
    "        Result DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 5: Implement Introspection Methods\n",
    "\n",
    "These are simple wrappers around Pandas methods.\n",
    "\n",
    "**Tasks:**\n",
    "1. `get_schema()` - Return column names as list\n",
    "2. `get_shape()` - Return (rows, columns) tuple\n",
    "3. `count_rows()` - Return row count\n",
    "4. `count_nulls()` - Return dict of column -> null count\n",
    "5. `validate_schema()` - Return list of validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these methods to DuckDBEngine\n",
    "\n",
    "def get_schema(self, df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Get DataFrame column names.\"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def get_shape(self, df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Get DataFrame shape.\"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def count_rows(self, df: pd.DataFrame) -> int:\n",
    "    \"\"\"Count rows in DataFrame.\"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def count_nulls(self, df: pd.DataFrame, columns: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"Count nulls in specified columns.\"\"\"\n",
    "    # TODO: Implement (can copy from PandasEngine)\n",
    "    pass\n",
    "\n",
    "def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Validate DataFrame schema.\"\"\"\n",
    "    # TODO: Implement (can copy from PandasEngine)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 6: Implement execute_operation()\n",
    "\n",
    "For now, just implement pivot (like PandasEngine).\n",
    "\n",
    "**DuckDB Pivot Pattern:**\n",
    "```python\n",
    "# Register DataFrame\n",
    "conn.register('df', df)\n",
    "\n",
    "# Execute pivot using DuckDB SQL\n",
    "sql = f\"\"\"\n",
    "    PIVOT df\n",
    "    ON {pivot_column}\n",
    "    USING FIRST({value_column})\n",
    "    GROUP BY {','.join(group_by)}\n",
    "\"\"\"\n",
    "result = conn.execute(sql).df()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "1. Extract pivot parameters (group_by, pivot_column, value_column, agg_func)\n",
    "2. Build DuckDB PIVOT SQL\n",
    "3. Execute and return result\n",
    "\n",
    "**Bonus:** Use DuckDB's native PIVOT syntax instead of Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method to DuckDBEngine\n",
    "\n",
    "def execute_operation(self, operation: str, params: Dict[str, Any], df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Execute built-in operation.\n",
    "    \n",
    "    Args:\n",
    "        operation: Operation name\n",
    "        params: Operation parameters\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Result DataFrame\n",
    "    \"\"\"\n",
    "    if operation == \"pivot\":\n",
    "        # TODO: Implement using DuckDB PIVOT\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported operation: {operation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Exercise 7: Test Your Engine\n",
    "\n",
    "Create comprehensive tests for DuckDBEngine.\n",
    "\n",
    "**Test Cases:**\n",
    "1. Read CSV file\n",
    "2. Write Parquet file\n",
    "3. Execute SQL query with JOIN\n",
    "4. Execute SQL with aggregation\n",
    "5. Pivot operation\n",
    "6. Schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temp directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Test directory: {temp_dir}\")\n",
    "\n",
    "# Create sample CSV\n",
    "test_csv = Path(temp_dir) / \"test.csv\"\n",
    "sample_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'category': ['A', 'B', 'A', 'B', 'A'],\n",
    "    'value': [100, 200, 150, 250, 120]\n",
    "})\n",
    "sample_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(f\"Sample data created: {test_csv}\")\n",
    "print(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create mock connection\n",
    "class MockConnection:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = Path(base_path)\n",
    "    \n",
    "    def get_path(self, path):\n",
    "        return str(self.base_path / path)\n",
    "\n",
    "conn = MockConnection(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test 1 - Read CSV\n",
    "engine = DuckDBEngine()\n",
    "df = engine.read(conn, format='csv', path='test.csv')\n",
    "print(\"Read test:\")\n",
    "print(df)\n",
    "assert len(df) == 5\n",
    "print(\"‚úÖ Read CSV passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test 2 - Write Parquet\n",
    "engine.write(df, conn, format='parquet', path='output.parquet')\n",
    "written_df = engine.read(conn, format='parquet', path='output.parquet')\n",
    "print(\"Write test:\")\n",
    "print(written_df)\n",
    "assert len(written_df) == 5\n",
    "print(\"‚úÖ Write Parquet passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test 3 - SQL Aggregation\n",
    "# Create context (simple dict for now)\n",
    "from odibi.context import PandasContext\n",
    "\n",
    "ctx = PandasContext()\n",
    "ctx.register('sales', df)\n",
    "\n",
    "result = engine.execute_sql(\n",
    "    \"SELECT category, SUM(value) as total FROM sales GROUP BY category\",\n",
    "    ctx\n",
    ")\n",
    "print(\"SQL aggregation test:\")\n",
    "print(result)\n",
    "assert len(result) == 2  # Two categories\n",
    "print(\"‚úÖ SQL aggregation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test 4 - Introspection methods\n",
    "schema = engine.get_schema(df)\n",
    "print(f\"Schema: {schema}\")\n",
    "assert schema == ['id', 'category', 'value']\n",
    "\n",
    "shape = engine.get_shape(df)\n",
    "print(f\"Shape: {shape}\")\n",
    "assert shape == (5, 3)\n",
    "\n",
    "rows = engine.count_rows(df)\n",
    "print(f\"Rows: {rows}\")\n",
    "assert rows == 5\n",
    "\n",
    "print(\"‚úÖ Introspection methods passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Exercise 8: Performance Comparison\n",
    "\n",
    "Compare DuckDBEngine performance vs PandasEngine.\n",
    "\n",
    "**Test:** Aggregation on larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create larger dataset\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(1000000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 1000000),\n",
    "    'value': np.random.randint(1, 1000, 1000000)\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {large_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DuckDB\n",
    "duck_engine = DuckDBEngine()\n",
    "duck_ctx = PandasContext()\n",
    "duck_ctx.register('data', large_df)\n",
    "\n",
    "start = time.time()\n",
    "duck_result = duck_engine.execute_sql(\n",
    "    \"SELECT category, AVG(value) as avg_val, COUNT(*) as count FROM data GROUP BY category\",\n",
    "    duck_ctx\n",
    ")\n",
    "duck_time = time.time() - start\n",
    "\n",
    "print(f\"DuckDB: {duck_time:.4f} seconds\")\n",
    "print(duck_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Pandas (for comparison)\n",
    "from odibi.engine.pandas_engine import PandasEngine\n",
    "\n",
    "pandas_engine = PandasEngine()\n",
    "pandas_ctx = PandasContext()\n",
    "pandas_ctx.register('data', large_df)\n",
    "\n",
    "start = time.time()\n",
    "pandas_result = pandas_engine.execute_sql(\n",
    "    \"SELECT category, AVG(value) as avg_val, COUNT(*) as count FROM data GROUP BY category\",\n",
    "    pandas_ctx\n",
    ")\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "print(f\"Pandas (via DuckDB): {pandas_time:.4f} seconds\")\n",
    "print(pandas_result)\n",
    "\n",
    "print(f\"\\nüìä Performance: DuckDB vs Pandas\")\n",
    "print(f\"DuckDB: {duck_time:.4f}s\")\n",
    "print(f\"Pandas: {pandas_time:.4f}s\")\n",
    "print(f\"Speedup: {pandas_time / duck_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Bonus Challenges\n",
    "\n",
    "### Challenge 1: Delta Lake Support\n",
    "Add Delta Lake reading to DuckDBEngine using DuckDB's `delta_scan()` extension:\n",
    "```python\n",
    "conn.execute(\"INSTALL delta\")\n",
    "conn.execute(\"LOAD delta\")\n",
    "df = conn.execute(\"SELECT * FROM delta_scan('path/to/delta')\").df()\n",
    "```\n",
    "\n",
    "### Challenge 2: Remote File Support\n",
    "Add S3/Azure support using DuckDB's extensions:\n",
    "```python\n",
    "conn.execute(\"INSTALL azure\")\n",
    "conn.execute(\"LOAD azure\")\n",
    "conn.execute(f\"SET azure_storage_connection_string = '{conn_str}'\")\n",
    "```\n",
    "\n",
    "### Challenge 3: Query Optimization\n",
    "Use DuckDB's `EXPLAIN` to analyze query performance:\n",
    "```python\n",
    "plan = conn.execute(f\"EXPLAIN {sql}\").df()\n",
    "```\n",
    "\n",
    "### Challenge 4: Streaming Reads\n",
    "Implement chunked reading for very large files:\n",
    "```python\n",
    "# Read in chunks\n",
    "result = conn.execute(f\"SELECT * FROM read_csv_auto('{path}') LIMIT 10000 OFFSET {offset}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Completion Checklist\n",
    "\n",
    "- [ ] DuckDBEngine class created\n",
    "- [ ] All 9 abstract methods implemented\n",
    "- [ ] read() supports CSV, Parquet, JSON\n",
    "- [ ] write() supports CSV, Parquet\n",
    "- [ ] execute_sql() works with context\n",
    "- [ ] Introspection methods work\n",
    "- [ ] Tests pass\n",
    "- [ ] Performance comparison complete\n",
    "- [ ] (Bonus) Delta Lake support\n",
    "- [ ] (Bonus) Remote file support\n",
    "\n",
    "Once complete, check `solutions.ipynb` for reference implementation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
